{
  "generated_at": "2026-02-11T23:37:00.223Z",
  "layers": [
    {
      "id": "observability-dashboard",
      "name": "Observability Dashboard",
      "type": "layer",
      "layer": null,
      "color": "sky",
      "icon": "ðŸ“Š",
      "description": "Runtime Visibility â€” read-only web UI that observes but never mutates. The single pane of glass for both observation and control.",
      "tags": [
        "read-only",
        "web ui",
        "live updates"
      ],
      "sort_order": 10,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "live-dashboard",
          "name": "Live Dashboard",
          "type": "app",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "ðŸ“Š",
          "description": "Real-time view of the entire runtime. Read-only â€” it observes but never mutates. Built as a simple web app (React / plain HTML) that polls the State Store + Supervisor health API. Runs as a separate process managed by the Supervisor. Think: the runtime equivalent of the process tree diagram, but live. Reads from two sources: State Store (goals, tasks, tool logs, escalations, checkpoints) and Supervisor health API (process status, heartbeat data, resource usage). It writes nothing â€” pure read-only observer. Optional: SSE/WebSocket push from State Store for live updates without polling. Human Gate approval actions can be embedded here, making it the single pane of glass for both observation and control.",
          "tags": [
            "read-only",
            "web ui",
            "sse/websocket"
          ],
          "sort_order": 11,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Single-page web app showing process status (up/down) and current goal. Polls Supervisor health API every 5s. Basic goal queue display from State Store. Static HTML + vanilla JS.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Real-time view of the entire runtime. Read-only â€” it observes but never mutates. Built as a simple web app (React / plain HTML) that polls the State Store + Supervisor health API. Runs as a separate process managed by the Supervisor. Think: the runtime equivalent of the process tree diagram, but live. Data sources: State Store â†’ goals, tasks, tool call logs, escalations, checkpoints, fast-path records, injection events. Supervisor Health API â†’ process status, uptime, restart count, memory, current model per instance. No direct process inspection â€” dashboard never connects to OpenCode or proxies directly. Push vs Poll: SSE from State Store for live updates; poll Supervisor health every 5s. Human Gate embedded: approval buttons for gated tasks + escalation responses in same UI.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Full process tree view with live status. Goal & task feed with click-to-inspect. Tool call timeline with filtering. Security events panel. Escalation queue with response actions.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "SSE/WebSocket for real-time push updates. Entity Explorer for User KG. Repo Map for Code Graph. Embedded Human Gate approval UI. Performance metrics and resource graphs.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-dashboard.feature",
                "title": "Live Dashboard (MVP)",
                "content": "Feature: Live Dashboard (MVP)\n  A read-only web UI showing process status and current goal.\n  Polls the Supervisor health API and State Store.\n\n  Background:\n    Given the Dashboard web app is running\n\n  Scenario: Display process status\n    Given the Supervisor health API reports Meta-Agent as \"running\" and Worker as \"running\"\n    When the Dashboard polls the health API\n    Then both processes are shown with \"running\" status indicators\n\n  Scenario: Display current goal\n    Given the State Store contains a goal \"Build user auth\" with status \"in-progress\"\n    When the Dashboard polls the State Store\n    Then the current goal \"Build user auth\" is displayed\n    And its status shows \"in-progress\"\n\n  Scenario: Auto-refresh on interval\n    Given the Dashboard is displaying process status\n    When 5 seconds have elapsed\n    Then the Dashboard polls the health API again\n    And the display updates with fresh data\n\n  Scenario: Show offline state\n    Given the Supervisor health API is unreachable\n    When the Dashboard polls the health API\n    Then a \"Supervisor Unreachable\" indicator is shown\n\n  Scenario: Read-only â€” no mutation endpoints\n    Given the Dashboard is running\n    Then it exposes no POST, PUT, or DELETE endpoints\n    And all data access is via GET requests\n"
              }
            ]
          }
        },
        {
          "id": "live-process-view",
          "name": "Live Process View",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "ðŸ”´",
          "description": "Process tree with real-time status: running, recovering, crashed, paused for every child. Uptime, restart count, current model, memory usage per instance.",
          "tags": [
            "running",
            "recovering",
            "crashed",
            "paused"
          ],
          "sort_order": 12,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Process tree with real-time status: running, recovering, crashed, paused for every child. Uptime, restart count, current model, memory usage per instance.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "goal-task-feed",
          "name": "Goal & Task Feed",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "ðŸ“‹",
          "description": "Live stream of the Goal Queue. Current goal, decomposed sub-tasks, completion status. See what the Meta-Agent is planning and what the Worker is executing. Clickable to inspect full task payloads.",
          "tags": [
            "live stream",
            "clickable",
            "task payloads"
          ],
          "sort_order": 13,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Live stream of the Goal Queue. Current goal, decomposed sub-tasks, completion status. See what the Meta-Agent is planning and what the Worker is executing. Clickable to inspect full task payloads.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "tool-call-timeline",
          "name": "Tool Call Timeline",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "ðŸ”§",
          "description": "Chronological feed of every tool call (both instances). Shows: tool name, args (truncated), response status, latency, sanitiser verdict (pass/block). Filterable by instance, tool, and status. This is your debugging lifeline.",
          "tags": [
            "chronological",
            "filterable",
            "debugging lifeline"
          ],
          "sort_order": 14,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Chronological feed of every tool call (both instances). Shows: tool name, args (truncated), response status, latency, sanitiser verdict (pass/block). Filterable by instance, tool, and status. This is your debugging lifeline.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "security-events",
          "name": "Security Events",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "ðŸ›¡",
          "description": "Sanitiser verdicts, blocked injections with raw payload preview, injection frequency per tool, auto-disable events. Links to full audit log entries. Alerts when injection rate exceeds threshold.",
          "tags": [
            "audit log",
            "injection frequency",
            "auto-disable"
          ],
          "sort_order": 15,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Sanitiser verdicts, blocked injections with raw payload preview, injection frequency per tool, auto-disable events. Links to full audit log entries. Alerts when injection rate exceeds threshold.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "escalation-queue",
          "name": "Escalation Queue",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "â¸",
          "description": "Worker escalation requests waiting for Meta-Agent or human review. Shows the Worker's question, context snapshot, and available actions: respond, override, or abort task.",
          "tags": [
            "respond",
            "override",
            "abort"
          ],
          "sort_order": 16,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Worker escalation requests waiting for Meta-Agent or human review. Shows the Worker's question, context snapshot, and available actions: respond, override, or abort task.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "entity-explorer",
          "name": "Entity Explorer",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "ðŸ‘¤",
          "description": "Browse the User Knowledge Graph. See people, projects, preferences, and their relationships. Understand what the agents \"know about you\".",
          "tags": [
            "new",
            "user kg",
            "browse"
          ],
          "sort_order": 17,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Browse the User Knowledge Graph. See people, projects, preferences, and their relationships. Understand what the agents \"know about you\".",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "repo-map",
          "name": "Repo Map",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "ðŸ—º",
          "description": "Visualise the Code Graph. Module hierarchy, file deps, data flows. See the agent's structural understanding of your codebase.",
          "tags": [
            "new",
            "code graph",
            "visualise"
          ],
          "sort_order": 18,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Visualise the Code Graph. Module hierarchy, file deps, data flows. See the agent's structural understanding of your codebase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "human-gate-dashboard",
          "name": "Human Gate (Dashboard)",
          "type": "component",
          "layer": "observability-dashboard",
          "color": "sky",
          "icon": "â›³",
          "description": "Approval queue + escalation responses. Gate actions embeddable in dashboard UI.",
          "tags": [
            "approval",
            "embedded"
          ],
          "sort_order": 19,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Approval queue + escalation responses. Gate actions embeddable in dashboard UI.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "Read-only web UI. Process view, goal feed, tool timeline, security events, graph explorers. The dashboard is a thin web UI (React / plain HTML) that reads from two sources: the State Store (goals, tasks, tool logs, escalations, checkpoints) and the Supervisor's health API (process status, heartbeat data, resource usage). It writes nothing â€” pure read-only observer. Optional: SSE/WebSocket push from State Store for live updates without polling. The Human Gate approval actions can be embedded here too, making it the single pane of glass for both observation and control.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "supervisor-layer",
      "name": "Supervisor",
      "type": "layer",
      "layer": null,
      "color": "purple",
      "icon": "ðŸ‘",
      "description": "The Only Immortal Process â€” process management, signal handling, recovery state machine. No LLM, no planning. If it dies, systemd/Docker restarts it.",
      "tags": [
        "immortal",
        "no llm",
        "process manager"
      ],
      "sort_order": 20,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "supervisor",
          "name": "Supervisor",
          "type": "app",
          "layer": "supervisor-layer",
          "color": "purple",
          "icon": "ðŸ‘",
          "description": "Manages all child processes. Heartbeat + crash recovery with tiered priority. No LLM, no planning â€” just process management, signal handling, and the recovery state machine. This is what makes it stable: it has almost no reasons to crash. If it does, systemd/Docker restarts it. Exposes a health API (HTTP on :9100) for the dashboard. Kill switch: /stop HTTP endpoint + SIGTERM handler â†’ instant halt of all children. Emergency brake for runaway agents.",
          "tags": [
            "immortal",
            "health api",
            "kill switch",
            "no llm"
          ],
          "sort_order": 21,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Spawn and monitor two child processes (meta-agent, worker). Detect crashes via waitpid(). Restart crashed children with basic retry logic. Expose /health HTTP endpoint returning JSON process status. Handle SIGTERM for graceful shutdown of all children.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Manages all child processes. Heartbeat + crash recovery with tiered priority. No LLM, no planning â€” just process management, signal handling, and the recovery state machine. This is what makes it stable: it has almost no reasons to crash. If it does, systemd/Docker restarts it. Exposes a health API (HTTP on :9100) for the dashboard. Kill switch: /stop HTTP endpoint + SIGTERM handler â†’ instant halt of all children. Why a Supervisor, not a cron job? (1) Instant detection: waitpid() returns the moment a child exits. Cron's worst-case latency = poll interval. (2) Crash loop handling: Supervisor tracks restart count + applies exponential backoff. (3) Multi-step recovery: kill â†’ checkpoint read â†’ config rebuild â†’ context inject â†’ respawn. (4) Lifecycle ownership: Supervisor owns the full process tree â€” PIDs, health, state. (5) Signal handling: catches SIGTERM/SIGCHLD and coordinates graceful shutdown. Stability hierarchy: Tier âˆž (Supervisor) immortal â†’ Tier 0 (Meta-Agent) recovered first â†’ Tier 1 (Worker) expendable. Recovery order: Supervisor â†’ Meta-Agent â†’ Worker. Each tier can recover the one below it.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Add exponential backoff on repeated crashes. Liveness probe (hang detection via output timeout_ms). Recovery state machine with tiered priority (meta-agent first). Checkpoint-aware recovery â€” read last checkpoint before respawn. Human Gate alerting after max 5 retries.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Full config-as-code DSL for spawn configuration. Resource monitoring (memory, CPU per child). Kill switch /stop HTTP endpoint. Dashboard SSE push for process events. Per-instance gate policies. Runtime flag switching for gate modes.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-process-management.feature",
                "title": "Supervisor Process Management (MVP)",
                "content": "Feature: Supervisor Process Management (MVP)\n  The Supervisor spawns and monitors child processes.\n  It detects crashes and restarts children with basic retry logic.\n\n  Background:\n    Given the Supervisor process is running\n\n  Scenario: Spawn child processes on startup\n    When the Supervisor starts\n    Then it spawns the Meta-Agent process\n    And it spawns the Worker process\n    And both processes are in \"running\" state\n\n  Scenario: Detect child crash via waitpid\n    Given the Meta-Agent process is running\n    When the Meta-Agent process exits unexpectedly\n    Then the Supervisor detects the exit within 100ms\n    And the exit is logged with the process ID and exit code\n\n  Scenario: Restart crashed child\n    Given the Worker process has crashed\n    When the Supervisor detects the crash\n    Then it restarts the Worker process\n    And the new process is in \"running\" state\n    And the restart count is incremented\n\n  Scenario: Respect maximum retry limit\n    Given the Worker has crashed 5 times consecutively\n    When the Worker crashes again\n    Then the Supervisor does not restart the Worker\n    And the Worker state is set to \"failed\"\n    And an alert is logged\n\n  Scenario: Health API returns process status\n    Given both child processes are running\n    When a GET request is made to /health\n    Then the response status is 200\n    And the response body contains status for each child process\n    And each status includes \"pid\", \"state\", and \"uptime\"\n\n  Scenario: Graceful shutdown on SIGTERM\n    Given both child processes are running\n    When the Supervisor receives SIGTERM\n    Then it sends SIGTERM to all child processes\n    And it waits for children to exit\n    And it exits with code 0\n"
              }
            ]
          }
        },
        {
          "id": "dual-heartbeat",
          "name": "Dual Heartbeat",
          "type": "component",
          "layer": "supervisor-layer",
          "color": "purple",
          "icon": "ðŸ’“",
          "description": "Monitors both OpenCode instances independently via waitpid() + liveness probes. Instant crash detection (zero latency) â€” waitpid() returns the moment a child exits. Periodic liveness probe for hang detection â€” if no output for timeout_ms, treat as hung. Detects: exit, hang, OOM. If the Worker crashes â†’ recover using Meta-Agent's last plan. If the Meta-Agent crashes â†’ recover it first (higher priority), then it re-dispatches the Worker. Exponential backoff, max 5 retries â†’ alert Human Gate.",
          "tags": [
            "waitpid",
            "zero latency",
            "meta first",
            "worker second",
            "exponential backoff"
          ],
          "sort_order": 22,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "waitpid() loop for crash detection. Basic restart on exit. Retry counter with max limit. Log crash events.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Monitors both OpenCode instances independently via waitpid() + liveness probes. Instant crash detection (zero latency) â€” waitpid() returns the moment a child exits. Periodic liveness probe for hang detection â€” if no output for timeout_ms, treat as hung. Detects: exit, hang, OOM. If Worker crashes â†’ recover using Meta-Agent's last plan. If Meta-Agent crashes â†’ recover it first (higher priority), then it re-dispatches Worker. Exponential backoff, max 5 retries â†’ alert Human Gate.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "human-gate",
          "name": "Human Gate",
          "type": "app",
          "layer": "supervisor-layer",
          "color": "pink",
          "icon": "â›³",
          "description": "Three modes: full-auto, approve-goals, approve-all. Plus write fence: dangerous ops require approval even in full-auto. Also surfaces escalation requests from the Worker. Write fence per-instance: Meta-Agent config mutations and Worker destructive ops can have independent gate policies. Gate mode is a runtime flag â€” switch between modes without restarting any process.",
          "tags": [
            "full-auto",
            "approve-goals",
            "approve-all",
            "write fence",
            "runtime flag"
          ],
          "sort_order": 23,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Basic approval queue. CLI-based approve/reject. Write fence for destructive operations (hardcoded list). Block until approved or timeout.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Three modes: full-auto, approve-goals, approve-all. Plus write fence: dangerous ops require approval even in full-auto. Also surfaces escalation requests from the Worker. Write fence per-instance: Meta-Agent config mutations and Worker destructive ops have independent gate policies. Gate mode is a runtime flag â€” switch between modes without restarting.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Runtime mode switching (full-auto, approve-goals, approve-all). Per-instance gate policies. Escalation forwarding from Meta-Agent. Dashboard-embeddable approval UI.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Configurable write fence per tool category. Approval delegation rules. Audit trail of all gate decisions. Timeout policies with configurable fallback actions.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-human-gate.feature",
                "title": "Human Gate (MVP)",
                "content": "Feature: Human Gate (MVP)\n  The Human Gate provides an approval queue for dangerous\n  operations and a write fence for destructive actions.\n\n  Background:\n    Given the Human Gate is running\n\n  Scenario: Block destructive operation for approval\n    Given the write fence includes \"database drop\" operations\n    When a task requests a database drop\n    Then the task is paused with status \"awaiting_approval\"\n    And the approval request is added to the queue\n\n  Scenario: Approve pending request\n    Given a task is paused awaiting approval\n    When a human approves the request\n    Then the task status changes to \"approved\"\n    And execution resumes\n\n  Scenario: Reject pending request\n    Given a task is paused awaiting approval\n    When a human rejects the request\n    Then the task status changes to \"rejected\"\n    And the task is aborted\n\n  Scenario: Timeout on unanswered request\n    Given a task has been awaiting approval for longer than the timeout\n    When the timeout expires\n    Then the task is aborted\n    And the timeout event is logged\n"
              }
            ]
          }
        },
        {
          "id": "fast-path-router",
          "name": "Fast Path Router",
          "type": "component",
          "layer": "supervisor-layer",
          "color": "lime",
          "icon": "âš¡",
          "description": "Rule engine (no LLM). Classifies tasks as fast, full, or gated. Scores incoming tasks by complexity signals: single-step? (e.g. \"format this file\"), no ambiguity? (clear input/output), no tool mutation needed? (current tools suffice). If all signals pass â†’ direct to Worker, skipping Meta-Agent. Meta-Agent notified after completion via State Store. Cuts latency and cost for simple tasks by ~50%. Configurable: fast_path: \"aggressive\" | \"conservative\" | \"off\". Can query User KG for context (\"does user prefer X for this type of task?\"). If fast-path task fails, re-routed through Meta-Agent.",
          "tags": [
            "fast",
            "full",
            "gated",
            "rule engine",
            "~50% savings",
            "new"
          ],
          "sort_order": 24,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Simple rule engine: match task text against patterns (single verb, no conditionals, target file exists). Three outputs: fast, full, gated. Configurable threshold.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Rule engine (no LLM). Classifies tasks as fast, full, or gated. Scores by complexity signals: single-step, no ambiguity, no tool mutation needed. Configurable: fast_path: \"aggressive\" | \"conservative\" | \"off\". Can query User KG for context. Cuts latency and cost ~50% for simple tasks. Fallback: failed fast-path tasks re-route through Meta-Agent.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "The only immortal process. No LLM. Process manager + crash recovery + heartbeat. Exposes health API on :9100. Kill switch: /stop HTTP endpoint + SIGTERM handler. If it dies, systemd/Docker restarts it. Recovery sequence on crash detection: (1) Kill hung process if still alive. (2) Read last checkpoint from State Store. (3) Rebuild config via DSL. (4) Inject resume context via Context Rebuilder. (5) Respawn fresh OpenCode process (new PID, clean slate, resume prompt). (6) Exponential backoff if repeated failures, max 5 retries â†’ alert Human Gate.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "task-router-layer",
      "name": "Task Router",
      "type": "layer",
      "layer": null,
      "color": "lime",
      "icon": "âš¡",
      "description": "Fast Path Decision Point â€” lightweight rule engine (no LLM) routes tasks by complexity: trivial goes direct to Worker, complex goes to Meta-Agent, dangerous requires human approval.",
      "tags": [
        "rule engine",
        "no llm",
        "classifier"
      ],
      "sort_order": 30,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "fast-path",
          "name": "Fast Path",
          "type": "component",
          "layer": "task-router-layer",
          "color": "lime",
          "icon": "âš¡",
          "description": "Rule engine says: single-step, unambiguous, existing tools suffice. Task goes directly to Worker. Meta-Agent notified post-completion via State Store. Flow: task â†’ classifier â†’ FAST â†’ Worker â†’ done â†’ State Store â†’ Meta-Agent reads.",
          "tags": [
            "trivial",
            "direct",
            "skip planner"
          ],
          "sort_order": 31,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Rule engine says: single-step, unambiguous, existing tools suffice. Task goes directly to Worker. Meta-Agent notified post-completion via State Store. Flow: task â†’ classifier â†’ FAST â†’ Worker â†’ done â†’ State Store â†’ Meta-Agent reads.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "full-path",
          "name": "Full Path",
          "type": "component",
          "layer": "task-router-layer",
          "color": "orange",
          "icon": "ðŸ§ ",
          "description": "Classifier says: multi-step, ambiguous, or needs tool changes. Task goes to Meta-Agent for decomposition. Normal planning loop. Flow: task â†’ classifier â†’ FULL â†’ Meta-Agent â†’ plan â†’ dispatch â†’ Worker â†’ State Store.",
          "tags": [
            "complex",
            "decomposition",
            "planning loop"
          ],
          "sort_order": 32,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Classifier says: multi-step, ambiguous, or needs tool changes. Task goes to Meta-Agent for decomposition. Normal planning loop. Flow: task â†’ classifier â†’ FULL â†’ Meta-Agent â†’ plan â†’ dispatch â†’ Worker â†’ State Store.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "gated-path",
          "name": "Gated Path",
          "type": "component",
          "layer": "task-router-layer",
          "color": "purple",
          "icon": "â›³",
          "description": "Classifier or Human Gate flags: destructive, high-cost, or security-sensitive. Task pauses for human approval before any routing. Flow: task â†’ classifier â†’ GATE â†’ Human Gate â†’ approve â†’ (fast or full path).",
          "tags": [
            "dangerous",
            "approval required",
            "security-sensitive"
          ],
          "sort_order": 33,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Classifier or Human Gate flags: destructive, high-cost, or security-sensitive. Task pauses for human approval. Flow: task â†’ classifier â†’ GATE â†’ Human Gate â†’ approve â†’ (fast or full path).",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "Fast path decision point. Rule engine (no LLM) routes tasks by complexity. Classifier is NOT an LLM â€” it's a rule engine (regex + heuristics on task text). Signals: single verb (\"format\", \"lint\", \"rename\"), no conditional language, target file exists, current tools suffice. Configurable threshold: fast_path: \"aggressive\" | \"conservative\" | \"off\". Meta-Agent stays aware: fast-path completions logged to State Store. Fallback: if fast-path task fails, re-routed through Meta-Agent.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "knowledge-graphs",
      "name": "Knowledge Graphs",
      "type": "layer",
      "layer": null,
      "color": "gold",
      "icon": "ðŸ§ ",
      "description": "Dual graph stores: User Knowledge Graph (domain context â€” people, projects, preferences) + RPG Code Graph (repo structure â€” files, modules, deps, data flows).",
      "tags": [
        "dual stores",
        "domain context",
        "repo structure"
      ],
      "sort_order": 40,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "user-knowledge-graph",
          "name": "User Knowledge Graph",
          "type": "app",
          "layer": "knowledge-graphs",
          "color": "gold",
          "icon": "ðŸ‘¤",
          "description": "A persistent graph of the user's world. Nodes are domain entities â€” people, projects, clients, teams, products, preferences, business rules, conventions, deadlines. Edges are typed relationships with metadata. This is not about code â€” it's about understanding who you are and what you care about so agents make contextually appropriate decisions. Entity types: person, project, org, team, preference, convention, deadline, stack, compliance, product, domain-concept, decision. Relationship types: OWNS, PREFERS, WORKS_WITH, HAS_CLIENT, USES_STACK, REQUIRES, CONVENTION, HAS_DEADLINE, DECIDED, DISLIKES. Populated by: (1) User directly â€” onboarding flow or dashboard edits. (2) Meta-Agent â€” infers entities from conversations and patterns over time. (3) Never by Worker â€” same injection-safety principle. Worker reads, never writes. Confidence layering: user-explicit (1.0) > meta-agent-inferred (0.8) > auto-extracted (0.6).",
          "tags": [
            "new",
            "persistent",
            "entity types",
            "relationship types",
            "confidence layering"
          ],
          "sort_order": 41,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "SQLite-backed entity store. Add/query entities with typed relationships. Basic traversal (1-hop neighbours). Manual entity creation via CLI or dashboard. Simple text search across entities.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "A persistent graph of the user's world. Nodes are domain entities â€” people, projects, clients, teams, products, preferences, business rules, conventions, deadlines. Edges are typed relationships with metadata. This is not about code â€” it's about understanding who you are and what you care about so agents make contextually appropriate decisions. Entity types: person, project, org, team, preference, convention, deadline, stack, compliance, product, domain-concept, decision. Relationship types: OWNS, PREFERS, WORKS_WITH, HAS_CLIENT, USES_STACK, REQUIRES, CONVENTION, HAS_DEADLINE, DECIDED, DISLIKES. Populated by: (1) User directly â€” onboarding flow or dashboard edits. (2) Meta-Agent â€” infers entities from conversations and patterns. (3) Never by Worker â€” injection safety. Confidence layering: user-explicit (1.0) > meta-agent-inferred (0.8) > auto-extracted (0.6). What it solves: Personalisation (\"Alice prefers typed SQL over ORMs\"), project awareness (\"acme-saas uses Next.js + Postgres, client needs SOC2\"), team context (\"Bob is backend lead, prefers PRs\"), decision memory (\"We decided JWT over sessions on Jan 15\"), convention enforcement (\"No ORMs, minimal comments, Tailwind\"), deadline awareness (Meta-Agent prioritises based on known deadlines).",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Meta-Agent write access for inferred entities. Confidence layering (user-explicit 1.0 > meta-inferred 0.8). Multi-hop traversal queries. Convention enforcement lookups. Deadline awareness queries.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Full graph query language. Temporal awareness (when was this preference set?). Conflict resolution for contradictory preferences. Export/import for portability. Dashboard entity editor.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-user-kg.feature",
                "title": "User Knowledge Graph (MVP)",
                "content": "Feature: User Knowledge Graph (MVP)\n  A persistent entity-relationship store for domain context:\n  people, projects, preferences, conventions, deadlines.\n\n  Background:\n    Given the User Knowledge Graph SQLite database exists\n\n  Scenario: Add an entity\n    When an entity is added with type \"person\" and name \"Alice\"\n    Then the entity exists in the graph with a unique ID\n    And it has type \"person\" and name \"Alice\"\n\n  Scenario: Add a relationship between entities\n    Given entities \"Alice\" (person) and \"acme-saas\" (project) exist\n    When a relationship \"OWNS\" is added from \"Alice\" to \"acme-saas\"\n    Then the edge exists with type \"OWNS\"\n    And it references both entities\n\n  Scenario: Query 1-hop neighbours\n    Given \"Alice\" has relationships to \"acme-saas\", \"Bob\", and \"minimal-comments\"\n    When querying neighbours of \"Alice\"\n    Then all 3 connected entities are returned\n    And each result includes the relationship type\n\n  Scenario: Search entities by text\n    Given entities \"Alice\", \"Bob\", and \"acme-saas\" exist\n    When searching for \"alice\"\n    Then the entity \"Alice\" is returned\n\n  Scenario: Add entity with metadata\n    When an entity is added with type \"preference\" name \"no-orms\" and metadata '{\"reason\": \"team decision\"}'\n    Then the entity exists with the metadata attached\n"
              }
            ]
          }
        },
        {
          "id": "rpg-code-graph",
          "name": "RPG Code Graph",
          "type": "app",
          "layer": "knowledge-graphs",
          "color": "emerald",
          "icon": "ðŸ—º",
          "description": "An RPG-style structural graph of the current codebase. Encodes file hierarchy, module boundaries, inter-module data flows, function signatures, class inheritance, and import dependencies. Inspired by Microsoft RPG/ZeroRepo (https://arxiv.org/abs/2509.16198). This is a code quality feature â€” it helps the Worker write structurally coherent code by understanding what exists, what depends on what, and where new code should go. Node types: module, file, function, class, interface, package, route, schema, test. Edge types: CONTAINS, IMPORTS, EXPORTS, DATA_FLOW, EXTENDS, IMPLEMENTS, DEPENDS_ON, TESTS, CALLS. Populated by: (1) Static analysis on init â€” AST parse via tree-sitter on repo load (~seconds for repos under 100K LoC). (2) Worker's Checkpointer â€” auto-updates after file edits (re-parse only changed files, diff old vs new, update edges incrementally). (3) Meta-Agent â€” can annotate with higher-level module boundaries and data flow intentions. Lightweight â€” no LLM needed for extraction. Query patterns: topo_order(module), data_flow(A,B), dependents(file), pattern(type,dir), where_to_add(capability).",
          "tags": [
            "new",
            "tree-sitter",
            "ast parsing",
            "disposable",
            "re-derivable",
            "query patterns"
          ],
          "sort_order": 42,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Static analysis on repo load using tree-sitter. Build initial graph from imports, exports, class hierarchy. Basic queries: list files in module, show imports for file. SQLite-backed.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "An RPG-style structural graph of the current codebase. Encodes file hierarchy, module boundaries, inter-module data flows, function signatures, class inheritance, and import dependencies. Inspired by Microsoft RPG/ZeroRepo (arxiv.org/abs/2509.16198). This is a code quality feature â€” it helps the Worker write structurally coherent code. Node types: module, file, function, class, interface, package, route, schema, test. Edge types: CONTAINS, IMPORTS, EXPORTS, DATA_FLOW, EXTENDS, IMPLEMENTS, DEPENDS_ON, TESTS, CALLS. What it solves: Dependency awareness (Worker knows what imports what before editing), placement decisions (\"where should rate limiting go?\" â†’ traverse moduleâ†’fileâ†’function), topological code generation (build in dependency order), data flow understanding (inter-module edges), pattern consistency (existing patterns visible), blast radius estimation (Meta-Agent traverses deps). Implementation: (1) Initial build on repo load â€” tree-sitter AST parse, extract files/imports/exports/classes/functions, infer modules from directory structure, ~seconds for repos under 100K LoC. (2) Incremental update on edit â€” Checkpointer detects file-edit tool calls, re-parses only changed files, diffs old vs new, updates edges incrementally. (3) Query patterns â€” topo_order(module), data_flow(A,B), dependents(file), pattern(type,dir), where_to_add(capability).",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Incremental updates via Checkpointer (re-parse only changed files). Dependency traversal (topo_order, dependents). Data flow edges between modules. Pattern queries.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Full where_to_add capability suggestions. Blast radius estimation. Meta-Agent annotations. Multi-language AST support. Dashboard Repo Map visualization.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-code-graph.feature",
                "title": "RPG Code Graph (MVP)",
                "content": "Feature: RPG Code Graph (MVP)\n  Static analysis on repo load builds a structural graph of the\n  codebase: files, imports, exports, classes, functions.\n\n  Background:\n    Given a repository with source files exists\n\n  Scenario: Build initial graph from repo\n    When the Code Graph builder runs on the repository\n    Then nodes are created for each source file\n    And edges are created for import relationships\n    And the graph is stored in SQLite\n\n  Scenario: Extract function exports\n    Given a file \"auth/handler.ts\" exports function \"verifyToken\"\n    When the AST parser processes the file\n    Then a function node \"verifyToken\" exists\n    And an EXPORTS edge connects the file to the function\n\n  Scenario: Extract import relationships\n    Given \"api/routes.ts\" imports from \"auth/handler.ts\"\n    When the AST parser processes both files\n    Then an IMPORTS edge connects \"api/routes.ts\" to \"auth/handler.ts\"\n\n  Scenario: Infer modules from directory structure\n    Given the repository has directories \"auth/\", \"api/\", \"db/\"\n    When the Code Graph builder runs\n    Then module nodes are created for \"auth\", \"api\", \"db\"\n    And CONTAINS edges connect modules to their files\n\n  Scenario: Query files in a module\n    Given the module \"auth\" contains \"handler.ts\" and \"middleware.ts\"\n    When querying files in module \"auth\"\n    Then both files are returned\n"
              }
            ]
          }
        }
      ],
      "versions": {
        "overview": {
          "content": "Dual graph stores. User Knowledge Graph holds domain context (people, projects, preferences). RPG Code Graph holds repo structure (files, modules, deps, data flows). Three stores, three purposes: State Store (operational â€” what's happening now, prunable), User KG (domain â€” who you are, long-lived), Code Graph (repo structure â€” what the codebase looks like, disposable and re-derivable from code).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "dual-agents",
      "name": "Dual OpenCode Instances",
      "type": "layer",
      "layer": null,
      "color": "orange",
      "icon": "ðŸ§ ",
      "description": "Two stock OpenCode instances. Meta-Agent (Planner) uses cheap/fast model (Haiku/Sonnet), plans and dispatches. Worker (Executor) uses strong model (Sonnet/Opus), executes one phase at a time. They share no tools â€” isolation by design.",
      "tags": [
        "stock opencode",
        "dual instances",
        "tool isolation"
      ],
      "sort_order": 50,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "meta-agent",
          "name": "Meta-Agent (Planner)",
          "type": "app",
          "layer": "dual-agents",
          "color": "orange",
          "icon": "ðŸ§ ",
          "description": "Stock OpenCode, planning system prompt, cheap/fast model (Haiku/Sonnet). This instance never touches the codebase or external APIs directly. It only plans, evaluates, and dispatches. Tier-0: recovered first. If this is down, the Worker has no direction. Only internal tools, no injection surface. Traverses User KG to align plans with user preferences, deadlines, team context. Reads Code Graph to understand repo structure before decomposing coding tasks. Also handles escalation responses: reads Worker's request_clarification entries from State Store, reasons about them, and writes guidance back â€” which the Worker receives on its next checkpoint resume or via check_escalation_response tool. System prompt: \"You are a task planner. Use your tools to: read the goal queue, check worker status, decompose goals into tasks, dispatch tasks, evaluate results, and generate follow-up goals. You may also evolve the worker's tools and config when needed.\" The loop emerges from the prompt + tool availability.",
          "tags": [
            "tier-0",
            "10 internal tools",
            "reads/writes user kg",
            "reads code graph",
            "haiku/sonnet",
            "new"
          ],
          "sort_order": 51,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Single OpenCode instance with planning system prompt. Read goal queue, decompose into sub-tasks, dispatch to Worker via State Store. Read Worker progress from checkpoints. Basic goal â†’ task decomposition.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Stock OpenCode, planning system prompt, cheap/fast model (Haiku/Sonnet). This instance never touches the codebase or external APIs directly. It only plans, evaluates, and dispatches. Tier-0: recovered first. If this is down, the Worker has no direction. Only internal tools, no injection surface. Traverses User KG to align plans with user preferences, deadlines, team context. Reads Code Graph to understand repo structure before decomposing coding tasks. Also handles escalation responses: reads Worker's request_clarification entries from State Store, reasons about them, and writes guidance back. System prompt: \"You are a task planner. Use your tools to: read the goal queue, check worker status, decompose goals into tasks, dispatch tasks, evaluate results, and generate follow-up goals. You may also evolve the worker's tools and config when needed.\" The loop emerges from the prompt + tool availability. How Meta-Agent uses both graphs: Before planning â€” \"What stack? Deadlines? Preferences?\" Before decomposing â€” \"Which modules? Dependency order? Blast radius?\" Task dispatch enrichment â€” includes relevant KG + Code Graph context in Worker's task prompt. Tool selection â€” User KG says \"prefers Brave over Google\" â†’ configures proxy. Knowledge curation â€” writes inferred preferences to User KG. Self-evolution: tool discovery via tool_registry.search, config mutation via typed builder DSL (not raw JSON), sub-goal generation, prompt evolution based on observed results. Guardrails: budget limits, scope limits, allowed tool categories, model whitelist.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Phase-locked BDD/TDD dispatch pipeline. Gate verification between phases. Escalation response handling. User KG reads for planning context. Code Graph reads for repo-aware decomposition.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Self-evolution: tool discovery + hot-swap via proxy_admin. Config mutation via DSL. Prompt evolution based on observed results. Knowledge curation â€” write inferred preferences to User KG. Budget and scope guardrails.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-planning-loop.feature",
                "title": "Meta-Agent Planning Loop (MVP)",
                "content": "Feature: Meta-Agent Planning Loop (MVP)\n  The Meta-Agent reads goals, decomposes them into tasks,\n  and dispatches work to the Worker via the State Store.\n\n  Background:\n    Given the Meta-Agent is running with a planning system prompt\n    And the State Store is accessible\n\n  Scenario: Read next goal from queue\n    Given the goal queue contains \"Build user authentication\"\n    When the Meta-Agent checks the goal queue\n    Then it receives the goal \"Build user authentication\"\n    And the goal status is set to \"in-progress\"\n\n  Scenario: Decompose goal into tasks\n    Given the Meta-Agent has received the goal \"Build user authentication\"\n    When it decomposes the goal\n    Then the State Store contains at least 2 sub-tasks\n    And each sub-task has a description and ordering\n\n  Scenario: Dispatch task to Worker\n    Given a sub-task \"Create login endpoint\" exists in the State Store\n    When the Meta-Agent dispatches the task\n    Then the task status is set to \"dispatched\"\n    And the task includes a description and success criteria\n\n  Scenario: Read Worker progress\n    Given a task has been dispatched to the Worker\n    When the Meta-Agent checks Worker progress\n    Then it receives the latest checkpoint for that task\n    And the checkpoint includes tool calls made and their results\n\n  Scenario: Complete goal when all tasks done\n    Given all sub-tasks for a goal are in \"complete\" status\n    When the Meta-Agent evaluates the goal\n    Then the goal status is set to \"complete\"\n    And the Meta-Agent reads the next goal from the queue\n"
              }
            ]
          }
        },
        {
          "id": "worker",
          "name": "Worker (Executor)",
          "type": "app",
          "layer": "dual-agents",
          "color": "cyan",
          "icon": "âš¡",
          "description": "Stock OpenCode, execution system prompt, strong model (Sonnet/Opus). Tier-1, ephemeral, no fork. Lower stability priority â€” if it crashes, the Meta-Agent re-dispatches. Treated as ephemeral and replaceable. On recovery, the agent continues without knowing it crashed â€” the Context Rebuilder injects a resume prompt that makes it look like a natural continuation. External tools, sanitiser required. Reads User KG to respect user preferences during execution (naming conventions, tech choices). Traverses Code Graph to write structurally coherent code (dependency-aware edits, correct placement). Has two escalation tools: request_clarification to pause and ask the planner for guidance, and check_escalation_response to poll for an answer. System prompt: \"If you're uncertain about scope, direction, or trade-offs, use request_clarification. Don't guess â€” ask.\" Receives tasks as structured system prompt injections: phase, task, constraints, forbidden_actions, available_tools, success_criteria. Single-phase isolation: the Worker sees \"Write failing step tests for this feature file. DO NOT implement any production code.\" It literally cannot skip ahead because it doesn't know what \"ahead\" is. The forbidden_actions field explicitly lists what it must not do (e.g. [\"create production files\", \"modify existing src/\", \"run tests in watch mode\"]).",
          "tags": [
            "tier-1",
            "dynamic tools",
            "reads user kg",
            "reads code graph",
            "sanitiser required",
            "ephemeral",
            "no fork",
            "sonnet/opus",
            "new"
          ],
          "sort_order": 52,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Single OpenCode instance with execution system prompt. Receive task from State Store, execute with available tools, report results. Basic tool access via MCP proxy.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Stock OpenCode, execution system prompt, strong model (Sonnet/Opus). Tier-1, ephemeral, no fork. Lower stability priority â€” if it crashes, the Meta-Agent re-dispatches. Treated as ephemeral and replaceable. On recovery, the agent continues without knowing it crashed â€” the Context Rebuilder injects a resume prompt that makes it look like a natural continuation. External tools, sanitiser required. Reads User KG to respect user preferences during execution. Traverses Code Graph for structurally coherent code. Has escalation tools: request_clarification and check_escalation_response. System prompt: \"If you're uncertain about scope, direction, or trade-offs, use request_clarification. Don't guess â€” ask.\" Receives tasks as structured injections: phase, task, constraints, forbidden_actions, available_tools, success_criteria. Single-phase isolation: Worker sees only current phase, never what comes next. Example forbidden_actions: [\"create production files\", \"modify existing src/\", \"run tests in watch mode\"]. The Worker literally cannot skip ahead because it doesn't know what \"ahead\" is.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Phase-locked execution (single phase per dispatch, forbidden_actions enforcement). Escalation tools (request_clarification, check_escalation_response). User KG reads for preference-aware execution. Code Graph reads for structural coherence.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Context-aware resume after crash (transparent to agent). Dynamic tool manifest â€” handles hot-swap mid-session. Confidence scoring on outputs. Full sanitiser integration on all external I/O.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-task-execution.feature",
                "title": "Worker Task Execution (MVP)",
                "content": "Feature: Worker Task Execution (MVP)\n  The Worker receives tasks from the State Store and executes\n  them using available tools via the MCP proxy.\n\n  Background:\n    Given the Worker is running with an execution system prompt\n    And the MCP proxy is accessible with at least one tool\n\n  Scenario: Receive dispatched task\n    Given a task \"Create login endpoint\" is in \"dispatched\" status\n    When the Worker checks for pending tasks\n    Then it receives the task with description and constraints\n\n  Scenario: Execute task with tools\n    Given the Worker has received a task\n    When it executes the task\n    Then it makes at least one tool call via the MCP proxy\n    And each tool call is logged to the State Store\n\n  Scenario: Report task completion\n    Given the Worker has finished executing a task\n    When it reports results\n    Then the task status is set to \"complete\"\n    And the result summary is written to the State Store\n\n  Scenario: Handle tool call failure\n    Given the Worker is executing a task\n    When a tool call returns an error\n    Then the Worker logs the error\n    And it attempts an alternative approach or reports failure\n\n  Scenario: Operate within provided constraints\n    Given the Worker receives a task with forbidden_actions [\"delete files\"]\n    When it executes the task\n    Then it does not call any tool that would delete files\n"
              }
            ]
          }
        },
        {
          "id": "goal-queue",
          "name": "goal_queue",
          "type": "component",
          "layer": "dual-agents",
          "color": "orange",
          "icon": "ðŸ“‹",
          "description": "push, pop, peek, reprioritise â€” manages the persistent goal queue.",
          "tags": [
            "meta-agent tool"
          ],
          "sort_order": 53,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "push, pop, peek, reprioritise â€” manages the persistent goal queue.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "state-reader",
          "name": "state_reader",
          "type": "component",
          "layer": "dual-agents",
          "color": "orange",
          "icon": "ðŸ“–",
          "description": "get_checkpoint, get_task_log, get_escalations â€” reads Worker's progress.",
          "tags": [
            "meta-agent tool"
          ],
          "sort_order": 54,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "get_checkpoint, get_task_log, get_escalations â€” reads Worker's progress.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "worker-control",
          "name": "worker_control",
          "type": "component",
          "layer": "dual-agents",
          "color": "orange",
          "icon": "ðŸ”§",
          "description": "dispatch, abort, respond_escalation â€” sends phase-locked work to Worker. Dispatch includes phase, forbidden_actions, success_criteria.",
          "tags": [
            "meta-agent tool",
            "phase-locked"
          ],
          "sort_order": 55,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "dispatch, abort, respond_escalation â€” sends phase-locked work to Worker. Dispatch includes phase, forbidden_actions, success_criteria.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "proxy-admin",
          "name": "proxy_admin",
          "type": "component",
          "layer": "dual-agents",
          "color": "orange",
          "icon": "ðŸ“¡",
          "description": "register, deregister, list â€” mutates Worker's tool manifest at runtime.",
          "tags": [
            "meta-agent tool",
            "hot-swap"
          ],
          "sort_order": 56,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "register, deregister, list â€” mutates Worker's tool manifest at runtime.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "config-mutator",
          "name": "config_mutator",
          "type": "component",
          "layer": "dual-agents",
          "color": "orange",
          "icon": "âš™",
          "description": "update_prompt, update_model, update_agents â€” evolves Worker's config via typed builder DSL. Validated, versioned, rollback-safe. Not raw JSON editing.",
          "tags": [
            "meta-agent tool",
            "dsl",
            "rollback-safe"
          ],
          "sort_order": 57,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "update_prompt, update_model, update_agents â€” evolves Worker's config via typed builder DSL. Validated, versioned, rollback-safe.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "tool-registry",
          "name": "tool_registry",
          "type": "component",
          "layer": "dual-agents",
          "color": "orange",
          "icon": "ðŸ”",
          "description": "search, inspect, install â€” discovers new MCP servers from a catalogue.",
          "tags": [
            "meta-agent tool",
            "discovery"
          ],
          "sort_order": 58,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "search, inspect, install â€” discovers new MCP servers from a catalogue.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "user-kg-read-meta",
          "name": "user_kg_read (Meta)",
          "type": "component",
          "layer": "dual-agents",
          "color": "gold",
          "icon": "ðŸ‘¤",
          "description": "query, traverse, search â€” Meta-Agent reads User KG for planning context.",
          "tags": [
            "meta-agent tool",
            "new"
          ],
          "sort_order": 59,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "query, traverse, search â€” Meta-Agent reads User KG for planning context.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "user-kg-write-meta",
          "name": "user_kg_write (Meta)",
          "type": "component",
          "layer": "dual-agents",
          "color": "gold",
          "icon": "âœ",
          "description": "add_entity, add_edge, annotate â€” Meta-Agent writes inferred entities to User KG.",
          "tags": [
            "meta-agent tool",
            "new"
          ],
          "sort_order": 60,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "add_entity, add_edge, annotate â€” Meta-Agent writes inferred entities to User KG.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "code-graph-read-meta",
          "name": "code_graph_read (Meta)",
          "type": "component",
          "layer": "dual-agents",
          "color": "emerald",
          "icon": "ðŸ—º",
          "description": "expand, path, topo_order â€” Meta-Agent reads Code Graph for repo-aware decomposition.",
          "tags": [
            "meta-agent tool",
            "new"
          ],
          "sort_order": 61,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "expand, path, topo_order â€” Meta-Agent reads Code Graph for repo-aware decomposition.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "code-graph-write-meta",
          "name": "code_graph_write (Meta)",
          "type": "component",
          "layer": "dual-agents",
          "color": "emerald",
          "icon": "âœ",
          "description": "annotate_module, set_data_flow â€” Meta-Agent annotates Code Graph with module boundaries and data flow intentions.",
          "tags": [
            "meta-agent tool",
            "new"
          ],
          "sort_order": 62,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "annotate_module, set_data_flow â€” Meta-Agent annotates Code Graph with module boundaries and data flow intentions.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "request-clarification",
          "name": "request_clarification",
          "type": "component",
          "layer": "dual-agents",
          "color": "teal",
          "icon": "ðŸ™‹",
          "description": "Writes question + context snapshot to State Store. Sets task status to paused:awaiting_guidance. Worker halts current execution and waits.",
          "tags": [
            "worker tool",
            "escalation"
          ],
          "sort_order": 63,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Writes question + context snapshot to State Store. Sets task status to paused:awaiting_guidance. Worker halts current execution and waits.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "check-escalation-response",
          "name": "check_escalation_response",
          "type": "component",
          "layer": "dual-agents",
          "color": "teal",
          "icon": "ðŸ“¨",
          "description": "Polls State Store for Meta-Agent's response. Returns guidance or still_pending. Worker resumes when guidance arrives.",
          "tags": [
            "worker tool",
            "escalation"
          ],
          "sort_order": 64,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Polls State Store for Meta-Agent's response. Returns guidance or still_pending. Worker resumes when guidance arrives.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "user-kg-read-worker",
          "name": "user_kg_read (Worker)",
          "type": "component",
          "layer": "dual-agents",
          "color": "gold",
          "icon": "ðŸ‘¤",
          "description": "Read-only. No writes (injection safety). Worker reads preferences but cannot poison the knowledge graph even if fully compromised.",
          "tags": [
            "worker tool",
            "read-only",
            "new"
          ],
          "sort_order": 65,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Read-only. No writes (injection safety). Worker reads preferences but cannot poison the knowledge graph.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "code-graph-read-worker",
          "name": "code_graph_read (Worker)",
          "type": "component",
          "layer": "dual-agents",
          "color": "emerald",
          "icon": "ðŸ—º",
          "description": "Read-only. Checkpointer writes on Worker's behalf after file edits trigger AST re-parse.",
          "tags": [
            "worker tool",
            "read-only",
            "new"
          ],
          "sort_order": 66,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Read-only. Checkpointer writes on Worker's behalf after file edits trigger AST re-parse.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "Two stock OpenCode instances. Meta-Agent (Planner) uses cheap model (Haiku/Sonnet), plans and dispatches. Worker (Executor) uses strong model (Sonnet/Opus), executes one phase at a time. Why two instances? (1) Separation of concerns: planning and execution have different tool sets, models, cost profiles, and risk levels. (2) Blast radius: a prompt injection in the Worker can't reach the Planner â€” they share no tools or MCP connection. (3) Independent recovery: Worker can crash and be re-dispatched without losing Meta-Agent's plan state. (4) Cost optimisation: Planner uses cheap model, Worker uses capable model. (5) Dogfooding: you build one runtime, not two systems. Both instances use the same config DSL, proxy, and checkpoint infra.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "escalation-flow",
      "name": "Escalation Flow",
      "type": "layer",
      "layer": null,
      "color": "teal",
      "icon": "ðŸ™‹",
      "description": "Worker â†” Meta-Agent communication via shared state. Worker hits ambiguity â†’ calls request_clarification({question, context, options}) â†’ State Store records escalation â†’ Worker pauses â†’ Meta-Agent picks it up on next cycle â†’ reasons â†’ responds via worker_control.respond_escalation â†’ State Store updates â†’ Worker calls check_escalation_response â†’ receives guidance â†’ resumes. Timeout handling: if Meta-Agent does not respond within escalation_timeout_ms, Worker can (a) proceed with best guess, (b) abort, or (c) escalate to Human Gate. Policy set in Worker system prompt.",
      "tags": [
        "async",
        "no blocking rpc",
        "structured payload"
      ],
      "sort_order": 60,
      "current_version": null,
      "display_state": "Concept",
      "children": [],
      "versions": {
        "overview": {
          "content": "Worker â†” Meta-Agent communication via shared state. Escalation sequence: Worker hits ambiguity â†’ calls request_clarification({question, context, options}) â†’ State Store records with status pending â†’ Worker pauses (returns control to idle loop) â†’ Meta-Agent's state_reader.get_escalations() picks it up â†’ Meta-Agent reasons â†’ calls worker_control.respond_escalation({task_id, guidance}) â†’ State Store updates to resolved â†’ Worker calls check_escalation_response() â†’ receives guidance â†’ resumes. Timeout: if no response within escalation_timeout_ms, Worker can: (a) proceed_best_guess, (b) abort, or (c) escalate_to_human. Design: Worker stays isolated (writes to State Store, not to Meta-Agent directly). Async by design (no blocking RPC). Structured payload: {question, context_snapshot, suggested_options[], urgency}. Crash safety: Checkpointer snapshots escalation state.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "shared-state",
      "name": "Shared State Store",
      "type": "layer",
      "layer": null,
      "color": "blue",
      "icon": "ðŸ’¾",
      "description": "The Bridge â€” SQLite WAL / Postgres. Goals, tasks, tool logs, checkpoints, escalations. Both instances communicate by sharing a database, not a connection.",
      "tags": [
        "sqlite wal",
        "postgres",
        "append-only"
      ],
      "sort_order": 70,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "state-store",
          "name": "State Store",
          "type": "app",
          "layer": "shared-state",
          "color": "blue",
          "icon": "ðŸ’¾",
          "description": "Append-only log that both instances read/write. The Meta-Agent writes goals and reads results. The Worker's Checkpointer writes tool call logs and progress. This is how the two OpenCode instances communicate without direct coupling â€” they share a database, not a connection. Also stores escalation records (question, context, response, status) and fast-path completion records so the Meta-Agent stays aware of tasks it didn't plan. Dashboard reads everything here.",
          "tags": [
            "checkpointer",
            "context rebuilder",
            "crash recovery"
          ],
          "sort_order": 71,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "SQLite WAL database with tables for goals, tasks, and tool_logs. Basic CRUD operations. Both agents read/write via simple SQL. No pruning, no optimization.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Append-only log that both instances read/write. The Meta-Agent writes goals and reads results. The Worker's Checkpointer writes tool call logs and progress. This is how the two OpenCode instances communicate without direct coupling â€” they share a database, not a connection. Also stores escalation records (question, context, response, status) and fast-path completion records so the Meta-Agent stays aware of tasks it didn't plan. Dashboard reads everything here.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Add checkpoints table, escalation records, fast-path completion records. Context rebuilder queries. Pruning policy (keep last N days). Indexes for common query patterns.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "SSE/WebSocket push for live dashboard updates. Postgres option for multi-machine deployments. Full audit trail with retention policies. Query optimization for dashboard views.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-shared-state.feature",
                "title": "Shared State Store (MVP)",
                "content": "Feature: Shared State Store (MVP)\n  The State Store is a SQLite WAL database that both agents\n  read and write. It stores goals, tasks, and tool logs.\n\n  Background:\n    Given the State Store SQLite database exists\n\n  Scenario: Create a goal\n    When a goal \"Build user authentication\" is inserted\n    Then the goal exists with status \"pending\"\n    And the goal has a created_at timestamp\n\n  Scenario: Create tasks for a goal\n    Given a goal exists with id \"goal-1\"\n    When tasks are inserted for goal \"goal-1\"\n    Then each task references the parent goal\n    And each task has status \"pending\" and an ordering index\n\n  Scenario: Log a tool call\n    Given a task exists with id \"task-1\"\n    When a tool call log is inserted with tool \"filesystem\", args hash, and result hash\n    Then the tool log exists with a timestamp\n    And the tool log references \"task-1\"\n\n  Scenario: Both agents can read/write concurrently\n    Given the Meta-Agent is writing a goal\n    And the Worker is writing a tool log\n    Then both writes succeed without conflict\n    And the WAL journal mode handles concurrent access\n\n  Scenario: Query task status by goal\n    Given a goal has 3 tasks with statuses \"complete\", \"in-progress\", \"pending\"\n    When querying tasks for that goal\n    Then all 3 tasks are returned with their statuses\n"
              }
            ]
          }
        },
        {
          "id": "checkpointer",
          "name": "Checkpointer",
          "type": "app",
          "layer": "shared-state",
          "color": "blue",
          "icon": "ðŸ“¸",
          "description": "Taps Worker's Proxy. Writes after every tool response: task ID, tool name, args, result hash, timestamp, plan summary. Also snapshots escalation state so crash recovery can restore a paused-and-waiting Worker correctly. If tool was a file edit, also triggers AST re-parse and Code Graph update. Strategy: tool results are facts; LLM reasoning can be re-derived. So we save the facts (tool call + result) and let the Context Rebuilder regenerate the reasoning frame on recovery. Runs async â€” doesn't block the agent. The proxy fires-and-forgets to the checkpointer; the Worker never waits for a checkpoint write to complete.",
          "tags": [
            "async",
            "fire-and-forget",
            "non-blocking",
            "code graph update"
          ],
          "sort_order": 72,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Intercept tool responses from Worker proxy. Write task_id, tool_name, args_hash, result_hash, timestamp to State Store. Fire-and-forget (async, non-blocking).",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Taps Worker's Proxy. Writes after every tool response: task ID, tool name, args, result hash, timestamp, plan summary. Also snapshots escalation state so crash recovery can restore a paused-and-waiting Worker correctly. If tool was a file edit, triggers AST re-parse and Code Graph update. Strategy: tool results are facts; LLM reasoning can be re-derived. So we save the facts and let the Context Rebuilder regenerate the reasoning frame on recovery. Runs async â€” the proxy fires-and-forgets; the Worker never waits for a checkpoint write to complete.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Escalation state snapshots. File-edit detection triggering Code Graph AST re-parse. Plan summary snapshots for context rebuilder. Idempotency markers for crash recovery.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Configurable checkpoint granularity. Compressed checkpoint storage. Checkpoint pruning with retention policy. Metrics on checkpoint write latency.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-checkpointing.feature",
                "title": "Checkpointer (MVP)",
                "content": "Feature: Checkpointer (MVP)\n  The Checkpointer taps the Worker's MCP proxy and records\n  every tool call for crash recovery and progress tracking.\n\n  Background:\n    Given the Checkpointer is attached to the Worker's MCP proxy\n    And the State Store is accessible\n\n  Scenario: Record tool call after response\n    Given the Worker calls the \"filesystem\" tool with args \"read file.ts\"\n    When the tool returns a successful response\n    Then the Checkpointer writes a record to the State Store\n    And the record includes task_id, tool_name, args_hash, result_hash, and timestamp\n\n  Scenario: Non-blocking operation\n    Given the Worker is executing a task\n    When a tool call completes\n    Then the Checkpointer writes asynchronously\n    And the Worker does not wait for the checkpoint write\n\n  Scenario: Maintain ordering of tool calls\n    Given the Worker makes 3 sequential tool calls\n    When all 3 are checkpointed\n    Then the records are ordered by timestamp\n    And each has a sequential index within the task\n\n  Scenario: Handle write failure gracefully\n    Given the State Store is temporarily unavailable\n    When the Checkpointer attempts to write\n    Then it retries with backoff\n    And the Worker execution is not affected\n"
              }
            ]
          }
        },
        {
          "id": "context-rebuilder",
          "name": "Context Rebuilder",
          "type": "app",
          "layer": "shared-state",
          "color": "blue",
          "icon": "ðŸ“",
          "description": "On crash recovery of either instance: generates resume prompt from compressed checkpoint + relevant graph context. For Worker: \"you were doing X, completed Y, next step Z\". For Meta-Agent: \"current goal is X, worker status is Y, pending goals are Z\". If Worker was in paused:awaiting_guidance state, resume prompt includes the escalation question and any response received while it was down. Lossy by design. You can't clone LLM hidden state â€” it's non-serialisable. This is like a save game, not a VM snapshot. The rebuilt context is \"good enough\" â€” the agent continues without knowing it crashed, picking up from the last checkpoint with a compressed summary of what came before.",
          "tags": [
            "lossy by design",
            "save game",
            "resume prompt"
          ],
          "sort_order": 73,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Read last checkpoint from State Store. Generate basic resume prompt: \"you were doing X, completed Y, next step Z\". Inject as system prompt on respawn.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "On crash recovery of either instance: generates resume prompt from compressed checkpoint + relevant graph context. For Worker: \"you were doing X, completed Y, next step Z\". For Meta-Agent: \"current goal is X, worker status is Y, pending goals are Z\". If Worker was in paused:awaiting_guidance state, resume prompt includes the escalation question and any response received while it was down. Lossy by design. You can't clone LLM hidden state â€” it's non-serialisable. This is like a save game, not a VM snapshot.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Include relevant User KG context in resume prompt. Include Code Graph context for coding tasks. Handle paused:awaiting_guidance state.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Compressed multi-checkpoint summaries. Relevance-ranked context selection. Token budget management for resume prompts.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-context-rebuild.feature",
                "title": "Context Rebuilder (MVP)",
                "content": "Feature: Context Rebuilder (MVP)\n  On crash recovery, the Context Rebuilder generates a resume\n  prompt from the last checkpoint so the agent can continue.\n\n  Background:\n    Given the State Store contains checkpoints for a crashed agent\n\n  Scenario: Generate resume prompt for Worker\n    Given the Worker crashed mid-task on task \"Create login endpoint\"\n    And the last checkpoint shows 3 completed tool calls\n    When the Context Rebuilder generates a resume prompt\n    Then the prompt includes \"you were doing: Create login endpoint\"\n    And it lists the 3 completed tool calls with their results\n    And it states the next expected action\n\n  Scenario: Generate resume prompt for Meta-Agent\n    Given the Meta-Agent crashed while processing goal \"Build auth\"\n    And the goal has 5 tasks, 2 completed and 1 in-progress\n    When the Context Rebuilder generates a resume prompt\n    Then the prompt includes the current goal state\n    And it lists completed and pending tasks\n    And it states the current Worker status\n\n  Scenario: Handle empty checkpoint\n    Given no checkpoints exist for the crashed agent\n    When the Context Rebuilder generates a resume prompt\n    Then it produces a minimal prompt with no prior context\n    And the agent starts fresh\n"
              }
            ]
          }
        }
      ],
      "versions": {
        "overview": {
          "content": "SQLite WAL / Postgres. Goals, tasks, tool logs, checkpoints, escalations. The bridge between both instances â€” they share a database, not a connection. Context preservation: What IS saved â€” task ID + current step index, tool call log (name, args, result hash), plan summary (goal queue snapshot), goal queue pointer, timestamps, escalation state. What is NOT saved â€” LLM hidden state (non-serialisable), full conversation history (too large), in-flight reasoning (ephemeral by nature). Tool results are facts; LLM reasoning can be re-derived.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "mcp-proxies",
      "name": "MCP Proxies",
      "type": "layer",
      "layer": null,
      "color": "orange",
      "icon": "â‡„",
      "description": "Tool proxy layer for both agent instances. Meta-Agent gets static manifest (10 internal tools, no sanitiser). Worker gets dynamic manifest (hot-swappable, sanitiser required, circuit breaker).",
      "tags": [
        "static manifest",
        "dynamic manifest",
        "hot-swappable"
      ],
      "sort_order": 80,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "mcp-proxy-meta",
          "name": "MCP Proxy â€” Meta-Agent",
          "type": "app",
          "layer": "mcp-proxies",
          "color": "orange",
          "icon": "â‡„",
          "description": "Hosts 10 planning tools (6 planning + 4 graph). These are your custom MCP servers â€” small, stable, purpose-built. No external API calls, no injection risk.",
          "tags": [
            "static",
            "no sanitiser",
            "low risk"
          ],
          "sort_order": 81,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Static MCP server hosting goal_queue, state_reader, worker_control tools. Simple stdio transport. No hot-swap needed.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Hosts 10 planning tools (6 planning + 4 graph). These are your custom MCP servers â€” small, stable, purpose-built. No external API calls, no injection risk.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "mcp-proxy-worker",
          "name": "MCP Proxy â€” Worker",
          "type": "app",
          "layer": "mcp-proxies",
          "color": "cyan",
          "icon": "â‡„",
          "description": "Hosts all external-facing tools + escalation + graph reads. Dynamic manifest â€” the Meta-Agent's proxy_admin tool adds/removes servers here at runtime. All responses pass through the Sanitiser. Health & circuit breaker: heartbeats downstream servers; dead endpoints auto-removed from manifest, Meta-Agent notified via State Store so it can find replacements.",
          "tags": [
            "sanitiser required",
            "hot-swappable",
            "checkpoint tap",
            "circuit breaker"
          ],
          "sort_order": 82,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "MCP proxy with configurable tool list. Route tool calls to downstream servers. Pass responses through sanitiser. Basic health check on downstream servers.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Hosts all external-facing tools + escalation + graph reads. Dynamic manifest â€” Meta-Agent's proxy_admin adds/removes servers at runtime. All responses pass through Sanitiser. Health & circuit breaker: heartbeats downstream servers; dead endpoints auto-removed from manifest, Meta-Agent notified via State Store so it can find replacements.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "Tool proxy layer. Meta-Agent gets static manifest (10 internal tools, no sanitiser, low risk). Worker gets dynamic manifest (hot-swappable, sanitiser required, circuit breaker, checkpoint tap).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "security-sandbox",
      "name": "Security Sandbox",
      "type": "layer",
      "layer": null,
      "color": "red",
      "icon": "ðŸ›¡",
      "description": "3-stage sanitiser pipeline (Worker proxy only). Regex heuristics â†’ structural strip (role tags, cap length) â†’ optional LLM classifier. Fail-closed. Isolated subprocess. Scans inbound responses (injection defence) AND outbound tool args (prevents data exfiltration via tricked agent).",
      "tags": [
        "fail-closed",
        "isolated subprocess",
        "bidirectional"
      ],
      "sort_order": 90,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "sanitiser",
          "name": "3-Stage Sanitiser",
          "type": "app",
          "layer": "security-sandbox",
          "color": "red",
          "icon": "ðŸ›¡",
          "description": "Sits between Worker's Proxy and downstream servers. Stage 1: Heuristic regex for common injection patterns. Stage 2: Structural strip (remove role tags, cap response length). Stage 3: Optional LLM classifier for sophisticated detection. The Meta-Agent's proxy does NOT need a sanitiser â€” its tools are all internal, no external input. Isolated subprocess. Fail-closed. Scans inbound responses (injection defence) and outbound tool args (prevents data exfiltration via a tricked agent â€” e.g. an injected prompt that encodes secrets into a search query). Injection events visible in dashboard Security Events panel.",
          "tags": [
            "3-stage",
            "fail-closed",
            "isolated subprocess",
            "bidirectional"
          ],
          "sort_order": 91,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "mvp": {
              "content": "Regex-based heuristic scanner for common injection patterns. Structural strip (remove role tags, cap response length). Pass/block verdict on each tool response. Logging to State Store.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "overview": {
              "content": "Sits between Worker's Proxy and downstream servers. Stage 1: Heuristic regex for common injection patterns. Stage 2: Structural strip (remove role tags, cap response length). Stage 3: Optional LLM classifier for sophisticated detection. The Meta-Agent's proxy does NOT need a sanitiser â€” its tools are all internal, no external input. Isolated subprocess. Fail-closed. Scans inbound responses (injection defence) and outbound tool args (prevents data exfiltration â€” e.g. an injected prompt encoding secrets into a search query). Injection events visible in dashboard Security Events panel.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v1": {
              "content": "Outbound scanning (prevent data exfiltration via tool args). Configurable rule sets per tool. Injection frequency tracking. Auto-disable tools exceeding threshold. Dashboard integration.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            },
            "v2": {
              "content": "Optional LLM classifier stage. Adaptive rules based on observed attack patterns. Per-tool confidence scoring. Full audit trail with payload samples.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {
            "mvp": [
              {
                "filename": "mvp-sanitiser.feature",
                "title": "3-Stage Sanitiser (MVP)",
                "content": "Feature: 3-Stage Sanitiser (MVP)\n  The Sanitiser sits between the Worker's MCP proxy and downstream\n  tool servers. It scans tool responses for injection attempts.\n\n  Background:\n    Given the Sanitiser is running as an isolated subprocess\n\n  Scenario: Pass clean tool response\n    Given a tool response contains normal text content\n    When the Sanitiser processes the response\n    Then the verdict is \"pass\"\n    And the response is forwarded to the Worker unchanged\n\n  Scenario: Block response with injection pattern\n    Given a tool response contains \"ignore previous instructions\"\n    When the Sanitiser processes the response\n    Then the verdict is \"block\"\n    And the response is not forwarded to the Worker\n    And the injection event is logged to the State Store\n\n  Scenario: Strip role tags from response\n    Given a tool response contains \"<system>\" tags\n    When the Sanitiser applies structural stripping\n    Then the role tags are removed from the response\n    And the cleaned response is forwarded\n\n  Scenario: Cap response length\n    Given a tool response exceeds the maximum allowed length\n    When the Sanitiser processes the response\n    Then the response is truncated to the maximum length\n    And the truncation is noted in the log\n\n  Scenario: Fail closed on processing error\n    Given the Sanitiser encounters an internal error during processing\n    When processing a tool response\n    Then the response is blocked (not forwarded)\n    And the error is logged\n"
              }
            ]
          }
        },
        {
          "id": "alert-pipeline",
          "name": "Alert Pipeline",
          "type": "component",
          "layer": "security-sandbox",
          "color": "red",
          "icon": "ðŸ“‹",
          "description": "Blocked injections logged to State Store. Meta-Agent can auto-disable compromised tools via injection feedback loop â€” reads sanitiser alerts and learns to avoid them. Dashboard shows real-time security events feed.",
          "tags": [
            "auto-disable",
            "injection feedback loop",
            "real-time"
          ],
          "sort_order": 92,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Blocked injections logged to State Store. Meta-Agent can auto-disable compromised tools via injection feedback loop â€” reads sanitiser alerts and learns to avoid them. Dashboard shows real-time security events.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "3-stage sanitiser: regex heuristics â†’ structural strip â†’ optional LLM classifier. Fail-closed. Isolated subprocess. Scans inbound + outbound. Worker proxy only. Security model: Meta-Agent has no sanitiser (internal tools only). Worker has full 3-stage sanitiser. Cross-instance isolation: Worker cannot reach Meta-Agent's tools even if fully compromised. Escalation tools are safe: they write structured data to State Store, not free-text to Meta-Agent's prompt. Dashboard is read-only â€” cannot be used as attack vector. Graph writes: Worker cannot write to either graph â€” cannot poison knowledge even if fully compromised. In full-auto mode: sanitiser is the only defence against prompt injection. Write fence still applies for destructive ops. Injection feedback loop lets Meta-Agent auto-disable compromised tools.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "downstream-tools",
      "name": "Downstream MCP Tool Servers",
      "type": "layer",
      "layer": null,
      "color": "amber",
      "icon": "ðŸ”§",
      "description": "External tool servers â€” search, email, database, filesystem, code execution, custom. Hot-swappable â€” Meta-Agent adds/removes at runtime via proxy_admin.",
      "tags": [
        "external",
        "hot-swappable",
        "runtime managed"
      ],
      "sort_order": 100,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "tool-search",
          "name": "Search",
          "type": "external",
          "layer": "downstream-tools",
          "color": "amber",
          "icon": "ðŸ”",
          "description": "External search MCP server.",
          "tags": [
            "ext"
          ],
          "sort_order": 101,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "External search MCP server.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "tool-email",
          "name": "Email",
          "type": "external",
          "layer": "downstream-tools",
          "color": "amber",
          "icon": "âœ‰",
          "description": "External email MCP server.",
          "tags": [
            "ext"
          ],
          "sort_order": 102,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "External email MCP server.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "tool-database",
          "name": "Database",
          "type": "external",
          "layer": "downstream-tools",
          "color": "amber",
          "icon": "ðŸ—„",
          "description": "External database MCP server.",
          "tags": [
            "write"
          ],
          "sort_order": 103,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "External database MCP server.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "tool-filesystem",
          "name": "Filesystem",
          "type": "external",
          "layer": "downstream-tools",
          "color": "amber",
          "icon": "ðŸ“‚",
          "description": "External filesystem MCP server.",
          "tags": [
            "write"
          ],
          "sort_order": 104,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "External filesystem MCP server.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "tool-code-exec",
          "name": "Code Exec",
          "type": "external",
          "layer": "downstream-tools",
          "color": "amber",
          "icon": "ðŸ’»",
          "description": "External code execution MCP server.",
          "tags": [
            "write"
          ],
          "sort_order": 105,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "External code execution MCP server.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "tool-custom",
          "name": "Custom",
          "type": "external",
          "layer": "downstream-tools",
          "color": "amber",
          "icon": "ðŸ§©",
          "description": "Custom MCP servers â€” hot-swappable, added/removed by Meta-Agent at runtime.",
          "tags": [
            "dynamic"
          ],
          "sort_order": 106,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Custom MCP servers â€” hot-swappable, added/removed by Meta-Agent at runtime.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "External MCP tool servers. Search, email, database, filesystem, code execution, custom. Hot-swappable â€” Meta-Agent adds/removes at runtime.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "bdd-tdd-pipeline",
      "name": "BDD/TDD Phase Pipeline",
      "type": "layer",
      "layer": null,
      "color": "teal",
      "icon": "ðŸ”„",
      "description": "Strict 8-phase pipeline enforced by Meta-Agent, executed by Worker. Each phase is a separate dispatch. Worker sees only the current phase â€” never what comes next. Every phase ends with a git commit creating a clean rollback point. Meta-Agent verifies phase gate before advancing. If gate fails, re-dispatch same phase. Phases â‘¦ and â‘§ are audit-only â€” violations feed back as targeted fix dispatches, then re-audit.",
      "tags": [
        "phase isolation",
        "git commits",
        "gate verification"
      ],
      "sort_order": 110,
      "current_version": null,
      "display_state": "Concept",
      "children": [
        {
          "id": "phase-feature",
          "name": "â‘  Feature",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "gold",
          "icon": "ðŸ“",
          "description": "Write the .feature file. Describe the behaviour in Gherkin. DO NOT write any tests or code. Gate: .feature file exists. Git commit after phase.",
          "tags": [
            "gherkin",
            "gate: .feature exists"
          ],
          "sort_order": 111,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Write the .feature file. Describe the behaviour in Gherkin. DO NOT write any tests or code. Gate: .feature file exists. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "phase-steps",
          "name": "â‘¡ Step Tests",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "cyan",
          "icon": "ðŸ§ª",
          "description": "Write failing step definitions for this feature file. DO NOT implement any production code. Gate: step files exist. Git commit after phase.",
          "tags": [
            "failing tests",
            "gate: step files exist"
          ],
          "sort_order": 112,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Write failing step definitions for this feature file. DO NOT implement any production code. Gate: step files exist. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "phase-units",
          "name": "â‘¢ Unit Tests",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "purple",
          "icon": "ðŸ§ª",
          "description": "Write failing unit tests for the components you'll need. DO NOT implement any production code. Gate: test files exist. Git commit after phase.",
          "tags": [
            "failing tests",
            "gate: test files exist"
          ],
          "sort_order": 113,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Write failing unit tests for the components you'll need. DO NOT implement any production code. Gate: test files exist. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "phase-red",
          "name": "â‘£ Red",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "red",
          "icon": "ðŸ”´",
          "description": "Run all tests. Confirm they fail. Report which tests fail and why. DO NOT fix anything. Gate: tests fail. Git commit after phase.",
          "tags": [
            "confirm failure",
            "gate: tests fail"
          ],
          "sort_order": 114,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Run all tests. Confirm they fail. Report which tests fail and why. DO NOT fix anything. Gate: tests fail. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "phase-green",
          "name": "â‘¤ Green",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "green",
          "icon": "ðŸŸ¢",
          "description": "Write the minimum production code to make all tests pass. DO NOT refactor or optimise. Gate: tests pass. Git commit after phase.",
          "tags": [
            "minimum code",
            "gate: tests pass"
          ],
          "sort_order": 115,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Write the minimum production code to make all tests pass. DO NOT refactor or optimise. Gate: tests pass. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "phase-refactor",
          "name": "â‘¥ Refactor",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "sky",
          "icon": "ðŸ”§",
          "description": "Refactor for clarity, DRY, naming. All tests must still pass. DO NOT add new functionality. Gate: tests still pass. Git commit after phase.",
          "tags": [
            "clarity",
            "dry",
            "gate: tests still pass"
          ],
          "sort_order": 116,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "Refactor for clarity, DRY, naming. All tests must still pass. DO NOT add new functionality. Gate: tests still pass. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "phase-arch-review",
          "name": "â‘¦ Arch Review",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "orange",
          "icon": "ðŸ›",
          "description": "LLM-driven audit agent (not a linter). Audit against Clean Architecture standards. Report violations: dependency direction (inner layers importing outer), layer boundary leaks (business logic in controllers, HTTP in domain), abstraction leaks (SQL in repository interface), use case isolation. Traverses Code Graph DATA_FLOW and IMPORTS edges to verify dependency direction structurally. DO NOT fix â€” only report. Report format: {violations: [{file, line, rule, severity, explanation}], passed: bool}. If violations found, Meta-Agent dispatches targeted fix phases then re-runs this review. Gate: 0 violations. Git commit after phase.",
          "tags": [
            "audit-only",
            "clean architecture",
            "gate: 0 violations"
          ],
          "sort_order": 117,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "LLM-driven audit agent. Audit against Clean Architecture: dependency direction (inner layers importing outer = violation), layer boundaries (business logic in controllers), abstraction leaks (SQL in repository interface), use case isolation. Traverses Code Graph DATA_FLOW and IMPORTS edges structurally. DO NOT fix â€” only report. Report: {violations: [{file, line, rule, severity, explanation}], passed: bool}. Gate: 0 violations. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        },
        {
          "id": "phase-sec-review",
          "name": "â‘§ Sec Review",
          "type": "phase",
          "layer": "bdd-tdd-pipeline",
          "color": "rose",
          "icon": "ðŸ”’",
          "description": "LLM-driven security audit agent. Check: injection vectors (SQL, XSS, command injection, path traversal), auth/authz gaps (endpoints without auth middleware, missing permission checks, privilege escalation), secrets exposure (hardcoded keys, tokens in logs, secrets in error messages, .env leaks), unsafe dependencies (known CVEs, deprecated crypto, insecure defaults). Checks compliance requirements from User KG (e.g. \"client requires SOC2\" â†’ enforce specific controls). DO NOT fix â€” only report. Same report format as Arch Review. Gate: 0 findings. Git commit after phase.",
          "tags": [
            "audit-only",
            "security",
            "gate: 0 findings",
            "user kg compliance"
          ],
          "sort_order": 118,
          "current_version": null,
          "display_state": "Concept",
          "versions": {
            "overview": {
              "content": "LLM-driven security audit. Injection vectors (SQL, XSS, command injection, path traversal), auth/authz gaps (missing middleware, privilege escalation), secrets exposure (hardcoded keys, tokens in logs, .env leaks), unsafe dependencies (CVEs, deprecated crypto). Checks User KG compliance requirements (e.g. SOC2). DO NOT fix â€” only report. Same report format. Gate: 0 findings. Git commit after phase.",
              "progress": 0,
              "status": "planned",
              "updated_at": "2026-02-11 14:27:05"
            }
          },
          "features": {}
        }
      ],
      "versions": {
        "overview": {
          "content": "Strict 8-phase pipeline enforced by Meta-Agent: Feature â†’ Steps â†’ Units â†’ Red â†’ Green â†’ Refactor â†’ Arch Review â†’ Sec Review. Every phase ends with a git commit. LLM agents are bad at process discipline. Given \"build a login feature\", they'll jump straight to implementation, skip tests, or refactor before proving anything works. Phase isolation prevents this â€” the Worker can't skip ahead because it doesn't know what \"ahead\" is. The forbidden_actions field explicitly blocks forward-leaking behaviour. This turns the Meta-Agent into a process enforcer, not just a task decomposer. Review phases â‘¦ â‘§ are audit-only â€” dispatched as read-only inspections. Worker reports violations but is forbidden from modifying code. Meta-Agent reads the report, dispatches separate fix phases with specific violations as constraints. Prevents the \"fix one thing, break another\" cascade. Phases are extensible: add performance review, accessibility audit, API design review â€” same pattern.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    }
  ],
  "nodes": [
    {
      "id": "roadmap",
      "name": "Roadmap",
      "type": "app",
      "layer": null,
      "color": "cyan",
      "icon": "ðŸ—º",
      "description": "Living documentation for the Open Autonomous Runtime. Self-tracking component with progression tree, versioned specs, and Gherkin feature files. Built with Clean Architecture, TypeScript, SQLite, and Cytoscape.js.",
      "tags": [
        "self-tracking",
        "clean architecture",
        "bdd",
        "progression tree"
      ],
      "sort_order": 1,
      "current_version": "1.0.0",
      "display_state": "v1",
      "versions": {
        "mvp": {
          "content": "Clean Architecture TypeScript codebase. SQLite graph database with 4 tables (nodes, edges, node_versions, features). BDD test suite (Cucumber + Vitest). CI merge gate. Static web view with architecture diagram. Progression tree home page with Cytoscape.js. Self-tracking as a component.",
          "progress": 100,
          "status": "complete",
          "updated_at": "2026-02-11 17:16:36"
        },
        "overview": {
          "content": "Living documentation for the Open Autonomous Runtime. Self-tracking component with progression tree UI, versioned specs, Gherkin feature files, and BDD/TDD pipeline. Built with Clean Architecture (TypeScript), SQLite graph database, and Cytoscape.js for the progression tree visualization.",
          "progress": 0,
          "status": "in-progress",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Secure API with key-based auth (scoped permissions, salted hashes, rate limiting, security headers, request logging, structured errors). Neo4j graph storage replacing SQLite (native traversals, multi-hop queries, shortest path, cycle detection, data migration, transaction safety). Enhanced component management (PATCH updates, filtering, search, full edge CRUD, version lifecycle, bulk operations up to 100 items, layer management). Feature-driven progress tracking (step-level maths from Gherkin Given/When/Then counts, passing_steps/total_steps percentage, test result recording, combined semver+step weighting, progress history, dashboard summary APIs, CSV export, automatic recalculation on feature/test changes). Version-scoped feature publishing (explicit version in URL path, Gherkin validation, batch upload, graph traversal endpoints for dependency trees, topological sort, shortest path, neighbourhood queries, next-implementable components, feature search).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 23:36:59"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-architecture-export.feature",
            "title": "Architecture Export",
            "content": "Feature: Architecture Export\n  As a web view consumer\n  I want to export the architecture graph to a JSON file\n  So that the static web page can render it without a database connection\n\n  Background:\n    Given a database with architecture data\n\n  Scenario: Export produces a JSON file at the specified path\n    Given an output path \"web/data.json\"\n    When I export the architecture\n    Then the write function is called with path \"web/data.json\"\n    And the written data contains a \"generated_at\" field\n\n  Scenario: Export returns statistics about the exported data\n    Given the database contains 3 nodes, 2 edges, 4 versions, and 1 feature\n    When I export the architecture\n    Then the export result includes stats with total counts\n\n  Scenario: Exported data contains the full architecture structure\n    Given the database contains nodes with versions and features\n    When I export the architecture\n    Then the written data includes layers with children\n    And the written data includes enriched nodes\n    And the written data includes relationship edges\n"
          },
          {
            "filename": "mvp-architecture-graph-assembly.feature",
            "title": "Architecture Graph Assembly",
            "content": "Feature: Architecture Graph Assembly\n  As a roadmap consumer\n  I want the system to assemble a complete architecture graph\n  So that I can see all components, their relationships, versions, and features in one view\n\n  Background:\n    Given a database with architecture data\n\n  Scenario: Assemble all nodes into the graph\n    Given the database contains nodes of types \"layer\", \"component\", and \"store\"\n    When I assemble the architecture graph\n    Then every node appears in the result\n\n  Scenario: Group components under their parent layer\n    Given a layer node \"supervisor-layer\"\n    And a component node \"supervisor\" belonging to layer \"supervisor-layer\"\n    When I assemble the architecture graph\n    Then the layer \"supervisor-layer\" contains child \"supervisor\"\n\n  Scenario: Enrich nodes with their version content\n    Given a component node \"meta-agent\"\n    And versions \"overview\", \"mvp\", \"v1\" exist for node \"meta-agent\"\n    When I assemble the architecture graph\n    Then node \"meta-agent\" has version keys \"overview\", \"mvp\", \"v1\"\n    And each version includes content, progress, and status\n\n  Scenario: Enrich nodes with their feature specs\n    Given a component node \"worker\"\n    And a feature file \"mvp-task-execution.feature\" for node \"worker\" version \"mvp\"\n    When I assemble the architecture graph\n    Then node \"worker\" has features under version \"mvp\"\n    And the feature includes filename and title\n\n  Scenario: Exclude containment edges from relationship output\n    Given a \"CONTAINS\" edge from \"supervisor-layer\" to \"supervisor\"\n    And a \"CONTROLS\" edge from \"supervisor\" to \"meta-agent\"\n    When I assemble the architecture graph\n    Then the edges list includes the \"CONTROLS\" edge\n    And the edges list does not include the \"CONTAINS\" edge\n\n  Scenario: Report accurate statistics\n    Given the database contains 3 nodes, 2 edges, 4 versions, and 1 feature\n    When I assemble the architecture graph\n    Then the stats report 3 total nodes\n    And the stats report 2 total edges\n    And the stats report 4 total versions\n    And the stats report 1 total feature\n\n  Scenario: Include a generation timestamp\n    When I assemble the architecture graph\n    Then the result includes a \"generated_at\" ISO timestamp\n"
          },
          {
            "filename": "mvp-component-management-commands.feature",
            "title": "Component Management Commands",
            "content": "Feature: Component Management Commands\n  As a project maintainer using OpenCode\n  I want slash commands to create, update, delete components and manage their properties\n  So that I can maintain the roadmap without editing SQL files directly\n\n  # â”€â”€ CreateComponent use case â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Create a new component with required fields\n    Given no node with id \"new-service\" exists\n    When I create a component with id \"new-service\" name \"New Service\" type \"app\" and layer \"supervisor-layer\"\n    Then the node \"new-service\" is saved with name \"New Service\" and type \"app\"\n    And a CONTAINS edge from \"supervisor-layer\" to \"new-service\" is created\n\n  Scenario: Create a component with optional description and tags\n    Given no node with id \"audit-log\" exists\n    When I create a component with id \"audit-log\" name \"Audit Log\" type \"component\" layer \"observability-dashboard\" description \"Tracks all changes\" and tags \"logging,audit\"\n    Then the node \"audit-log\" is saved with description \"Tracks all changes\"\n    And the node \"audit-log\" has tags \"logging\" and \"audit\"\n\n  Scenario: Create a component generates default version entries\n    Given no node with id \"new-service\" exists\n    When I create a component with id \"new-service\" name \"New Service\" type \"app\" and layer \"supervisor-layer\"\n    Then versions \"overview\", \"mvp\", \"v1\", \"v2\" are created for node \"new-service\"\n    And all versions have progress 0 and status \"planned\"\n\n  Scenario: Reject creating a component with a duplicate id\n    Given a component node \"supervisor\" exists\n    When I create a component with id \"supervisor\" name \"Duplicate\" type \"app\" and layer \"supervisor-layer\"\n    Then the create operation fails with error \"already exists\"\n\n  Scenario: Reject creating a component with an invalid type\n    When I create a component with id \"bad-type\" name \"Bad\" type \"invalid\" and layer \"supervisor-layer\"\n    Then the create operation fails with error \"Invalid node type\"\n\n  # â”€â”€ DeleteComponent use case â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Delete an existing component and its related data\n    Given a component node \"doomed\" exists\n    And a version \"mvp\" for component \"doomed\" with progress 0\n    And a feature for component \"doomed\"\n    And a \"CONTAINS\" edge from \"supervisor-layer\" to \"doomed\"\n    When I delete the component \"doomed\"\n    Then the node \"doomed\" is removed\n    And all versions for \"doomed\" are removed\n    And all features for \"doomed\" are removed\n    And all edges referencing \"doomed\" are removed\n\n  Scenario: Reject deleting a nonexistent component\n    Given no node with id \"ghost\" exists\n    When I delete the component \"ghost\"\n    Then the delete operation fails with error \"Node not found\"\n\n  # â”€â”€ Publish workflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Publish rebuilds data and exports JSON\n    Given a database with architecture data\n    When I run the publish workflow\n    Then the export produces a JSON file\n    And the exported data contains the latest node data\n\n  # â”€â”€ Individual command files removed in favour of AGENTS.md â”€â”€â”€â”€â”€\n\n  Scenario: Individual component command files have been removed\n    Given the project has an .opencode/commands directory\n    Then no file \"component-create.md\" exists in .opencode/commands\n    And no file \"component-delete.md\" exists in .opencode/commands\n    And no file \"component-update.md\" exists in .opencode/commands\n    And no file \"component-publish.md\" exists in .opencode/commands\n\n  Scenario: Only bdd and ship command files remain\n    Given the project has an .opencode/commands directory\n    Then only command files \"bdd.md\" and \"ship.md\" exist\n\n  # â”€â”€ AGENTS.md documents all API endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: AGENTS.md documents the REST API endpoints\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"REST API\"\n    And the file \"AGENTS.md\" contains \"POST\" \"/api/components\"\n    And the file \"AGENTS.md\" contains \"DELETE\" \"/api/components\"\n    And the file \"AGENTS.md\" contains \"GET\" \"/api/architecture\"\n    And the file \"AGENTS.md\" contains \"GET\" \"/api/health\"\n\n  Scenario: AGENTS.md contains curl examples for API usage\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"curl\"\n    And the file \"AGENTS.md\" contains \"https://roadmap-5vvp.onrender.com\"\n\n  # â”€â”€ Remaining command files use adapter layer, not raw DB access â”€\n\n  Scenario: Remaining command files must not contain raw sqlite3 CLI references\n    Given the project has an .opencode/commands directory\n    Then no command file contains a raw \"sqlite3\" CLI invocation\n\n  # â”€â”€ CLI adapter scripts exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: CLI adapter scripts exist for component management\n    Given the project source directory\n    Then a CLI adapter \"component-create.ts\" exists in src/adapters/cli\n    And a CLI adapter \"component-delete.ts\" exists in src/adapters/cli\n    And a CLI adapter \"export.ts\" exists in src/adapters/cli\n"
          },
          {
            "filename": "mvp-component-version-state.feature",
            "title": "Component Version State",
            "content": "Feature: Component Version State\n  As a user viewing the roadmap\n  I want each component to show its current version as a derived state\n  So that I can see at a glance whether a component is Concept, MVP, or a released version\n\n  Background:\n    Given an architecture graph with nodes and versions\n\n  # â”€â”€â”€ Domain: Node entity gains current_version â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Node with no current_version is Concept\n    Given a node with id \"future-thing\" and no current_version\n    Then the node display state should be \"Concept\"\n\n  Scenario: Node with version less than 1.0.0 is MVP\n    Given a node with id \"early-thing\" and current_version \"0.3.0\"\n    Then the node display state should be \"MVP\"\n    And the node display version should be \"0.3.0\"\n\n  Scenario: Node with version 1.0.0 or above shows major version\n    Given a node with id \"shipped-thing\" and current_version \"1.2.0\"\n    Then the node display state should be \"v1\"\n    And the node display version should be \"1.2.0\"\n\n  Scenario: Node with version 2.0.0 shows v2\n    Given a node with id \"mature-thing\" and current_version \"2.0.0\"\n    Then the node display state should be \"v2\"\n    And the node display version should be \"2.0.0\"\n\n  Scenario: Node type includes app for top-level services\n    Given a node with id \"my-app\" and type \"app\"\n    Then the node type should be \"app\"\n    And the node should be valid\n\n  # â”€â”€â”€ Domain: Version entity accepts flexible version tags â”€\n\n  Scenario: Version entity accepts arbitrary version strings\n    Given a version with version tag \"v3\"\n    Then the version should be valid\n\n  Scenario: Version entity still accepts overview tag\n    Given a version with version tag \"overview\"\n    Then the version should be valid\n\n  # â”€â”€â”€ Domain: Feature entity accepts flexible version strings â”€\n\n  Scenario: Feature derives version from v3 filename prefix\n    Given a feature file named \"v3-advanced-thing.feature\"\n    Then the derived version should be \"v3\"\n\n  Scenario: Feature still derives mvp from unprefixed filename\n    Given a feature file named \"basic-thing.feature\"\n    Then the derived version should be \"mvp\"\n\n  # â”€â”€â”€ Use Case: GetArchitecture includes version state â”€â”€â”€â”€\n\n  Scenario: Exported architecture includes current_version and display state\n    Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n    When I assemble the architecture\n    Then the enriched node \"roadmap\" should have current_version \"0.7.5\"\n    And the enriched node \"roadmap\" should have display_state \"MVP\"\n\n  Scenario: Exported architecture includes current_version null for Concept\n    Given a node \"future-thing\" with no current_version exists in the database\n    When I assemble the architecture\n    Then the enriched node \"future-thing\" should have display_state \"Concept\"\n\n  # â”€â”€â”€ Schema: node_versions accepts flexible version tags â”€\n\n  Scenario: Database accepts version tag beyond mvp/v1/v2\n    Given a node \"test-node\" exists in the database\n    When I save a version with tag \"v3\" for node \"test-node\"\n    Then the version should be persisted successfully\n\n  Scenario: Database stores current_version on nodes\n    Given a node \"test-node\" with current_version \"1.0.0\" is saved\n    When I retrieve the node \"test-node\"\n    Then the node current_version should be \"1.0.0\"\n"
          },
          {
            "filename": "mvp-data-persistence.feature",
            "title": "Data Persistence",
            "content": "Feature: Data Persistence\n  As the system\n  I want SQLite repositories to correctly store and retrieve architecture data\n  So that the database layer fulfils the domain contracts\n\n  Background:\n    Given a fresh in-memory SQLite database with the schema loaded\n\n  Scenario: Save and retrieve a node\n    Given a node with id \"test-comp\", name \"Test Component\", and type \"component\"\n    When I save the node via the repository\n    And I find the node by id \"test-comp\"\n    Then the retrieved node has name \"Test Component\"\n\n  Scenario: Find nodes by type\n    Given a saved layer node \"layer-1\"\n    And a saved component node \"comp-1\" in layer \"layer-1\"\n    And a saved component node \"comp-2\" in layer \"layer-1\"\n    When I find nodes by type \"component\"\n    Then I receive 2 nodes\n\n  Scenario: Find nodes by layer\n    Given a saved layer node \"layer-1\"\n    And a saved component node \"comp-a\" in layer \"layer-1\"\n    And a saved component node \"comp-b\" in layer \"layer-1\"\n    When I find nodes by layer \"layer-1\"\n    Then I receive 2 nodes\n\n  Scenario: Check node existence\n    Given a saved component node \"exists-node\"\n    When I check if node \"exists-node\" exists\n    Then the result is true\n    When I check if node \"missing-node\" exists\n    Then the result is false\n\n  Scenario: Delete a node\n    Given a saved component node \"to-delete\"\n    When I delete node \"to-delete\"\n    And I find the node by id \"to-delete\"\n    Then the retrieved node is null\n\n  Scenario: Save and retrieve edges\n    Given saved nodes \"src-node\" and \"tgt-node\"\n    And an edge from \"src-node\" to \"tgt-node\" of type \"CONTROLS\"\n    When I save the edge via the repository\n    And I find edges by source \"src-node\"\n    Then I receive 1 edge with target \"tgt-node\"\n\n  Scenario: Find relationship edges excluding containment\n    Given saved nodes \"layer-x\" and \"comp-x\" and \"comp-y\"\n    And a saved \"CONTAINS\" edge from \"layer-x\" to \"comp-x\"\n    And a saved \"DEPENDS_ON\" edge from \"comp-x\" to \"comp-y\"\n    When I find relationship edges\n    Then I receive only the \"DEPENDS_ON\" edge\n\n  Scenario: Save and retrieve versions\n    Given a saved component node \"versioned-comp\"\n    And a version \"mvp\" for node \"versioned-comp\" with progress 25 and status \"in-progress\"\n    When I save the version via the repository\n    And I find versions by node \"versioned-comp\"\n    Then I receive 1 version with progress 25\n\n  Scenario: Save version with progress\n    Given a saved component node \"progress-comp\"\n    And a version \"mvp\" for node \"progress-comp\" with progress 75 and status \"in-progress\"\n    When I save the version via the repository\n    And I find the version for node \"progress-comp\" version \"mvp\"\n    Then the version has progress 75 and status \"in-progress\"\n\n  Scenario: Save and retrieve features\n    Given a saved component node \"featured-comp\"\n    And a feature for node \"featured-comp\" version \"mvp\" with filename \"mvp-test.feature\"\n    When I save the feature via the repository\n    And I find features by node \"featured-comp\"\n    Then I receive 1 feature with filename \"mvp-test.feature\"\n\n  Scenario: Delete all features for idempotent re-seeding\n    Given a saved component node \"reseed-comp\"\n    And 3 saved features for node \"reseed-comp\"\n    When I delete all features\n    And I find features by node \"reseed-comp\"\n    Then I receive 0 features\n"
          },
          {
            "filename": "mvp-delete-feature.feature",
            "title": "Delete a single feature file via API",
            "content": "Feature: Delete a single feature file via API\n  As an API consumer\n  I want to delete a specific feature file from a component\n  So that I can remove outdated or incorrect BDD specs without affecting other features\n\n  Background:\n    Given the API server is running\n\n  # â”€â”€ Happy path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Delete an existing feature file returns 204\n    Given a component \"df-comp\" exists in the database\n    And the component \"df-comp\" has a feature \"mvp-example.feature\"\n    When I send a DELETE request to \"/api/components/df-comp/features/mvp-example.feature\"\n    Then the response status is 204\n\n  Scenario: Deleted feature is no longer returned by GET\n    Given a component \"df-verify\" exists in the database\n    And the component \"df-verify\" has a feature \"mvp-gone.feature\"\n    When I send a DELETE request to \"/api/components/df-verify/features/mvp-gone.feature\"\n    And I send a GET request to \"/api/components/df-verify/features\"\n    Then the response status is 200\n    And the response body does not include feature \"mvp-gone.feature\"\n\n  Scenario: Other features for the same component are not affected\n    Given a component \"df-multi\" exists in the database\n    And the component \"df-multi\" has a feature \"mvp-keep.feature\"\n    And the component \"df-multi\" has a feature \"mvp-remove.feature\"\n    When I send a DELETE request to \"/api/components/df-multi/features/mvp-remove.feature\"\n    And I send a GET request to \"/api/components/df-multi/features\"\n    Then the response status is 200\n    And the response body includes feature \"mvp-keep.feature\"\n    And the response body does not include feature \"mvp-remove.feature\"\n\n  # â”€â”€ Error cases â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Delete feature for nonexistent component returns 404\n    When I send a DELETE request to \"/api/components/ghost-comp/features/mvp-test.feature\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  Scenario: Delete nonexistent feature file returns 404\n    Given a component \"df-nofile\" exists in the database\n    When I send a DELETE request to \"/api/components/df-nofile/features/mvp-missing.feature\"\n    Then the response status is 404\n    And the response body has field \"error\"\n"
          },
          {
            "filename": "mvp-docker-render-deployment.feature",
            "title": "Docker-based Render Deployment",
            "content": "Feature: Docker-based Render Deployment\n  As an operator deploying the roadmap application to Render\n  I want the service to use a Docker runtime with sqlite3 installed\n  So that the build step can create the SQLite database\n\n  # â”€â”€ Render Blueprint (Docker) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: render.yaml specifies Docker runtime\n    Given the project source directory\n    Then a file \"render.yaml\" exists in the project\n    And the render.yaml specifies a web service\n    And the render.yaml specifies the Docker runtime\n    And the render.yaml does not specify a Node.js runtime\n\n  # â”€â”€ Dockerfile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Dockerfile exists in project root\n    Given the project source directory\n    Then a file \"Dockerfile\" exists in the project\n\n  Scenario: Dockerfile uses Node.js base image\n    Given the project source directory\n    Then the Dockerfile has a FROM instruction with a Node.js image\n\n  Scenario: Dockerfile installs sqlite3 system package\n    Given the project source directory\n    Then the Dockerfile installs sqlite3 via apt-get\n\n  Scenario: Dockerfile copies package files and installs dependencies\n    Given the project source directory\n    Then the Dockerfile copies package manifest files\n    And the Dockerfile runs npm ci\n\n  Scenario: Dockerfile copies source and builds the project\n    Given the project source directory\n    Then the Dockerfile copies the application source\n    And the Dockerfile runs the build command\n\n  Scenario: Dockerfile exposes the service port\n    Given the project source directory\n    Then the Dockerfile exposes port 3000\n\n  Scenario: Dockerfile specifies the start command\n    Given the project source directory\n    Then the Dockerfile has a CMD or ENTRYPOINT for npm start\n\n  # â”€â”€ .dockerignore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: .dockerignore exists and excludes build artifacts\n    Given the project source directory\n    Then a file \".dockerignore\" exists in the project\n    And the .dockerignore excludes \"node_modules\"\n    And the .dockerignore excludes \"dist\"\n    And the .dockerignore excludes \".git\"\n"
          },
          {
            "filename": "mvp-domain-entities.feature",
            "title": "Domain Entities",
            "content": "Feature: Domain Entities\n  As a developer\n  I want well-defined domain entities with consistent behavior\n  So that the data model is reliable across all layers\n\n  Scenario: Create a Node with required fields\n    Given node properties with id \"test-node\", name \"Test Node\", and type \"component\"\n    When I create the Node entity\n    Then the node has id \"test-node\"\n    And the node has name \"Test Node\"\n    And the node has type \"component\"\n\n  Scenario: Node parses tags from a JSON string\n    Given node properties with tags stored as JSON\n    When I create the Node entity\n    Then the node has parsed tags \"runtime\" and \"core\"\n\n  Scenario: Node accepts tags as an array\n    Given node properties with tags provided as array\n    When I create the Node entity\n    Then the node has parsed tags \"alpha\" and \"beta\"\n\n  Scenario: Node defaults optional fields to null or zero\n    Given node properties with only required fields\n    When I create the Node entity\n    Then the node layer is null\n    And the node color is null\n    And the node icon is null\n    And the node description is null\n    And the node tags are an empty array\n    And the node sort_order is 0\n\n  Scenario: Node identifies itself as a layer\n    Given node properties with type \"layer\"\n    When I create the Node entity\n    Then the node reports it is a layer\n\n  Scenario: Create an Edge with required fields\n    Given edge properties with source \"a\", target \"b\", and type \"CONTROLS\"\n    When I create the Edge entity\n    Then the edge has source_id \"a\" and target_id \"b\"\n    And the edge has type \"CONTROLS\"\n\n  Scenario: Edge identifies containment relationships\n    Given edge properties with type \"CONTAINS\"\n    When I create the Edge entity\n    Then the edge reports it is a containment edge\n\n  Scenario: Edge identifies non-containment relationships\n    Given edge properties with type \"DEPENDS_ON\"\n    When I create the Edge entity\n    Then the edge reports it is not a containment edge\n\n  Scenario: Create a Version with defaults\n    Given version properties with node_id \"comp-1\" and version \"mvp\"\n    When I create the Version entity\n    Then the version progress is 0\n    And the version status is \"planned\"\n    And the version content is null\n\n  Scenario: Version identifies its status\n    Given a version with status \"complete\"\n    When I check the version status\n    Then isComplete returns true\n    And isInProgress returns false\n\n  Scenario: Version identifies in-progress status\n    Given a version with status \"in-progress\"\n    When I check the version status\n    Then isInProgress returns true\n    And isComplete returns false\n\n  Scenario: Feature derives version from filename prefix\n    Then version for filename \"mvp-basic.feature\" is \"mvp\"\n    And version for filename \"v1-advanced.feature\" is \"v1\"\n    And version for filename \"v2-future.feature\" is \"v2\"\n    And version for filename \"other.feature\" is \"mvp\"\n\n  Scenario: Feature extracts title from Gherkin content\n    Then the title extracted from a Feature line is \"My Cool Feature\"\n    And the title falls back to filename \"fallback.feature\" giving \"fallback\"\n"
          },
          {
            "filename": "mvp-drizzle-migration.feature",
            "title": "Drizzle ORM Migration",
            "content": "Feature: Drizzle ORM Migration\n  As a project maintainer\n  I want the infrastructure layer to use Drizzle ORM instead of raw SQL\n  So that the database layer is type-safe, schema is code, and progress survives rebuilds\n\n  The migration replaces hand-written better-sqlite3 queries with Drizzle's\n  type-safe query builder. Domain entities, repository interfaces, and use cases\n  are unchanged â€” only the infrastructure implementations change.\n\n  # â”€â”€ Schema as Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Drizzle schema defines all four tables\n    Given the Drizzle schema module\n    Then it should export a nodes table definition\n    And it should export an edges table definition\n    And it should export a nodeVersions table definition\n    And it should export a features table definition\n\n  Scenario: Nodes table has correct columns\n    Given the Drizzle schema nodes table\n    Then it should have a text primary key column \"id\"\n    And it should have a text column \"name\" that is not null\n    And it should have a text column \"type\" that is not null\n    And it should have optional text columns \"layer\", \"color\", \"icon\", \"description\"\n    And it should have an integer column \"sort_order\" defaulting to 0\n\n  Scenario: Node versions table preserves progress on content upsert\n    Given a Drizzle database with schema applied\n    And a node \"test-comp\" exists\n    And a version \"mvp\" for \"test-comp\" with progress 75 and status \"in-progress\"\n    When I upsert version \"mvp\" for \"test-comp\" with new content \"Updated\" via Drizzle\n    Then the version should have content \"Updated\"\n    And the version should have progress 75\n    And the version should have status \"in-progress\"\n\n  # â”€â”€ Repository Parity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Drizzle node repository implements INodeRepository\n    Given a Drizzle database with schema applied\n    When I save a node via the Drizzle repository\n    Then I can retrieve it by id\n    And I can find it by type\n    And I can find it by layer\n    And I can check it exists\n    And I can delete it\n\n  Scenario: Drizzle edge repository implements IEdgeRepository\n    Given a Drizzle database with schema applied\n    And two nodes exist for edge testing\n    When I save an edge via the Drizzle repository\n    Then I can retrieve it by source\n    And I can retrieve it by target\n    And I can retrieve relationships excluding CONTAINS\n    And I can delete it\n\n  Scenario: Drizzle version repository implements IVersionRepository\n    Given a Drizzle database with schema applied\n    And a node \"comp-1\" exists\n    When I save a version via the Drizzle repository\n    Then I can retrieve versions by node\n    And I can retrieve a specific version by node and version tag\n    And I can update progress and status\n    And I can delete versions by node\n\n  Scenario: Drizzle feature repository implements IFeatureRepository\n    Given a Drizzle database with schema applied\n    And a node \"comp-1\" exists\n    When I save a feature via the Drizzle repository\n    Then I can retrieve features by node\n    And I can retrieve features by node and version\n    And I can delete all features\n    And I can delete features by node\n\n  # â”€â”€ Progress Persistence Through Rebuild â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progress update survives database rebuild via Drizzle upsert\n    Given a Drizzle database with schema and seed data\n    When I update progress for \"roadmap\" version \"mvp\" to 100 with status \"complete\" via Drizzle\n    And I re-run the seed data via Drizzle upsert\n    Then the version \"mvp\" for \"roadmap\" should have progress 100\n    And the version \"mvp\" for \"roadmap\" should have status \"complete\"\n\n  Scenario: Seed content updates propagate while preserving progress\n    Given a Drizzle database with schema and seed data\n    When I update progress for \"roadmap\" version \"mvp\" to 80 with status \"in-progress\" via Drizzle\n    And I re-seed with updated content for \"roadmap\" version \"mvp\"\n    Then the version \"mvp\" for \"roadmap\" should have the updated content\n    And the version \"mvp\" for \"roadmap\" should have progress 80\n    And the version \"mvp\" for \"roadmap\" should have status \"in-progress\"\n\n  Scenario: Build script does not delete the database\n    Given the package.json build scripts\n    Then the build:db script should not contain \"rm -f\"\n    And the build:db script should not contain \"rm db/\"\n"
          },
          {
            "filename": "mvp-feature-seeding.feature",
            "title": "Feature Seeding",
            "content": "Feature: Feature Seeding\n  As a specification author\n  I want feature files on disk to be seeded into the database\n  So that Gherkin specs appear alongside their component in the architecture view\n\n  Background:\n    Given a database with architecture data\n\n  Scenario: Seed a feature file for an existing component\n    Given a component node \"sanitiser\" exists\n    And a feature file \"mvp-sanitiser.feature\" with content:\n      \"\"\"\n      Feature: Sanitiser MVP\n        Scenario: Block injection\n          Given a response containing role tags\n          When the sanitiser processes it\n          Then the injection is blocked\n      \"\"\"\n    When I seed the feature files\n    Then the feature for node \"sanitiser\" is saved with version \"mvp\"\n    And the feature title is \"Sanitiser MVP\"\n\n  Scenario: Derive version from filename prefix\n    Given a component node \"worker\" exists\n    And a feature file \"v1-advanced-tools.feature\" with content:\n      \"\"\"\n      Feature: Advanced tool support\n      \"\"\"\n    When I seed the feature files\n    Then the feature is saved with version \"v1\"\n\n  Scenario: Default to mvp version when no prefix matches\n    Given a component node \"worker\" exists\n    And a feature file \"basic-execution.feature\" with content:\n      \"\"\"\n      Feature: Basic execution\n      \"\"\"\n    When I seed the feature files\n    Then the feature is saved with version \"mvp\"\n\n  Scenario: Skip feature files for unknown nodes\n    Given no node with id \"nonexistent\" exists\n    And a feature file targeting node \"nonexistent\"\n    When I seed the feature files\n    Then the feature is skipped\n    And the result reports 0 seeded and 1 skipped\n\n  Scenario: Clear existing features before re-seeding\n    Given a component node \"sanitiser\" exists\n    And the database already has features for node \"sanitiser\"\n    When I seed the feature files\n    Then all previous features are deleted before new ones are inserted\n\n  Scenario: Extract title from Gherkin Feature line\n    Given a component node \"worker\" exists\n    And a feature file \"mvp-task-execution.feature\" with content:\n      \"\"\"\n      Feature: Task execution under constraints\n        Scenario: Execute with tools\n      \"\"\"\n    When I seed the feature files\n    Then the feature title is \"Task execution under constraints\"\n\n  Scenario: Fall back to filename when no Feature line exists\n    Given a component node \"worker\" exists\n    And a feature file \"mvp-notes.feature\" with content:\n      \"\"\"\n      Some notes without a Feature line\n      \"\"\"\n    When I seed the feature files\n    Then the feature title is \"mvp-notes\"\n"
          },
          {
            "filename": "mvp-package-version-sync.feature",
            "title": "Sync roadmap current_version from package.json",
            "content": "Feature: Sync roadmap current_version from package.json\n  As a project maintainer\n  I want the roadmap component's current_version to be read from package.json\n  So that bumping package.json is the single source of truth for the project version\n\n  The roadmap node is the self-tracking component. Its current_version\n  drives derived progress for all its version phases (MVP, v1, v2).\n  Instead of hardcoding the version in seed.sql, the system should\n  read it from package.json at assembly time.\n\n  Rule: GetArchitecture accepts an optional package version\n\n    Background:\n      Given an architecture graph with nodes and versions\n\n    Scenario: Package version overrides roadmap node's current_version\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the enriched node \"roadmap\" should have current_version \"1.0.0\"\n\n    Scenario: Package version updates derived progress for roadmap\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the version \"mvp\" for node \"roadmap\" should have progress 100\n      And the version \"mvp\" for node \"roadmap\" should have status \"complete\"\n\n    Scenario: Other nodes are not affected by the package version\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a node \"worker\" with current_version \"0.3.0\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"worker\"\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the version \"mvp\" for node \"worker\" should have progress 30\n\n    Scenario: No package version preserves database value\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      When I assemble the architecture\n      Then the enriched node \"roadmap\" should have current_version \"0.7.5\"\n      And the version \"mvp\" for node \"roadmap\" should have progress 75\n\n    Scenario: Package version applies to roadmap in progression tree\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the enriched node \"roadmap\" should have display_state \"v1\"\n\n    Scenario: Null package version treated as absent\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      And the package version is null\n      When I assemble the architecture with the package version\n      Then the enriched node \"roadmap\" should have current_version \"0.7.5\"\n\n  Rule: seed.sql no longer hardcodes roadmap current_version\n\n    Scenario: seed.sql roadmap node has no hardcoded current_version\n      Given the project source directory\n      Then the roadmap node in seed.sql should not have a hardcoded current_version\n\n  Rule: CLI and API adapters read from package.json\n\n    Scenario: Export adapter reads version from package.json\n      Given the project source directory\n      Then the file \"src/adapters/cli/export.ts\" contains \"package.json\"\n\n    Scenario: API adapter reads version from package.json\n      Given the project source directory\n      Then the file \"src/adapters/api/start.ts\" contains \"package.json\"\n"
          },
          {
            "filename": "mvp-progression-click-dialog.feature",
            "title": "Progression tree click-to-open component dialog",
            "content": "Feature: Progression tree click-to-open component dialog\n  As a user viewing the progression tree\n  I want to click a component node to see its full details in a centered dialog\n  So that I can explore version specs and feature files without leaving the graph view\n\n  # â”€â”€â”€ Dialog structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Web view contains a dialog overlay container\n    Given the web view HTML\n    Then it should contain a dialog overlay element with class \"dialog-overlay\"\n    And the dialog overlay should be hidden by default\n\n  Scenario: Dialog has a close button\n    Given the web view HTML\n    Then the dialog should contain a close button\n\n  # â”€â”€â”€ Click replaces hover â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progression tree uses click events instead of hover for node details\n    Given the web view HTML\n    Then the cytoscape node event should use \"click\" not \"mouseover\"\n    And there should be no \"mouseover\" handler for showing node details\n\n  # â”€â”€â”€ Dialog content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Dialog renders component name and description\n    Given the web view HTML\n    Then the dialog render function should display the node name\n    And the dialog render function should display the node description\n\n  Scenario: Dialog renders version strip with MVP, v1, v2 tabs\n    Given the web view HTML\n    Then the dialog render function should include a version strip\n    And the version strip should support \"mvp\" versions\n    And the version strip should support \"v1\" versions\n    And the version strip should support \"v2\" versions\n\n  Scenario: Dialog renders version content when a version tab is selected\n    Given the web view HTML\n    Then the dialog render function should include version content display\n\n  Scenario: Dialog renders feature files section\n    Given the web view HTML\n    Then the dialog render function should include a features section\n\n  Scenario: Dialog renders progress badge\n    Given the web view HTML\n    Then the dialog render function should include a progress badge\n\n  # â”€â”€â”€ Dialog dismiss behavior â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Dialog can be closed with Escape key\n    Given the web view HTML\n    Then there should be a keydown listener for \"Escape\" to close the dialog\n\n  Scenario: Dialog can be closed by clicking the overlay background\n    Given the web view HTML\n    Then clicking the overlay background should close the dialog\n"
          },
          {
            "filename": "mvp-progression-tree-design-update.feature",
            "title": "Progression tree design update â€” hexagonal nodes and full-width layout",
            "content": "Feature: Progression tree design update â€” hexagonal nodes and full-width layout\n  As a user viewing the roadmap progression tree\n  I want component nodes rendered as hexagons instead of rectangles\n  And the tree to fill the available width without manual zoom controls\n  So that the visual design feels polished and the tree is always fully visible\n\n  # â”€â”€â”€ Hexagonal node shape â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progression tree nodes use hexagonal shape\n    Given the web view HTML\n    Then the cytoscape node shape should be \"hexagon\" not \"roundrectangle\"\n\n  Scenario: Hexagonal nodes have adequate dimensions for labels\n    Given the web view HTML\n    Then the cytoscape node width should be at least 120 pixels\n    And the cytoscape node height should be at least 120 pixels\n\n  # â”€â”€â”€ Zoom removal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: User zooming is disabled on the progression tree\n    Given the web view HTML\n    Then userZoomingEnabled should be false in the cytoscape config\n\n  Scenario: User panning is disabled on the progression tree\n    Given the web view HTML\n    Then userPanningEnabled should be false in the cytoscape config\n\n  # â”€â”€â”€ Full-width fit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progression tree fits the container width after rendering\n    Given the web view HTML\n    Then the cytoscape instance should call fit after layout completes\n\n  Scenario: Progression tree re-fits on window resize\n    Given the web view HTML\n    Then there should be a resize listener that calls fit on the cytoscape instance\n"
          },
          {
            "filename": "mvp-progression-tree.feature",
            "title": "Progression Tree",
            "content": "Feature: Progression Tree\n  As a user viewing the roadmap\n  I want a game-style progression tree showing apps and their dependencies\n  So that I can see what needs to be built to unlock the next app\n\n  Background:\n    Given an architecture graph with app-type nodes and dependency edges\n\n  # â”€â”€â”€ Data Model: App-type filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Only app-type nodes appear in the progression tree\n    Given nodes of types \"app\", \"component\", \"layer\", \"external\", \"phase\"\n    When I filter for progression tree nodes\n    Then only nodes with type \"app\" should be included\n\n  Scenario: App nodes have dependency edges between them\n    Given app node \"supervisor\" depends on \"state-store\"\n    When I retrieve the dependency graph for apps\n    Then there should be a DEPENDS_ON edge from \"supervisor\" to \"state-store\"\n\n  # â”€â”€â”€ Use Case: GetArchitecture provides progression data â”€\n\n  Scenario: Architecture export includes progression tree data\n    Given app nodes with DEPENDS_ON edges exist\n    When I assemble the architecture\n    Then the result should include a progression_tree section\n    And the progression_tree should contain only app-type nodes\n    And the progression_tree should contain DEPENDS_ON edges between apps\n\n  Scenario: Progression tree nodes include version state\n    Given app node \"supervisor\" with current_version \"0.1.0\"\n    And app node \"state-store\" with no current_version\n    When I assemble the architecture\n    Then progression node \"supervisor\" should have display_state \"MVP\"\n    And progression node \"state-store\" should have display_state \"Concept\"\n\n  # â”€â”€â”€ Visual States â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Concept nodes are visually locked\n    Given an app node with no current_version\n    Then its visual state should be \"locked\"\n\n  Scenario: MVP nodes are visually in-progress\n    Given an app node with current_version \"0.2.0\"\n    Then its visual state should be \"in-progress\"\n\n  Scenario: Released nodes are visually complete\n    Given an app node with current_version \"1.0.0\"\n    Then its visual state should be \"complete\"\n\n  # â”€â”€â”€ Dependency Ordering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Nodes with no dependencies are root nodes\n    Given app node \"state-store\" with no dependencies\n    When I compute the tree layout\n    Then \"state-store\" should be at the top level\n\n  Scenario: Dependent nodes appear below their dependencies\n    Given app node \"supervisor\" depends on \"state-store\"\n    When I compute the tree layout\n    Then \"supervisor\" should appear below \"state-store\"\n\n  # â”€â”€â”€ Tab Structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Web view has two tabs\n    Given the web view is loaded\n    Then there should be a \"Progression\" tab\n    And there should be an \"Architecture\" tab\n    And the \"Progression\" tab should be active by default\n"
          },
          {
            "filename": "mvp-quality-checks-alignment.feature",
            "title": "Quality Checks Alignment",
            "content": "Feature: Quality Checks Alignment\n  As a developer on the roadmap project\n  I want the same quality gates as the open-bot repository\n  So that code quality standards are consistent across all projects\n\n  # â”€â”€â”€ Pre-commit pipeline completeness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Pre-commit pipeline includes coverage enforcement\n    Given the pre-commit script in package.json\n    Then it should include the \"test:coverage\" step\n    And it should not include a bare \"test:unit\" step without coverage\n\n  Scenario: Pre-commit pipeline includes BDD feature tests\n    Given the pre-commit script in package.json\n    Then it should include the \"test:features\" step\n\n  Scenario: Pre-commit pipeline runs 7 stages in order\n    Given the pre-commit script in package.json\n    Then it should run these stages in order:\n      | stage              |\n      | check:code-quality |\n      | lint               |\n      | format:check       |\n      | typecheck          |\n      | build:ts           |\n      | test:coverage      |\n      | test:features      |\n\n  # â”€â”€â”€ Coverage thresholds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Coverage thresholds are set at 90%\n    Given the vitest config file\n    Then the coverage thresholds should be:\n      | metric     | value |\n      | statements | 90    |\n      | branches   | 90    |\n      | functions  | 90    |\n      | lines      | 90    |\n\n  Scenario: Coverage excludes CLI adapter entry points\n    Given the vitest config file\n    Then the coverage exclude list should contain \"src/adapters/cli/**\"\n\n  # â”€â”€â”€ Code quality script: BDD feature coverage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script checks feature files exist\n    Given the code quality script\n    Then it should check that \"features/\" directory contains .feature files\n\n  Scenario: Code quality script checks every feature has scenarios\n    Given the code quality script\n    Then it should verify each feature file has at least one Scenario\n\n  Scenario: Code quality script runs cucumber dry-run\n    Given the code quality script\n    Then it should run a cucumber-js dry-run to detect undefined steps\n\n  Scenario: Code quality script detects orphaned step definitions\n    Given the code quality script\n    Then it should check for orphaned step definitions via usage report\n\n  # â”€â”€â”€ Code quality script: barrel bypass detection â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script detects direct imports bypassing barrels\n    Given the code quality script\n    Then it should check for direct imports that bypass barrel exports in source\n\n  # â”€â”€â”€ Code quality script: domain error checking â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script checks domain layer uses domain-specific errors\n    Given the code quality script\n    Then it should check that the domain layer does not use \"throw new Error(\"\n\n  # â”€â”€â”€ Code quality script: enhanced dead code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script runs ESLint unused-vars check\n    Given the code quality script\n    Then it should count ESLint \"@typescript-eslint/no-unused-vars\" violations\n\n  # â”€â”€â”€ Knip: unused exports and dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Knip is installed as a devDependency\n    Given the package.json file\n    Then \"knip\" should be in devDependencies\n\n  Scenario: Knip config exists with correct entry points\n    Given the knip config file\n    Then it should specify entry points including \"src/adapters/cli/*.ts\"\n    And it should specify project files including \"src/**/*.ts\"\n\n  Scenario: Knip has an npm script\n    Given the package.json file\n    Then there should be a \"check:knip\" npm script\n\n  Scenario: Code quality script runs knip\n    Given the code quality script\n    Then it should invoke knip to check for unused exports and dependencies\n\n  # â”€â”€â”€ dependency-cruiser: architectural boundaries â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: dependency-cruiser is installed as a devDependency\n    Given the package.json file\n    Then \"dependency-cruiser\" should be in devDependencies\n\n  Scenario: dependency-cruiser config enforces Clean Architecture\n    Given the dependency-cruiser config file\n    Then it should have a rule preventing domain from importing infrastructure\n    And it should have a rule preventing domain from importing adapters\n    And it should have a rule detecting circular dependencies\n\n  Scenario: dependency-cruiser has an npm script\n    Given the package.json file\n    Then there should be a \"check:deps\" npm script\n\n  Scenario: Code quality script runs dependency-cruiser\n    Given the code quality script\n    Then it should invoke dependency-cruiser to validate architecture\n\n  # â”€â”€â”€ AGENTS.md completeness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: AGENTS.md documents all pre-commit stages\n    Given the AGENTS.md file\n    Then it should document \"test:coverage\" as a pre-commit stage\n    And it should document \"test:features\" as a pre-commit stage\n\n  Scenario: AGENTS.md documents the code quality checks\n    Given the AGENTS.md file\n    Then it should document at least 10 code quality script checks\n\n  Scenario: AGENTS.md documents knip and dependency-cruiser\n    Given the AGENTS.md file\n    Then it should document \"knip\" as a quality tool\n    And it should document \"dependency-cruiser\" as a quality tool\n"
          },
          {
            "filename": "mvp-remove-update-progress.feature",
            "title": "Remove manual updateProgress in favour of derived progress",
            "content": "Feature: Remove manual updateProgress in favour of derived progress\n  As a project maintainer\n  I want the manual updateProgress endpoint and use case removed\n  Because derived progress from current_version is the sole source of truth\n  And the manual write path is dead code that silently gets overridden\n\n  # â”€â”€ Use case removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: UpdateProgress use case file does not exist\n    Given the project source directory\n    Then no file \"update-progress.ts\" exists in src/use-cases\n\n  Scenario: UpdateProgress is not exported from use-cases barrel\n    Given the project source directory\n    Then the file \"src/use-cases/index.ts\" does not contain \"UpdateProgress\"\n    And the file \"src/use-cases/index.ts\" does not contain \"update-progress\"\n\n  # â”€â”€ CLI adapter removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: component-update CLI adapter does not exist\n    Given the project source directory\n    Then no file \"component-update.ts\" exists in src/adapters/cli\n\n  # â”€â”€ API route removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: API routes file does not contain updateProgress handler\n    Given the project source directory\n    Then the file \"src/adapters/api/routes.ts\" does not contain \"handleUpdateProgress\"\n    And the file \"src/adapters/api/routes.ts\" does not contain \"UpdateProgress\"\n    And the file \"src/adapters/api/routes.ts\" does not contain \"parseProgressInput\"\n\n  Scenario: No PATCH progress route is registered\n    Given the project source directory\n    Then the file \"src/adapters/api/routes.ts\" does not contain \"PATCH\"\n\n  # â”€â”€ Repository interface cleaned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: IVersionRepository does not declare updateProgress\n    Given the project source directory\n    Then the file \"src/domain/repositories/version-repository.ts\" does not contain \"updateProgress\"\n\n  # â”€â”€ Infrastructure implementations cleaned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: SQLite version repository does not implement updateProgress\n    Given the project source directory\n    Then the file \"src/infrastructure/sqlite/version-repository.ts\" does not contain \"updateProgress\"\n\n  Scenario: Drizzle version repository does not implement updateProgress\n    Given the project source directory\n    Then the file \"src/infrastructure/drizzle/version-repository.ts\" does not contain \"updateProgress\"\n\n  # â”€â”€ OpenCode commands cleaned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: component-progress command file does not exist\n    Given the project has an .opencode/commands directory\n    Then no file \"component-progress.md\" exists in .opencode/commands\n\n  Scenario: component-update command file has been removed\n    Given the project has an .opencode/commands directory\n    Then no file \"component-update.md\" exists in .opencode/commands\n\n  # â”€â”€ Derived progress remains intact â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Version entity still has deriveProgress method\n    Given the project source directory\n    Then the file \"src/domain/entities/version.ts\" contains \"deriveProgress\"\n\n  Scenario: GetArchitecture still applies derived progress\n    Given the project source directory\n    Then the file \"src/use-cases/get-architecture.ts\" contains \"applyDerivedProgress\"\n\n  # â”€â”€ Documentation updated â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: README does not reference update-progress\n    Given the project README file\n    Then the README does not contain \"update-progress\"\n\n  Scenario: AGENTS.md does not reference update-progress\n    Given the project source directory\n    Then the file \"AGENTS.md\" does not contain \"update-progress\"\n"
          },
          {
            "filename": "mvp-render-api-commands.feature",
            "title": "Render API Commands and README Update",
            "content": "Feature: Render API Commands and README Update\n  As a project maintainer\n  I want the OpenCode commands to use the production Render API URL\n  And the README to reflect the live Render deployment\n  So that commands work against the deployed service and documentation is accurate\n\n  # â”€â”€ API documentation uses Render production URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: AGENTS.md documents API with Render production URL\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"https://roadmap-5vvp.onrender.com\"\n\n  Scenario: AGENTS.md curl examples use the Render production URL\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"curl\"\n    And the file \"AGENTS.md\" contains \"https://roadmap-5vvp.onrender.com/api/components\"\n\n  Scenario: Individual component command files no longer exist\n    Given the project has an .opencode/commands directory\n    Then no file \"component-create.md\" exists in .opencode/commands\n    And no file \"component-delete.md\" exists in .opencode/commands\n    And no file \"component-update.md\" exists in .opencode/commands\n    And no file \"component-publish.md\" exists in .opencode/commands\n\n  # â”€â”€ README reflects Render deployment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: README references the Render live URL\n    Given the project README file\n    Then the README contains the Render deployment URL \"https://roadmap-5vvp.onrender.com\"\n\n  Scenario: README does not reference GitHub Pages deployment\n    Given the project README file\n    Then the README does not contain \"github.io/roadmap\"\n    And the README does not contain \"GitHub Pages\"\n\n  Scenario: README does not reference the pages.yml workflow\n    Given the project README file\n    Then the README does not contain \"pages.yml\"\n\n  Scenario: README deployment section describes Render\n    Given the project README file\n    Then the README deployment section mentions \"Render\"\n\n  Scenario: README tech stack references Render instead of GitHub Actions for deployment\n    Given the project README file\n    Then the README does not contain \"CI/CD to GitHub Pages\"\n\n  # â”€â”€ GitHub Pages workflow removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: GitHub Pages workflow file does not exist\n    Given the project source directory\n    Then no file \"pages.yml\" exists in .github/workflows\n"
          },
          {
            "filename": "mvp-render-deployment.feature",
            "title": "Render Deployment",
            "content": "Feature: Render Deployment\n  As an operator deploying the roadmap application\n  I want the app configured for Render hosting\n  So that I can deploy both the API and web view as a single service\n\n  # â”€â”€ Static File Serving â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: API server serves index.html at root\n    Given the API server is running with static file serving\n    When I request the path \"/\"\n    Then the render response status is 200\n    And the render response content type contains \"text/html\"\n\n  Scenario: API server serves data.json from web directory\n    Given the API server is running with static file serving\n    When I request the path \"/data.json\"\n    Then the render response status is 200\n    And the render response content type contains \"application/json\"\n\n  Scenario: API routes still work alongside static serving\n    Given the API server is running with static file serving\n    When I request the path \"/api/health\"\n    Then the render response status is 200\n    And the render response body has field \"status\" with value \"ok\"\n\n  Scenario: Unknown static file returns 404\n    Given the API server is running with static file serving\n    When I request the path \"/nonexistent-file.xyz\"\n    Then the render response status is 404\n\n  Scenario: Path traversal attempts are rejected\n    Given the API server is running with static file serving\n    When I request the path \"/../package.json\"\n    Then the render response status is 404\n\n  # â”€â”€ Render Blueprint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: render.yaml exists with required fields\n    Given the project source directory\n    Then a file \"render.yaml\" exists in the project\n    And the render.yaml specifies a web service\n    And the render.yaml specifies the Docker runtime\n\n  # â”€â”€ Production Start â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Package.json has a start script for production\n    Given the project source directory\n    Then the package.json has a \"start\" script\n    And the start script runs the compiled server\n\n  Scenario: Server listens on PORT environment variable\n    Given the API server is running with static file serving on a dynamic port\n    When I request the path \"/api/health\"\n    Then the render response status is 200\n\n  # â”€â”€ CORS headers on static files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Static file responses include CORS headers\n    Given the API server is running with static file serving\n    When I request the path \"/data.json\"\n    Then the render response includes CORS headers\n"
          },
          {
            "filename": "mvp-rest-api.feature",
            "title": "REST API Adapter",
            "content": "Feature: REST API Adapter\n  As an LLM-powered CLI coding tool\n  I want a REST API to read and manage the architecture graph and feature files\n  So that I can traverse the component graph, understand dependencies, and manage feature files programmatically\n\n  # â”€â”€ Health Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Health endpoint returns server status\n    Given the API server is running\n    When I send a GET request to \"/api/health\"\n    Then the response status is 200\n    And the response body has field \"status\" with value \"ok\"\n\n  # â”€â”€ Architecture Graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Retrieve full architecture graph\n    Given the API server is running\n    And the database contains architecture data\n    When I send a GET request to \"/api/architecture\"\n    Then the response status is 200\n    And the response body has field \"layers\"\n    And the response body has field \"nodes\"\n    And the response body has field \"edges\"\n    And the response body has field \"progression_tree\"\n    And the response body has field \"stats\"\n\n  # â”€â”€ Component CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: List all components\n    Given the API server is running\n    And the database contains architecture data\n    When I send a GET request to \"/api/components\"\n    Then the response status is 200\n    And the response body is a non-empty array\n\n  Scenario: Get a single component by ID\n    Given the API server is running\n    And a component \"test-comp\" exists in the database\n    When I send a GET request to \"/api/components/test-comp\"\n    Then the response status is 200\n    And the response body has field \"id\" with value \"test-comp\"\n\n  Scenario: Get a nonexistent component returns 404\n    Given the API server is running\n    When I send a GET request to \"/api/components/nonexistent-comp\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  Scenario: Create a new component via POST\n    Given the API server is running\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"new-api-comp\",\"name\":\"New API Component\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n      \"\"\"\n    Then the response status is 201\n    And the response body has field \"id\" with value \"new-api-comp\"\n\n  Scenario: Create a component with duplicate ID returns 409\n    Given the API server is running\n    And a component \"dup-comp\" exists in the database\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"dup-comp\",\"name\":\"Duplicate\",\"type\":\"app\",\"layer\":\"supervisor-layer\"}\n      \"\"\"\n    Then the response status is 409\n    And the response body has field \"error\"\n\n  Scenario: Create a component with invalid type returns 400\n    Given the API server is running\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"bad-type\",\"name\":\"Bad\",\"type\":\"invalid\",\"layer\":\"supervisor-layer\"}\n      \"\"\"\n    Then the response status is 400\n    And the response body has field \"error\"\n\n  Scenario: Create a component with missing required fields returns 400\n    Given the API server is running\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"missing-name\"}\n      \"\"\"\n    Then the response status is 400\n    And the response body has field \"error\"\n\n  Scenario: Delete a component via DELETE\n    Given the API server is running\n    And a component \"delete-me\" exists in the database\n    When I send a DELETE request to \"/api/components/delete-me\"\n    Then the response status is 204\n\n  Scenario: Delete a nonexistent component returns 404\n    Given the API server is running\n    When I send a DELETE request to \"/api/components/ghost-comp\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  # â”€â”€ Feature File Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Get feature files for a component\n    Given the API server is running\n    And a component \"feat-comp\" exists in the database\n    And the component \"feat-comp\" has feature files\n    When I send a GET request to \"/api/components/feat-comp/features\"\n    Then the response status is 200\n    And the response body is a non-empty array\n\n  Scenario: Get feature files for a nonexistent component returns 404\n    Given the API server is running\n    When I send a GET request to \"/api/components/nonexistent-feat/features\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  Scenario: Upload a feature file for a component\n    Given the API server is running\n    And a component \"upload-comp\" exists in the database\n    When I send a PUT request to \"/api/components/upload-comp/features/mvp-test.feature\" with body:\n      \"\"\"\n      Feature: Test Upload\n        Scenario: A test scenario\n          Given something\n          Then something happens\n      \"\"\"\n    Then the response status is 200\n    And the response body has field \"filename\" with value \"mvp-test.feature\"\n\n  Scenario: Upload a feature file for a nonexistent component returns 404\n    Given the API server is running\n    When I send a PUT request to \"/api/components/ghost-upload/features/mvp-test.feature\" with body:\n      \"\"\"\n      Feature: Ghost Upload\n        Scenario: A test\n          Given something\n      \"\"\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  # â”€â”€ Graph Traversal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Get edges for a component (dependencies and dependents)\n    Given the API server is running\n    And a component \"graph-comp\" exists in the database\n    And the component \"graph-comp\" has edges\n    When I send a GET request to \"/api/components/graph-comp/edges\"\n    Then the response status is 200\n    And the response body has field \"inbound\"\n    And the response body has field \"outbound\"\n\n  Scenario: Get dependency tree for a component\n    Given the API server is running\n    And a component \"dep-comp\" exists in the database\n    When I send a GET request to \"/api/components/dep-comp/dependencies\"\n    Then the response status is 200\n    And the response body has field \"dependencies\"\n    And the response body has field \"dependents\"\n\n  # â”€â”€ API Server Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: API adapter script exists\n    Given the project source directory\n    Then a file \"src/adapters/api/server.ts\" exists in the project\n    And a file \"src/adapters/api/routes.ts\" exists in the project\n    And a file \"src/adapters/api/index.ts\" exists in the project\n"
          },
          {
            "filename": "mvp-roadmap-component.feature",
            "title": "Roadmap as a Component",
            "content": "Feature: Roadmap as a Component\n  As a developer of the roadmap itself\n  I want the roadmap to track itself as a component in the architecture graph\n  So that it appears in the progression tree with its own features and version\n\n  # â”€â”€â”€ Self-tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Roadmap exists as an app node\n    Given the architecture database is seeded\n    When I look up the node \"roadmap\"\n    Then it should exist with type \"app\"\n    And it should have a current_version\n    And its display state should be \"MVP\"\n\n  Scenario: Roadmap has version specs\n    Given the architecture database is seeded\n    When I retrieve versions for node \"roadmap\"\n    Then there should be an \"overview\" version\n    And there should be an \"mvp\" version\n\n  # â”€â”€â”€ Feature files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Roadmap feature files live under components/roadmap/features\n    Given feature files exist at \"components/roadmap/features/\"\n    When I seed features into the database\n    Then features should be linked to node \"roadmap\"\n\n  Scenario: Roadmap feature files are versioned correctly\n    Given a feature file \"mvp-architecture-graph-assembly.feature\" under \"components/roadmap/features/\"\n    When I seed features into the database\n    Then the feature version should be \"mvp\"\n\n  # â”€â”€â”€ Progression Tree Presence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Roadmap appears in the progression tree\n    Given the architecture is assembled with progression data\n    When I look at the progression tree\n    Then \"roadmap\" should be present as a node\n    And it should show its current version\n\n  # â”€â”€â”€ App Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Key runtime components are classified as apps\n    Given the architecture database is seeded\n    Then the following nodes should have type \"app\":\n      | node_id                |\n      | roadmap                |\n      | supervisor             |\n      | meta-agent             |\n      | worker                 |\n      | state-store            |\n      | user-knowledge-graph   |\n      | rpg-code-graph         |\n      | live-dashboard         |\n      | mcp-proxy-meta         |\n      | mcp-proxy-worker       |\n      | sanitiser              |\n      | human-gate             |\n      | checkpointer           |\n      | context-rebuilder      |\n\n  Scenario: Internal tools and layers remain as non-app types\n    Given the architecture database is seeded\n    Then node \"observability-dashboard\" should have type \"layer\"\n    And node \"goal-queue\" should have type \"component\"\n    And node \"phase-feature\" should have type \"phase\"\n    And node \"tool-search\" should have type \"external\"\n"
          },
          {
            "filename": "mvp-version-derived-progress.feature",
            "title": "Version-Derived Progress",
            "content": "Feature: Version-Derived Progress\n  As a project manager\n  I want each version phase (MVP, v1, v2) to derive its progress from the component's current_version number\n  So that version numbers are the single source of truth for build progress\n\n  The mapping rule is:\n    - major 0 = MVP phase, progress = minor * 10 + patch\n    - major 1 = v1 phase, progress = minor * 10 + patch\n    - major 2 = v2 phase, progress = minor * 10 + patch\n    - Once a phase's major version is reached, that phase is 100%\n    - Phases beyond the current major are 0%\n    - No current_version means all phases are 0%\n\n  Examples:\n    current_version 0.5.0 -> MVP 50%, v1 0%, v2 0%\n    current_version 0.7.5 -> MVP 75%, v1 0%, v2 0%\n    current_version 1.0.0 -> MVP 100%, v1 0%, v2 0%\n    current_version 1.3.3 -> MVP 100%, v1 33%, v2 0%\n    current_version 2.7.0 -> MVP 100%, v1 100%, v2 70%\n    current_version 3.0.0 -> MVP 100%, v1 100%, v2 100%\n\n  Rule: Domain â€” Version entity derives progress from current_version\n\n    Scenario: Derive MVP progress from pre-1.0 current_version\n      Given a node with current_version \"0.5.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 50\n\n    Scenario: Derive MVP progress at zero\n      Given a node with current_version \"0.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 0\n\n    Scenario: MVP is 100% once major version reaches 1\n      Given a node with current_version \"1.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 100\n\n    Scenario: MVP stays 100% for higher major versions\n      Given a node with current_version \"2.3.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 100\n\n    Scenario: Derive v1 progress from major version 1\n      Given a node with current_version \"1.3.0\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 30\n\n    Scenario: v1 is 0% when still in MVP phase\n      Given a node with current_version \"0.8.0\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 0\n\n    Scenario: v1 is 100% once major version reaches 2\n      Given a node with current_version \"2.0.0\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 100\n\n    Scenario: Derive v2 progress from major version 2\n      Given a node with current_version \"2.7.0\"\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 70\n\n    Scenario: v2 is 0% when still in v1 phase\n      Given a node with current_version \"1.5.0\"\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 0\n\n    Scenario: v2 is 100% once major version reaches 3\n      Given a node with current_version \"3.0.0\"\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 100\n\n    Scenario: All phases are 0% when no current_version\n      Given a node with no current_version\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 0\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 0\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 0\n\n    Scenario: Derive status from progress value\n      Given a node with current_version \"0.5.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 50\n      And the derived status should be \"in-progress\"\n\n    Scenario: Status is planned when progress is 0\n      Given a node with current_version \"0.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 0\n      And the derived status should be \"planned\"\n\n    Scenario: Status is complete when progress is 100\n      Given a node with current_version \"1.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 100\n      And the derived status should be \"complete\"\n\n    Scenario: Minor digit 9 gives 90%\n      Given a node with current_version \"0.9.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 90\n\n    Scenario: Patch digit adds sub-10% precision\n      Given a node with current_version \"0.7.5\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 75\n\n    Scenario: Patch digit works across phases\n      Given a node with current_version \"1.4.5\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 45\n\n    Scenario: Overview version is unaffected by derivation\n      Given a node with current_version \"1.5.0\"\n      When I derive phase progress for version \"overview\"\n      Then the derived progress should be 0\n\n  Rule: Use Case â€” GetArchitecture uses derived progress\n\n    Background:\n      Given an architecture graph with nodes and versions\n\n    Scenario: Exported architecture uses derived progress for MVP\n      Given a node \"test-comp\" with current_version \"0.7.0\" exists in the database\n      And a version \"mvp\" with manual progress 20 exists for \"test-comp\"\n      When I assemble the architecture\n      Then the version \"mvp\" for node \"test-comp\" should have progress 70\n\n    Scenario: Exported architecture uses derived progress for v1\n      Given a node \"test-comp\" with current_version \"1.4.0\" exists in the database\n      And a version \"v1\" with manual progress 0 exists for \"test-comp\"\n      When I assemble the architecture\n      Then the version \"v1\" for node \"test-comp\" should have progress 40\n\n    Scenario: Exported architecture derives status from progress\n      Given a node \"test-comp\" with current_version \"1.0.0\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"test-comp\"\n      When I assemble the architecture\n      Then the version \"mvp\" for node \"test-comp\" should have progress 100\n      And the version \"mvp\" for node \"test-comp\" should have status \"complete\"\n\n    Scenario: Node without current_version keeps manual progress\n      Given a node \"manual-comp\" with no current_version exists in the database\n      And a version \"mvp\" with manual progress 40 exists for \"manual-comp\"\n      When I assemble the architecture\n      Then the version \"mvp\" for node \"manual-comp\" should have progress 40\n"
          }
        ],
        "v1": [
          {
            "filename": "v1-api-component-management.feature",
            "title": "API Component and Graph Management",
            "content": "@wip @v1\nFeature: API Component and Graph Management\n  As an LLM engineer using the roadmap API headlessly\n  I want comprehensive endpoints to manage components, edges, and versions\n  So that I can programmatically build and maintain the architecture graph\n  without any manual intervention or web UI interaction\n\n  The MVP API provides basic CRUD for components and simple progress updates.\n  V1 extends this with full edge management, version lifecycle, bulk operations,\n  component search, filtering, and batch mutations. Every mutation endpoint\n  validates inputs rigorously and returns structured error responses.\n  All endpoints require appropriate API key scopes (see v1-secure-api.feature).\n\n  # â”€â”€ Component CRUD (Enhanced) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Components can be created, read, updated, and deleted via the API\n\n    Scenario: Create a component with all fields\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\n          \"id\": \"full-component\",\n          \"name\": \"Full Component\",\n          \"type\": \"component\",\n          \"layer\": \"supervisor-layer\",\n          \"description\": \"A fully specified component for testing\",\n          \"tags\": [\"runtime\", \"core\", \"v1\"],\n          \"color\": \"#3498DB\",\n          \"icon\": \"server\",\n          \"sort_order\": 42\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"id\" with value \"full-component\"\n      And the response body has field \"description\"\n      And the response body has field \"tags\" containing \"runtime\"\n      And the response body has field \"color\" with value \"#3498DB\"\n      And the response body has field \"sort_order\" with value \"42\"\n\n    Scenario: Create a component with minimal fields\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"minimal-comp\",\"name\":\"Minimal\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"description\" with value null\n      And the response body has field \"tags\" as an empty array\n      And the response body has field \"sort_order\" with value \"0\"\n\n    Scenario: Create a store-type component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"new-store\",\"name\":\"New Store\",\"type\":\"store\",\"layer\":\"shared-state\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"type\" with value \"store\"\n\n    Scenario: Create an app-type component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"new-app\",\"name\":\"New App\",\"type\":\"app\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"type\" with value \"app\"\n\n    Scenario: Reject component with invalid type\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"bad-type\",\"name\":\"Bad\",\"type\":\"widget\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"type\"\n\n    Scenario: Reject component with invalid layer reference\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"bad-layer\",\"name\":\"Bad\",\"type\":\"component\",\"layer\":\"nonexistent-layer\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"layer\"\n\n    Scenario: Reject component with ID longer than 64 characters\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with an ID of 65 characters\n      Then the response status is 400\n      And the response body has field \"error\" containing \"id\"\n\n    Scenario: Reject component with empty name\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"no-name\",\"name\":\"\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"name\"\n\n  # â”€â”€ Component Update â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Components can be partially updated via PATCH\n\n    Scenario: Update component name\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"patch-comp\" exists\n      When I send a PATCH request to \"/api/components/patch-comp\" with body:\n        \"\"\"\n        {\"name\":\"Updated Name\"}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"name\" with value \"Updated Name\"\n      And the response body has field \"id\" with value \"patch-comp\"\n\n    Scenario: Update component description\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"desc-comp\" exists\n      When I send a PATCH request to \"/api/components/desc-comp\" with body:\n        \"\"\"\n        {\"description\":\"New description for the component\"}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"description\" with value \"New description for the component\"\n\n    Scenario: Update component tags\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"tag-comp\" exists with tags [\"old\"]\n      When I send a PATCH request to \"/api/components/tag-comp\" with body:\n        \"\"\"\n        {\"tags\":[\"new\",\"updated\"]}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"tags\" containing \"new\" and \"updated\"\n\n    Scenario: Update component sort_order\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"sort-comp\" exists with sort_order 10\n      When I send a PATCH request to \"/api/components/sort-comp\" with body:\n        \"\"\"\n        {\"sort_order\":99}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"sort_order\" with value \"99\"\n\n    Scenario: Update component current_version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"ver-comp\" exists with current_version null\n      When I send a PATCH request to \"/api/components/ver-comp\" with body:\n        \"\"\"\n        {\"current_version\":\"0.5.0\"}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"current_version\" with value \"0.5.0\"\n\n    Scenario: Update component current_version triggers progress recalculation\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"recalc-comp\" exists with version \"mvp\" at progress 0\n      When I send a PATCH request to \"/api/components/recalc-comp\" with body:\n        \"\"\"\n        {\"current_version\":\"0.7.5\"}\n        \"\"\"\n      Then the response status is 200\n      And the version \"mvp\" for \"recalc-comp\" now has derived progress 75\n\n    Scenario: Reject update with invalid current_version format\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-ver-comp\" exists\n      When I send a PATCH request to \"/api/components/bad-ver-comp\" with body:\n        \"\"\"\n        {\"current_version\":\"not-semver\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version\"\n\n    Scenario: Update nonexistent component returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PATCH request to \"/api/components/ghost\" with body:\n        \"\"\"\n        {\"name\":\"Ghost\"}\n        \"\"\"\n      Then the response status is 404\n\n    Scenario: Update preserves unmodified fields\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"preserve-comp\" exists with name \"Original\" and description \"Keep me\"\n      When I send a PATCH request to \"/api/components/preserve-comp\" with body:\n        \"\"\"\n        {\"name\":\"Changed\"}\n        \"\"\"\n      Then the response body has field \"name\" with value \"Changed\"\n      And the response body has field \"description\" with value \"Keep me\"\n\n  # â”€â”€ Component Listing and Filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Components can be listed with filtering and search\n\n    Scenario: List all components\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And the database contains 60 components\n      When I send a GET request to \"/api/components\"\n      Then the response status is 200\n      And the response body is an array\n      And layers are excluded from the result\n\n    Scenario: Filter components by type\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?type=store\"\n      Then the response status is 200\n      And every item in the response has type \"store\"\n\n    Scenario: Filter components by layer\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?layer=supervisor-layer\"\n      Then the response status is 200\n      And every item in the response has layer \"supervisor-layer\"\n\n    Scenario: Filter components by tag\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components with tag \"runtime\" exist\n      When I send a GET request to \"/api/components?tag=runtime\"\n      Then the response status is 200\n      And every item in the response has \"runtime\" in its tags\n\n    Scenario: Search components by name (partial match)\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?search=proxy\"\n      Then the response status is 200\n      And every item has \"proxy\" in its name (case-insensitive)\n\n    Scenario: Combine multiple filters\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?type=component&layer=supervisor-layer\"\n      Then the response status is 200\n      And every item has type \"component\" and layer \"supervisor-layer\"\n\n    Scenario: Empty filter result returns empty array\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?type=nonexistent\"\n      Then the response status is 200\n      And the response body is an empty array\n\n  # â”€â”€ Edge Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Edges can be created, read, and deleted via the API\n\n    Scenario: Create a new edge between components\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"edge-src\" and \"edge-tgt\" exist\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"edge-src\",\"target_id\":\"edge-tgt\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"source_id\" with value \"edge-src\"\n      And the response body has field \"target_id\" with value \"edge-tgt\"\n      And the response body has field \"type\" with value \"DEPENDS_ON\"\n\n    Scenario: Create an edge with label and metadata\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"meta-src\" and \"meta-tgt\" exist\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\n          \"source_id\": \"meta-src\",\n          \"target_id\": \"meta-tgt\",\n          \"type\": \"CONTROLS\",\n          \"label\": \"spawns and monitors\",\n          \"metadata\": {\"restart_policy\": \"always\", \"max_retries\": 5}\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"label\" with value \"spawns and monitors\"\n      And the response body has field \"metadata\"\n\n    Scenario: Reject edge with invalid type\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"a\",\"target_id\":\"b\",\"type\":\"INVALID_TYPE\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"type\"\n\n    Scenario: Reject edge with nonexistent source\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"real-tgt\" exists but \"fake-src\" does not\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"fake-src\",\"target_id\":\"real-tgt\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"source\"\n\n    Scenario: Reject edge with nonexistent target\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"real-src\" exists but \"fake-tgt\" does not\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"real-src\",\"target_id\":\"fake-tgt\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"target\"\n\n    Scenario: Reject duplicate edge (same source, target, type)\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And an edge from \"dup-src\" to \"dup-tgt\" with type \"DEPENDS_ON\" already exists\n      When I send a POST request to \"/api/edges\" with the same edge\n      Then the response status is 409\n      And the response body has field \"error\" containing \"already exists\"\n\n    Scenario: Reject self-referencing edge\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"self-ref\" exists\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"self-ref\",\"target_id\":\"self-ref\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"self-referencing\"\n\n    Scenario: List all edges for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"hub-comp\" has 3 inbound and 2 outbound edges\n      When I send a GET request to \"/api/components/hub-comp/edges\"\n      Then the response status is 200\n      And the response body has field \"inbound\" as an array of 3 edges\n      And the response body has field \"outbound\" as an array of 2 edges\n\n    Scenario: Filter edges by type\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/edges?type=DEPENDS_ON\"\n      Then the response status is 200\n      And every edge in the response has type \"DEPENDS_ON\"\n\n    Scenario: List all edges in the graph\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/edges\"\n      Then the response status is 200\n      And the response body is a non-empty array of edge objects\n\n    Scenario: Delete an edge\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And an edge with id 42 exists\n      When I send a DELETE request to \"/api/edges/42\"\n      Then the response status is 204\n      And the edge no longer exists\n\n    Scenario: Delete nonexistent edge returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a DELETE request to \"/api/edges/99999\"\n      Then the response status is 404\n\n  # â”€â”€ Version Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Component versions can be managed via the API\n\n    Scenario: List versions for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"ver-list\" has versions \"overview\", \"mvp\", \"v1\", \"v2\"\n      When I send a GET request to \"/api/components/ver-list/versions\"\n      Then the response status is 200\n      And the response body is an array of 4 version objects\n      And each version has fields: version, content, progress, status, updated_at\n      And each phase version (mvp, v1, v2) includes step-based progress fields:\n        | field          | description                              |\n        | total_steps    | Total Given/When/Then steps for version  |\n        | passing_steps  | Steps in passing scenarios               |\n        | step_progress  | passing_steps / total_steps * 100        |\n\n    Scenario: Get a single version for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"ver-single\" has version \"mvp\" with progress 75\n      When I send a GET request to \"/api/components/ver-single/versions/mvp\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"mvp\"\n      And the response body has field \"progress\" with value \"75\"\n\n    Scenario: Create or update version content\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"ver-upsert\" exists\n      When I send a PUT request to \"/api/components/ver-upsert/versions/v1\" with body:\n        \"\"\"\n        {\n          \"content\": \"V1 adds Neo4j storage, secure API, and feature-driven progress tracking.\",\n          \"progress\": 0,\n          \"status\": \"planned\"\n        }\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"content\"\n\n    Scenario: Delete all versions for a component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"ver-del\" has versions \"overview\", \"mvp\", \"v1\"\n      When I send a DELETE request to \"/api/components/ver-del/versions\"\n      Then the response status is 204\n      And no versions exist for \"ver-del\"\n\n    Scenario: Version progress reflects step-based calculation\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"step-ver\" has version \"v1\" with 40 total steps and 30 passing\n      When I send a GET request to \"/api/components/step-ver/versions/v1\"\n      Then the response status is 200\n      And the response body has field \"total_steps\" with value \"40\"\n      And the response body has field \"passing_steps\" with value \"30\"\n      And the response body has field \"step_progress\" with value \"75\"\n      And the response body has field \"progress\" reflecting the combined weighted value\n\n  # â”€â”€ Bulk Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Bulk operations allow efficient batch mutations\n\n    Scenario: Bulk create multiple components\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/bulk/components\" with body:\n        \"\"\"\n        {\n          \"components\": [\n            {\"id\":\"bulk-1\",\"name\":\"Bulk One\",\"type\":\"component\",\"layer\":\"supervisor-layer\"},\n            {\"id\":\"bulk-2\",\"name\":\"Bulk Two\",\"type\":\"component\",\"layer\":\"supervisor-layer\"},\n            {\"id\":\"bulk-3\",\"name\":\"Bulk Three\",\"type\":\"store\",\"layer\":\"shared-state\"}\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"created\" with value 3\n      And the response body has field \"errors\" as an empty array\n\n    Scenario: Bulk create with partial failure\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"existing-bulk\" already exists\n      When I send a POST request to \"/api/bulk/components\" with body:\n        \"\"\"\n        {\n          \"components\": [\n            {\"id\":\"new-bulk\",\"name\":\"New\",\"type\":\"component\",\"layer\":\"supervisor-layer\"},\n            {\"id\":\"existing-bulk\",\"name\":\"Dup\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n          ]\n        }\n        \"\"\"\n      Then the response status is 207\n      And the response body has field \"created\" with value 1\n      And the response body has field \"errors\" as an array of 1 error\n      And the error references \"existing-bulk\" with status 409\n\n    Scenario: Bulk create edges\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"b-src-1\", \"b-src-2\", \"b-tgt\" exist\n      When I send a POST request to \"/api/bulk/edges\" with body:\n        \"\"\"\n        {\n          \"edges\": [\n            {\"source_id\":\"b-src-1\",\"target_id\":\"b-tgt\",\"type\":\"DEPENDS_ON\"},\n            {\"source_id\":\"b-src-2\",\"target_id\":\"b-tgt\",\"type\":\"DEPENDS_ON\"}\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"created\" with value 2\n\n    Scenario: Bulk operations are limited to 100 items\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/bulk/components\" with 101 components\n      Then the response status is 400\n      And the response body has field \"error\" containing \"maximum 100\"\n\n    Scenario: Bulk delete components\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"del-1\", \"del-2\", \"del-3\" exist\n      When I send a POST request to \"/api/bulk/delete/components\" with body:\n        \"\"\"\n        {\"ids\":[\"del-1\",\"del-2\",\"del-3\"]}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"deleted\" with value 3\n\n  # â”€â”€ Layer Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Layers can be managed alongside components\n\n    Scenario: List all layers\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/layers\"\n      Then the response status is 200\n      And the response body is an array of layer objects\n      And each layer has field \"type\" with value \"layer\"\n\n    Scenario: Get a layer with its children\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And layer \"supervisor-layer\" contains 4 components\n      When I send a GET request to \"/api/layers/supervisor-layer\"\n      Then the response status is 200\n      And the response body has field \"id\" with value \"supervisor-layer\"\n      And the response body has field \"children\" as an array of 4 components\n\n    Scenario: Create a new layer\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/layers\" with body:\n        \"\"\"\n        {\"id\":\"new-layer\",\"name\":\"New Layer\",\"color\":\"#E74C3C\",\"icon\":\"layers\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"type\" with value \"layer\"\n\n    Scenario: Move a component to a different layer\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"movable-comp\" is in layer \"old-layer\"\n      When I send a PATCH request to \"/api/components/movable-comp\" with body:\n        \"\"\"\n        {\"layer\":\"new-layer\"}\n        \"\"\"\n      Then the response status is 200\n      And the CONTAINS edge from \"old-layer\" to \"movable-comp\" is removed\n      And a CONTAINS edge from \"new-layer\" to \"movable-comp\" is created\n\n  # â”€â”€ Architecture Graph Endpoint (Enhanced) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: The architecture endpoint returns the full enriched graph\n\n    Scenario: Get full architecture graph\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/architecture\"\n      Then the response status is 200\n      And the response body has field \"generated_at\" as an ISO 8601 timestamp\n      And the response body has field \"layers\" as a non-empty array\n      And the response body has field \"nodes\" as a non-empty array\n      And the response body has field \"edges\" as a non-empty array\n      And the response body has field \"progression_tree\"\n      And the response body has field \"stats\"\n\n    Scenario: Architecture stats are accurate\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/architecture\"\n      Then the stats field has \"total_nodes\" matching the actual node count\n      And the stats field has \"total_edges\" matching the actual edge count\n      And the stats field has \"total_versions\" matching the actual version count\n      And the stats field has \"total_features\" matching the actual feature count\n\n    Scenario: Enriched nodes include versions and features\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"enriched-comp\" has versions and features\n      When I send a GET request to \"/api/architecture\"\n      Then the node \"enriched-comp\" in the response has field \"versions\"\n      And the node \"enriched-comp\" has field \"features\"\n      And the node \"enriched-comp\" has field \"display_state\"\n\n    Scenario: Progression tree contains only app-type nodes\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/architecture\"\n      Then every node in the progression_tree has type \"app\"\n      And every edge in the progression_tree has type \"DEPENDS_ON\"\n\n    Scenario: Architecture response uses derived progress\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"derived-comp\" has current_version \"0.7.5\"\n      When I send a GET request to \"/api/architecture\"\n      Then the version \"mvp\" for node \"derived-comp\" has progress 75 in the response\n"
          },
          {
            "filename": "v1-api-feature-publishing.feature",
            "title": "API Feature File Publishing and Graph Traversal",
            "content": "@wip @v1\nFeature: API Feature File Publishing and Graph Traversal\n  As an LLM engineer working autonomously\n  I want API endpoints to publish, retrieve, and validate Gherkin feature files\n  with an explicit version parameter that categorises every feature\n  and traverse the architecture graph to understand component dependencies\n  So that I can headlessly manage version-tagged BDD specifications for each\n  component, and the system can calculate completion rates using total steps\n  versus passing steps per version tier\n\n  The MVP API supports basic feature upload with version derived from filename.\n  V1 makes version a mandatory part of the upload URL path, ensuring every\n  feature file is explicitly categorised under a version (mvp, v1, v2, etc.).\n  This enables precise step-level progress maths: the app counts total Given/\n  When/Then steps across all features for a version, compares them against\n  passing steps from test results, and derives a completion percentage.\n\n  V1 also adds Gherkin validation, batch publishing with required version\n  fields, version-scoped retrieval and deletion, and rich graph traversal\n  endpoints purpose-built for autonomous coding agents.\n\n  # â”€â”€ Version-Scoped Feature Upload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature uploads require an explicit version in the URL path\n\n    Scenario: Upload a feature file with explicit version in path\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"upload-comp\" exists\n      When I send a PUT request to \"/api/components/upload-comp/versions/v1/features/neo4j-storage.feature\" with body:\n        \"\"\"\n        Feature: Neo4j Storage\n          As a developer\n          I want data stored in Neo4j\n          So that graph traversals are efficient\n\n          Scenario: Save a node\n            Given an empty database\n            When I save a node with id \"test\"\n            Then the node exists in Neo4j\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"filename\" with value \"neo4j-storage.feature\"\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"title\" with value \"Neo4j Storage\"\n      And the response body has field \"node_id\" with value \"upload-comp\"\n      And the response body has field \"step_count\" with value \"3\"\n\n    Scenario: Upload a feature to the MVP version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"mvp-comp\" exists\n      When I send a PUT request to \"/api/components/mvp-comp/versions/mvp/features/basic-crud.feature\" with body:\n        \"\"\"\n        Feature: Basic CRUD\n          Scenario: Create a record\n            Given no records exist\n            When I create a record\n            Then 1 record exists\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"mvp\"\n      And the response body has field \"step_count\" with value \"3\"\n\n    Scenario: Upload a feature to the V2 version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"v2-comp\" exists\n      When I send a PUT request to \"/api/components/v2-comp/versions/v2/features/advanced-search.feature\" with body:\n        \"\"\"\n        Feature: Advanced Search\n          Scenario: Full-text search\n            Given indexed content exists\n            When I search for \"keyword\"\n            Then matching results are returned\n            And results are ranked by relevance\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v2\"\n      And the response body has field \"step_count\" with value \"4\"\n\n    Scenario: Version in path overrides any filename prefix\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"override-comp\" exists\n      When I send a PUT request to \"/api/components/override-comp/versions/v1/features/mvp-legacy-name.feature\" with body:\n        \"\"\"\n        Feature: Legacy Named Feature\n          Scenario: A scenario\n            Given a step\n            When an action\n            Then a result\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v1\"\n      And the feature is stored under version \"v1\" regardless of the \"mvp-\" filename prefix\n\n    Scenario: Reject upload with invalid version value\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-ver\" exists\n      When I send a PUT request to \"/api/components/bad-ver/versions/invalid/features/test.feature\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version\"\n      And the response body has field \"error\" containing \"mvp, v1, v2\"\n\n    Scenario: Reject upload without version in path (old MVP-style URL)\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request to \"/api/components/some-comp/features/test.feature\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version is required\"\n\n    Scenario: Upload extracts title from Feature: line\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"title-comp\" exists\n      When I send a PUT request to \"/api/components/title-comp/versions/v1/features/my-feature.feature\" with body:\n        \"\"\"\n        Feature: My Custom Title Here\n          Scenario: Something\n            Given a step\n            When an action\n            Then a result\n        \"\"\"\n      Then the response body has field \"title\" with value \"My Custom Title Here\"\n\n    Scenario: Upload to nonexistent component returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request to \"/api/components/ghost/versions/v1/features/test.feature\" with body:\n        \"\"\"\n        Feature: Ghost Upload\n          Scenario: Test\n            Given a step\n        \"\"\"\n      Then the response status is 404\n      And the response body has field \"error\" containing \"not found\"\n\n    Scenario: Upload replaces existing feature with same filename and version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"replace-comp\" has feature \"existing.feature\" under version \"v1\" with title \"Old\"\n      When I send a PUT request to \"/api/components/replace-comp/versions/v1/features/existing.feature\" with body:\n        \"\"\"\n        Feature: New Title\n          Scenario: Updated scenario\n            Given a new step\n            When a new action\n            Then a new result\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"title\" with value \"New Title\"\n      And only one feature with filename \"existing.feature\" exists for \"replace-comp\" version \"v1\"\n\n    Scenario: Same filename under different versions creates separate records\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"multi-ver\" exists\n      When I upload \"auth.feature\" to \"multi-ver\" under version \"mvp\" with 3 steps\n      And I upload \"auth.feature\" to \"multi-ver\" under version \"v1\" with 8 steps\n      Then 2 feature records exist for \"multi-ver\" with filename \"auth.feature\"\n      And the \"mvp\" version has step_count 3\n      And the \"v1\" version has step_count 8\n\n    Scenario: Upload preserves features under other versions\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"preserve-comp\" has features under versions \"mvp\" and \"v1\"\n      When I upload a new feature under version \"v2\"\n      Then the \"mvp\" and \"v1\" features are unchanged\n      And \"preserve-comp\" now has features across 3 versions\n\n    Scenario: Response includes step count breakdown\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"step-count-comp\" exists\n      When I send a PUT request to \"/api/components/step-count-comp/versions/v1/features/detailed.feature\" with body:\n        \"\"\"\n        Feature: Detailed Steps\n          Scenario: First scenario\n            Given step one\n            And step two\n            When action one\n            Then result one\n            And result two\n            But not result three\n\n          Scenario: Second scenario\n            Given step three\n            When action two\n            Then result four\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"step_count\" with value \"9\"\n      And the response body has field \"scenario_count\" with value \"2\"\n      And the response body has field \"given_count\" with value \"3\"\n      And the response body has field \"when_count\" with value \"2\"\n      And the response body has field \"then_count\" with value \"4\"\n\n  # â”€â”€ Feature File Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Uploaded feature files are validated for Gherkin syntax\n\n    Scenario: Valid Gherkin is accepted\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"valid-gherkin\" exists\n      When I send a PUT request to \"/api/components/valid-gherkin/versions/v1/features/valid.feature\" with valid Gherkin content\n      Then the response status is 200\n\n    Scenario: Feature file without Feature: keyword is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-gherkin\" exists\n      When I send a PUT request to \"/api/components/bad-gherkin/versions/v1/features/bad.feature\" with body:\n        \"\"\"\n        This is not a valid feature file.\n        It has no Feature: keyword.\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"Feature\"\n\n    Scenario: Feature file without any scenarios is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"no-scenario\" exists\n      When I send a PUT request to \"/api/components/no-scenario/versions/v1/features/empty.feature\" with body:\n        \"\"\"\n        Feature: Empty Feature\n          This feature has a description but no scenarios.\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"scenario\"\n\n    Scenario: Feature file with empty body is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"empty-body\" exists\n      When I send a PUT request to \"/api/components/empty-body/versions/v1/features/empty.feature\" with empty body\n      Then the response status is 400\n      And the response body has field \"error\" containing \"empty\"\n\n    Scenario: Feature file with scenarios but no steps is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"no-steps\" exists\n      When I send a PUT request to \"/api/components/no-steps/versions/v1/features/stepless.feature\" with body:\n        \"\"\"\n        Feature: Stepless Feature\n          Scenario: Empty scenario\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"steps\"\n\n    Scenario: Feature filename must end with .feature\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-ext\" exists\n      When I send a PUT request to \"/api/components/bad-ext/versions/v1/features/test.txt\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \".feature\"\n\n    Scenario: Feature filename must be kebab-case\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-name\" exists\n      When I send a PUT request to \"/api/components/bad-name/versions/v1/features/under_score.feature\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \"filename\"\n\n    Scenario: Validation response includes line number for parse errors\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"parse-err\" exists\n      When I send a PUT request to \"/api/components/parse-err/versions/v1/features/broken.feature\" with Gherkin containing a syntax error at line 5\n      Then the response status is 400\n      And the response body has field \"error\" containing line number information\n\n  # â”€â”€ Version-Scoped Feature Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files are retrieved scoped to their explicit version\n\n    Scenario: List all features for a component across all versions\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-list\" has 2 features under \"mvp\", 2 under \"v1\", 1 under \"v2\"\n      When I send a GET request to \"/api/components/feat-list/features\"\n      Then the response status is 200\n      And the response body is an array of 5 feature objects\n      And each object has fields: filename, version, title, content, step_count, updated_at\n\n    Scenario: List features for a specific version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-ver\" has 3 \"mvp\" features and 4 \"v1\" features\n      When I send a GET request to \"/api/components/feat-ver/versions/v1/features\"\n      Then the response status is 200\n      And the response body is an array of 4 features\n      And every feature has version \"v1\"\n\n    Scenario: Get a single feature by version and filename\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-single\" has feature \"auth.feature\" under version \"v1\"\n      When I send a GET request to \"/api/components/feat-single/versions/v1/features/auth.feature\"\n      Then the response status is 200\n      And the response body has field \"filename\" with value \"auth.feature\"\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"content\" containing the full Gherkin text\n      And the response body has field \"step_count\"\n\n    Scenario: Get feature from wrong version returns 404\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"ver-miss\" has feature \"auth.feature\" under version \"mvp\" only\n      When I send a GET request to \"/api/components/ver-miss/versions/v1/features/auth.feature\"\n      Then the response status is 404\n      And the response body has field \"error\" containing \"not found\"\n\n    Scenario: Get nonexistent feature returns 404\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-missing\" exists\n      When I send a GET request to \"/api/components/feat-missing/versions/v1/features/ghost.feature\"\n      Then the response status is 404\n\n    Scenario: Get features for nonexistent component returns 404\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components/nonexistent/versions/v1/features\"\n      Then the response status is 404\n\n    Scenario: Get raw feature content as plain text\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"raw-feat\" has feature \"raw.feature\" under version \"v1\"\n      When I send a GET request to \"/api/components/raw-feat/versions/v1/features/raw.feature\" with header \"Accept: text/plain\"\n      Then the response status is 200\n      And the response content type is \"text/plain\"\n      And the response body is the raw Gherkin text\n\n    Scenario: Feature listing includes step counts for progress maths\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"step-list\" has 3 features under version \"v1\"\n      When I send a GET request to \"/api/components/step-list/versions/v1/features\"\n      Then each feature object has field \"step_count\" as a positive integer\n      And each feature object has field \"scenario_count\" as a positive integer\n      And the response includes a \"totals\" field with:\n        | field               | description                         |\n        | total_features      | Number of features in this version  |\n        | total_scenarios     | Sum of scenarios across features    |\n        | total_steps         | Sum of all steps across features    |\n        | total_given_steps   | Sum of Given/And steps              |\n        | total_when_steps    | Sum of When steps                   |\n        | total_then_steps    | Sum of Then/But steps               |\n\n  # â”€â”€ Version-Scoped Feature Deletion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files can be deleted scoped to their version\n\n    Scenario: Delete a single feature by version and filename\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"del-feat\" has feature \"remove-me.feature\" under version \"v1\"\n      When I send a DELETE request to \"/api/components/del-feat/versions/v1/features/remove-me.feature\"\n      Then the response status is 204\n      And the feature \"remove-me.feature\" under version \"v1\" no longer exists for \"del-feat\"\n\n    Scenario: Delete all features for a specific version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"del-ver\" has 3 \"mvp\" and 2 \"v1\" features\n      When I send a DELETE request to \"/api/components/del-ver/versions/v1/features\"\n      Then the response status is 204\n      And 3 \"mvp\" features still exist for \"del-ver\"\n      And 0 \"v1\" features exist for \"del-ver\"\n\n    Scenario: Delete all features across all versions\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"del-all\" has features under \"mvp\", \"v1\", and \"v2\"\n      When I send a DELETE request to \"/api/components/del-all/features\"\n      Then the response status is 204\n      And no features exist for \"del-all\"\n\n    Scenario: Delete nonexistent feature returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a DELETE request to \"/api/components/del-feat/versions/v1/features/ghost.feature\"\n      Then the response status is 404\n\n    Scenario: Deleting features triggers progress recalculation\n      Given component \"del-recalc\" has features under version \"v1\" contributing to progress\n      When I delete all features for \"del-recalc\" version \"v1\"\n      Then the step-based progress for \"del-recalc\" version \"v1\" drops to 0 percent\n\n  # â”€â”€ Batch Feature Publishing with Explicit Version â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Batch uploads require an explicit version per feature entry\n\n    Scenario: Batch upload features for a single component and version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"batch-comp\" exists\n      When I send a POST request to \"/api/components/batch-comp/versions/v1/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"filename\": \"first.feature\",\n              \"content\": \"Feature: First\\n  Scenario: S1\\n    Given a step\\n    When an action\\n    Then a result\"\n            },\n            {\n              \"filename\": \"second.feature\",\n              \"content\": \"Feature: Second\\n  Scenario: S2\\n    Given another step\\n    Then another result\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"uploaded\" with value 2\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"total_steps\" with value \"5\"\n      And the response body has field \"errors\" as an empty array\n\n    Scenario: Batch upload with partial validation failure\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"batch-partial\" exists\n      When I send a POST request to \"/api/components/batch-partial/versions/v1/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"filename\": \"valid.feature\",\n              \"content\": \"Feature: Valid\\n  Scenario: S1\\n    Given a step\"\n            },\n            {\n              \"filename\": \"invalid.feature\",\n              \"content\": \"This is not valid Gherkin\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 207\n      And the response body has field \"uploaded\" with value 1\n      And the response body has field \"errors\" as an array of 1 error\n      And the error references \"invalid.feature\"\n\n    Scenario: Batch upload limited to 50 features\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a batch upload with 51 features\n      Then the response status is 400\n      And the response body has field \"error\" containing \"maximum 50\"\n\n    Scenario: Cross-component batch publish requires version per entry\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"cross-1\" and \"cross-2\" exist\n      When I send a POST request to \"/api/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"node_id\": \"cross-1\",\n              \"version\": \"v1\",\n              \"filename\": \"a.feature\",\n              \"content\": \"Feature: A\\n  Scenario: S\\n    Given a step\"\n            },\n            {\n              \"node_id\": \"cross-2\",\n              \"version\": \"v2\",\n              \"filename\": \"b.feature\",\n              \"content\": \"Feature: B\\n  Scenario: S\\n    Given a step\\n    Then a result\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And \"cross-1\" has feature \"a.feature\" under version \"v1\"\n      And \"cross-2\" has feature \"b.feature\" under version \"v2\"\n\n    Scenario: Cross-component batch rejects entry without version field\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"node_id\": \"some-comp\",\n              \"filename\": \"no-version.feature\",\n              \"content\": \"Feature: Missing Version\\n  Scenario: S\\n    Given a step\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version is required\"\n\n    Scenario: Batch upload triggers progress recalculation once\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"batch-recalc\" exists\n      When I batch upload 5 features to \"batch-recalc\" version \"v1\"\n      Then progress recalculation happens once (not 5 times)\n      And the step-based progress reflects all 5 features\n\n  # â”€â”€ Graph Traversal for Autonomous Coding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: LLM engineers can traverse the graph to plan implementation\n\n    Scenario: Get dependency tree for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"dep-root\" has dependencies \"dep-a\" and \"dep-b\"\n      And \"dep-a\" has dependency \"dep-c\"\n      When I send a GET request to \"/api/components/dep-root/dependencies?depth=2\"\n      Then the response status is 200\n      And the response body has field \"dependencies\" as a tree structure\n      And the tree includes \"dep-a\", \"dep-b\" at depth 1\n      And the tree includes \"dep-c\" at depth 2\n\n    Scenario: Get reverse dependencies (dependents)\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components \"consumer-1\" and \"consumer-2\" depend on \"provider\"\n      When I send a GET request to \"/api/components/provider/dependents\"\n      Then the response status is 200\n      And the response body contains \"consumer-1\" and \"consumer-2\"\n\n    Scenario: Get full component context for coding\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"context-comp\" exists with versions, features, and edges\n      When I send a GET request to \"/api/components/context-comp/context\"\n      Then the response status is 200\n      And the response body has field \"component\" with full component details\n      And the response body has field \"versions\" with all version data including step counts\n      And the response body has field \"features\" grouped by version with step counts\n      And the response body has field \"dependencies\" with outbound DEPENDS_ON edges\n      And the response body has field \"dependents\" with inbound DEPENDS_ON edges\n      And the response body has field \"layer\" with the parent layer details\n      And the response body has field \"siblings\" listing other components in the same layer\n      And the response body has field \"progress\" with per-version step-based progress\n\n    Scenario: Get implementation order via topological sort\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a dependency graph with no cycles\n      When I send a GET request to \"/api/graph/implementation-order\"\n      Then the response status is 200\n      And the response body is an array of component IDs\n      And every component appears after all its dependencies\n      And the order is a valid topological sort\n\n    Scenario: Implementation order detects cycles\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a circular dependency exists between \"cycle-a\", \"cycle-b\", \"cycle-c\"\n      When I send a GET request to \"/api/graph/implementation-order\"\n      Then the response status is 409\n      And the response body has field \"error\" containing \"cycle\"\n      And the response body has field \"cycle\" listing the involved components\n\n    Scenario: Get components by completion status\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/graph/components-by-status?version=mvp\"\n      Then the response status is 200\n      And the response body has field \"complete\" as an array of components with 100% step coverage\n      And the response body has field \"in_progress\" as an array with partial step coverage\n      And the response body has field \"planned\" as an array with 0% step coverage\n\n    Scenario: Get next implementable components\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/graph/next-implementable?version=mvp\"\n      Then the response status is 200\n      And the response body is an array of component objects\n      And every component has all its dependencies at 100% step coverage for \"mvp\"\n      And every component itself has step coverage below 100% for \"mvp\"\n\n    Scenario: Get shortest path between two components\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components \"path-start\" and \"path-end\" are connected via intermediate nodes\n      When I send a GET request to \"/api/graph/path?from=path-start&to=path-end\"\n      Then the response status is 200\n      And the response body has field \"path\" as an array of nodes\n      And the response body has field \"edges\" describing each hop\n      And the path is the shortest available route\n\n    Scenario: Path between unconnected components returns empty\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components \"island-1\" and \"island-2\" have no connecting path\n      When I send a GET request to \"/api/graph/path?from=island-1&to=island-2\"\n      Then the response status is 200\n      And the response body has field \"path\" as an empty array\n\n    Scenario: Get component neighbourhood\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"center\" has edges to and from multiple components\n      When I send a GET request to \"/api/components/center/neighbourhood?hops=2\"\n      Then the response status is 200\n      And the response body has field \"nodes\" with all components within 2 hops\n      And the response body has field \"edges\" with all edges between those nodes\n      And the response includes the edge types and directions\n\n    Scenario: Get layer overview for planning\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/graph/layer-overview\"\n      Then the response status is 200\n      And the response body is an array of layer summaries\n      And each summary has field \"layer_id\"\n      And each summary has field \"total_components\"\n      And each summary has field \"completed_mvp\" as a count\n      And each summary has field \"completed_v1\" as a count\n      And each summary has field \"overall_progress\" as a percentage\n\n  # â”€â”€ Feature Seeding from Filesystem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files seeded from the filesystem use filename prefix for version\n\n    Scenario: Trigger feature re-seed via API\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And feature files exist in the \"components/\" directory tree\n      When I send a POST request to \"/api/admin/seed-features\"\n      Then the response status is 200\n      And the response body has field \"seeded\" with a positive integer\n      And the response body has field \"skipped\" with an integer\n      And features with \"mvp-\" prefix are stored under version \"mvp\"\n      And features with \"v1-\" prefix are stored under version \"v1\"\n      And features with \"v2-\" prefix are stored under version \"v2\"\n      And features without a version prefix default to version \"mvp\"\n\n    Scenario: Re-seed is idempotent\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I trigger feature re-seed twice\n      Then the second run produces the same result as the first\n      And no duplicate features exist in the database\n\n    Scenario: Re-seed clears stale features\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And the database has a feature for a deleted file\n      When I trigger feature re-seed\n      Then the stale feature is removed from the database\n      And only features matching filesystem files remain\n\n    Scenario: Seed requires admin scope\n      Given the API server is running\n      And a valid API key with scope \"write\" but not \"admin\"\n      When I send a POST request to \"/api/admin/seed-features\"\n      Then the response status is 403\n\n    Scenario: Seed reports step counts per version\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/seed-features\"\n      Then the response body has field \"step_totals\" as an object with:\n        | version | total_steps | total_scenarios |\n        | mvp     | (integer)   | (integer)       |\n        | v1      | (integer)   | (integer)       |\n        | v2      | (integer)   | (integer)       |\n\n  # â”€â”€ Feature File Export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files can be exported back to the filesystem\n\n    Scenario: Export all features to the filesystem\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And features exist in the database for components \"comp-a\" and \"comp-b\"\n      When I send a POST request to \"/api/admin/export-features\"\n      Then the response status is 200\n      And feature files are written to \"components/comp-a/features/\"\n      And feature files are written to \"components/comp-b/features/\"\n      And each file contains the Gherkin content from the database\n\n    Scenario: Export creates component directories if missing\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And a feature exists for component \"new-comp\" but no directory exists\n      When I trigger feature export\n      Then the directory \"components/new-comp/features/\" is created\n      And the feature file is written there\n\n    Scenario: Export for a single component\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/export-features?component=specific-comp\"\n      Then the response status is 200\n      And only features for \"specific-comp\" are exported to the filesystem\n\n  # â”€â”€ Feature Content Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature content can be searched across all components\n\n    Scenario: Search features by keyword\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And features exist containing the word \"authentication\"\n      When I send a GET request to \"/api/features/search?q=authentication\"\n      Then the response status is 200\n      And the response body is an array of matching features\n      And each result has fields: node_id, filename, version, title, step_count, snippet\n\n    Scenario: Search with no results returns empty array\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/features/search?q=xyznonexistent\"\n      Then the response status is 200\n      And the response body is an empty array\n\n    Scenario: Search is case-insensitive\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a feature contains \"Neo4j\"\n      When I send a GET request to \"/api/features/search?q=neo4j\"\n      Then the response body contains the matching feature\n\n    Scenario: Search can be scoped to a version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/features/search?q=test&version=v1\"\n      Then every result in the response has version \"v1\"\n\n    Scenario: Search returns snippet with highlighted match\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a feature contains \"rate limiting\"\n      When I send a GET request to \"/api/features/search?q=rate+limiting\"\n      Then each result has a \"snippet\" field showing context around the match\n"
          },
          {
            "filename": "v1-feature-driven-progress.feature",
            "title": "Feature-Driven Progress Tracking",
            "content": "@wip @v1\nFeature: Feature-Driven Progress Tracking\n  As the roadmap application\n  I want to read version-tagged feature files (mvp, v1, v2) and calculate\n  component completion progress using step-level maths\n  So that progress is objective, automatable, and reflects actual BDD\n  specification coverage at the granularity of individual Given/When/Then steps\n\n  The MVP derives progress from semver current_version numbers. V1 adds a\n  step-based progress system that works with explicitly version-tagged feature\n  files. Every feature file is categorised under a version (via the upload API\n  or filesystem prefix). The system counts total steps (Given, When, Then, And,\n  But) across all features for a version, then compares against passing steps\n  from test results to derive a completion percentage.\n\n  Step-based progress formula:\n    completion% = (passing_steps / total_steps) * 100\n\n  Where:\n    - total_steps = count of all Given/When/Then/And/But lines across all\n      feature files tagged with that version for that component\n    - passing_steps = count of steps in scenarios that passed in the most\n      recent test run for that version\n\n  A scenario's steps only count as passing if the entire scenario passed.\n  Partially passing scenarios contribute 0 passing steps (fail-fast semantics).\n\n  # â”€â”€ Step Counting from Feature Files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: The system counts individual steps from Gherkin feature content\n\n    Scenario: Count steps in a simple feature file\n      Given a feature file with content:\n        \"\"\"\n        Feature: Simple Feature\n          Scenario: Basic flow\n            Given a user exists\n            When the user logs in\n            Then the dashboard is displayed\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n      And the given step count is 1\n      And the when step count is 1\n      And the then step count is 1\n\n    Scenario: And/But steps are counted as steps\n      Given a feature file with content:\n        \"\"\"\n        Feature: And/But Steps\n          Scenario: Complex assertions\n            Given a user exists\n            And the user has admin role\n            When the user accesses settings\n            Then the settings page loads\n            And the admin panel is visible\n            But the delete button is hidden\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 6\n\n    Scenario: Count steps across multiple scenarios\n      Given a feature file with content:\n        \"\"\"\n        Feature: Multi-Scenario\n          Scenario: First\n            Given step one\n            When step two\n            Then step three\n\n          Scenario: Second\n            Given step four\n            When step five\n            Then step six\n            And step seven\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 7\n\n    Scenario: Count steps in Scenario Outline (template counts once)\n      Given a feature file with content:\n        \"\"\"\n        Feature: Outline Feature\n          Scenario Outline: Parameterised\n            Given a <role> user\n            When the user performs <action>\n            Then the result is <outcome>\n\n            Examples:\n              | role  | action | outcome |\n              | admin | edit   | success |\n              | guest | edit   | denied  |\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n      And the step count reflects the template, not the expanded examples\n\n    Scenario: Count steps within Rule blocks\n      Given a feature file with content:\n        \"\"\"\n        Feature: Rules Feature\n          Rule: Authentication\n            Scenario: Login\n              Given credentials\n              When I submit them\n              Then I am logged in\n\n          Rule: Authorisation\n            Scenario: Access check\n              Given I am logged in\n              And I have role \"admin\"\n              When I access the resource\n              Then access is granted\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 7\n\n    Scenario: Background steps are counted once per feature\n      Given a feature file with content:\n        \"\"\"\n        Feature: Background Feature\n          Background:\n            Given a database connection\n            And the schema is initialised\n\n          Scenario: Read data\n            When I query the database\n            Then results are returned\n\n          Scenario: Write data\n            When I insert a record\n            Then the record exists\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 6\n      And the background steps count as 2 (not multiplied by scenarios)\n\n    Scenario: Steps with docstrings count as one step each\n      Given a feature file with content:\n        \"\"\"\n        Feature: Docstring Feature\n          Scenario: Upload content\n            Given the API is running\n            When I upload with body:\n              \\\"\\\"\\\"\n              {\"key\": \"value\"}\n              \\\"\\\"\\\"\n            Then the response status is 200\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n\n    Scenario: Steps with data tables count as one step each\n      Given a feature file with content:\n        \"\"\"\n        Feature: Table Feature\n          Scenario: Tabular data\n            Given these users exist:\n              | name  | role  |\n              | Alice | admin |\n              | Bob   | user  |\n            When I list users\n            Then I see 2 users\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n\n    Scenario: Empty feature file has 0 steps\n      Given a feature file with no scenarios\n      When the step counter processes the file\n      Then the total step count is 0\n\n  # â”€â”€ Aggregated Step Counts Per Component Per Version â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Step counts are aggregated across all features for a component version\n\n    Scenario: Aggregate steps across multiple features for a version\n      Given component \"agg-comp\" has these features under version \"v1\":\n        | filename              | step_count |\n        | auth.feature          | 12         |\n        | permissions.feature   | 8          |\n        | rate-limiting.feature | 15         |\n      When I query the step totals for \"agg-comp\" version \"v1\"\n      Then the total steps are 35\n      And the feature count is 3\n\n    Scenario: Step counts are independent per version\n      Given component \"ver-steps\" has:\n        | version | total_steps |\n        | mvp     | 20          |\n        | v1      | 45          |\n        | v2      | 30          |\n      When I query the step totals for each version\n      Then the \"mvp\" total is 20 steps\n      And the \"v1\" total is 45 steps\n      And the \"v2\" total is 30 steps\n\n    Scenario: Component with no features for a version has 0 total steps\n      Given component \"no-feat\" has features under \"mvp\" but none under \"v1\"\n      When I query the step totals for \"no-feat\" version \"v1\"\n      Then the total steps are 0\n      And the feature count is 0\n\n    Scenario: Adding a feature updates the aggregated step count\n      Given component \"add-feat\" has 20 total steps under version \"v1\"\n      When a new feature with 8 steps is uploaded under version \"v1\"\n      Then the total steps for \"add-feat\" version \"v1\" become 28\n\n    Scenario: Removing a feature updates the aggregated step count\n      Given component \"rm-feat\" has 30 total steps under version \"v1\" across 3 features\n      When a feature with 10 steps is deleted from version \"v1\"\n      Then the total steps for \"rm-feat\" version \"v1\" become 20\n\n  # â”€â”€ Step-Based Progress Calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Progress is calculated as passing_steps / total_steps * 100\n\n    Scenario: 100% progress when all steps pass\n      Given component \"full-pass\" has 20 total steps under version \"mvp\"\n      And test results show 20 of 20 steps passing\n      When I calculate step-based progress for \"full-pass\" version \"mvp\"\n      Then the progress is 100 percent\n      And the status is \"complete\"\n\n    Scenario: 0% progress when no test results exist\n      Given component \"no-tests\" has 15 total steps under version \"v1\"\n      And no test results exist for \"no-tests\" version \"v1\"\n      When I calculate step-based progress for \"no-tests\" version \"v1\"\n      Then the progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: Partial progress with some scenarios passing\n      Given component \"partial\" has 40 total steps under version \"v1\"\n      And test results show scenarios containing 30 steps passed\n      And scenarios containing 10 steps failed\n      When I calculate step-based progress for \"partial\" version \"v1\"\n      Then the progress is 75 percent\n      And the status is \"in-progress\"\n\n    Scenario: Failed scenario contributes 0 passing steps\n      Given component \"fail-scenario\" has a feature with 2 scenarios:\n        | scenario   | steps | passed |\n        | Scenario A | 5     | yes    |\n        | Scenario B | 5     | no     |\n      When I calculate step-based progress for \"fail-scenario\"\n      Then the passing steps are 5 (only from Scenario A)\n      And the total steps are 10\n      And the progress is 50 percent\n\n    Scenario: 0% progress when all scenarios fail\n      Given component \"all-fail\" has 25 total steps under version \"mvp\"\n      And test results show 0 scenarios passing\n      When I calculate step-based progress for \"all-fail\" version \"mvp\"\n      Then the progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: 0% progress when component has 0 total steps\n      Given component \"empty-comp\" has 0 total steps under version \"v1\"\n      When I calculate step-based progress for \"empty-comp\" version \"v1\"\n      Then the progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: Progress rounds to nearest integer\n      Given component \"round-comp\" has 3 total steps under version \"mvp\"\n      And test results show 2 steps passing\n      When I calculate step-based progress for \"round-comp\" version \"mvp\"\n      Then the progress is 67 percent (rounded from 66.67)\n\n    Scenario: Progress is capped at 100\n      Given step counts and results that could produce over 100\n      When I calculate step-based progress\n      Then the progress is exactly 100\n      And the status is \"complete\"\n\n  # â”€â”€ Recording Step-Level Test Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Test results record per-scenario pass/fail and step counts\n\n    Scenario: Record test results with step counts\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"results-comp\" has features under version \"mvp\" with 15 total steps\n      When I send a POST request to \"/api/components/results-comp/versions/mvp/test-results\" with body:\n        \"\"\"\n        {\n          \"results\": [\n            {\n              \"scenario\": \"Create a node\",\n              \"feature\": \"crud.feature\",\n              \"status\": \"passed\",\n              \"steps\": 5,\n              \"duration_ms\": 120\n            },\n            {\n              \"scenario\": \"Delete a node\",\n              \"feature\": \"crud.feature\",\n              \"status\": \"passed\",\n              \"steps\": 4,\n              \"duration_ms\": 85\n            },\n            {\n              \"scenario\": \"Handle invalid input\",\n              \"feature\": \"validation.feature\",\n              \"status\": \"failed\",\n              \"steps\": 6,\n              \"duration_ms\": 200,\n              \"error\": \"Expected 400, got 500\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"recorded\" with value 3\n      And the response body has field \"total_steps\" with value 15\n      And the response body has field \"passing_steps\" with value 9\n      And the response body has field \"failing_steps\" with value 6\n      And the response body has field \"progress_percent\" with value 60\n\n    Scenario: Test results automatically update step-based progress\n      Given component \"auto-prog\" has 20 total steps under version \"v1\"\n      And current progress for \"auto-prog\" version \"v1\" is 0 percent\n      When I record test results with 15 passing steps\n      Then the progress for \"auto-prog\" version \"v1\" updates to 75 percent\n      And the status updates to \"in-progress\"\n\n    Scenario: New test results overwrite previous results\n      Given component \"overwrite\" has previous test results showing 50% progress\n      When I submit new test results showing 80% progress\n      Then the stored results reflect the new submission\n      And the progress is recalculated to 80 percent\n\n    Scenario: Query test results for a component version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"query-comp\" has recorded test results for version \"v1\"\n      When I send a GET request to \"/api/components/query-comp/versions/v1/test-results\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"total_steps\" as a positive integer\n      And the response body has field \"passing_steps\" as an integer\n      And the response body has field \"failing_steps\" as an integer\n      And the response body has field \"progress_percent\" as a number 0-100\n      And the response body has field \"last_run\" as an ISO 8601 timestamp\n      And the response body has field \"scenarios\" as an array of per-scenario results\n\n    Scenario: Per-scenario result includes step count\n      Given recorded test results for component \"detail-comp\" version \"mvp\"\n      When I query the test results\n      Then each scenario entry has fields:\n        | field       | type     | description                          |\n        | scenario    | string   | Scenario name                        |\n        | feature     | string   | Feature filename                     |\n        | status      | string   | \"passed\" or \"failed\"                 |\n        | steps       | number   | Number of steps in this scenario     |\n        | duration_ms | number   | Execution time in milliseconds       |\n        | error       | string?  | Error message if failed              |\n        | last_run    | string   | ISO 8601 timestamp                   |\n\n    Scenario: Test results can be submitted from CI pipeline (Cucumber JSON)\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components/ci-comp/versions/mvp/test-results/cucumber\" with Cucumber JSON output\n      Then the results are parsed extracting scenario names, step counts, and pass/fail\n      And the step-based progress is recalculated\n\n    Scenario: Test results rejected for nonexistent component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components/ghost/versions/v1/test-results\" with results\n      Then the response status is 404\n\n    Scenario: Test results rejected for nonexistent version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"real-comp\" exists but has no features under version \"v2\"\n      When I send a POST request to \"/api/components/real-comp/versions/v2/test-results\" with results\n      Then the response status is 400\n      And the response body has field \"error\" containing \"no features\"\n\n  # â”€â”€ Combined Progress (Semver + Steps) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Final progress blends semver-derived and step-based sources\n\n    Scenario: Combined progress with configurable weights\n      Given the progress configuration has weights:\n        | source        | weight |\n        | semver        | 0.3    |\n        | step_coverage | 0.7    |\n      And component \"weighted\" has:\n        | source                | value |\n        | semver progress       | 50    |\n        | step-based progress   | 80    |\n      When I calculate combined progress for \"weighted\" version \"mvp\"\n      Then the combined progress is 71 percent\n      And the calculation is round((50 * 0.3) + (80 * 0.7)) = 71\n\n    Scenario: Default weights are 50/50\n      Given no custom progress configuration exists\n      And component \"default\" has:\n        | source                | value |\n        | semver progress       | 60    |\n        | step-based progress   | 40    |\n      When I calculate combined progress for \"default\" version \"mvp\"\n      Then the combined progress is 50 percent\n\n    Scenario: No features falls back to semver-only progress\n      Given component \"no-feat\" has current_version \"0.7.0\"\n      And no feature files exist under any version\n      When I calculate combined progress for \"no-feat\" version \"mvp\"\n      Then the combined progress is 70 percent\n      And the progress source is \"semver_only\"\n\n    Scenario: No current_version falls back to step-based-only progress\n      Given component \"no-ver\" has no current_version\n      And step-based progress for \"no-ver\" version \"mvp\" is 60 percent\n      When I calculate combined progress for \"no-ver\" version \"mvp\"\n      Then the combined progress is 60 percent\n      And the progress source is \"step_coverage_only\"\n\n    Scenario: Neither source available gives 0% progress\n      Given component \"empty\" has no current_version and no features\n      When I calculate combined progress for \"empty\" version \"mvp\"\n      Then the combined progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: Status derived from combined progress\n      Then the following status derivation applies:\n        | combined_progress | status      |\n        | 0                 | planned     |\n        | 1-99              | in-progress |\n        | 100               | complete    |\n\n  # â”€â”€ Progress Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Progress blend weights are configurable per component\n\n    Scenario: Set custom weights\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"custom-w\" exists\n      When I send a PUT request to \"/api/components/custom-w/progress-config\" with body:\n        \"\"\"\n        {\"semver_weight\": 0.2, \"step_weight\": 0.8}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"semver_weight\" with value \"0.2\"\n      And the response body has field \"step_weight\" with value \"0.8\"\n\n    Scenario: Weights must sum to 1.0\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request to \"/api/components/bad-w/progress-config\" with body:\n        \"\"\"\n        {\"semver_weight\": 0.5, \"step_weight\": 0.6}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"sum to 1.0\"\n\n    Scenario: Weights must be between 0 and 1\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request with a negative weight\n      Then the response status is 400\n      And the response body has field \"error\" containing \"between 0 and 1\"\n\n    Scenario: Get current weights for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"config-comp\" has custom weights\n      When I send a GET request to \"/api/components/config-comp/progress-config\"\n      Then the response status is 200\n      And the response body has field \"semver_weight\"\n      And the response body has field \"step_weight\"\n\n    Scenario: Default weights returned when no custom config\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"default-c\" has no custom weights\n      When I send a GET request to \"/api/components/default-c/progress-config\"\n      Then the response status is 200\n      And the response body has field \"semver_weight\" with value \"0.5\"\n      And the response body has field \"step_weight\" with value \"0.5\"\n\n  # â”€â”€ Progress Dashboard Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: The API provides aggregated step-based progress data\n\n    Scenario: Get progress summary for all components at a version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/summary?version=mvp\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"mvp\"\n      And the response body has field \"total_components\" as a positive integer\n      And the response body has field \"average_progress\" as a number 0-100\n      And the response body has field \"complete_count\" as an integer\n      And the response body has field \"in_progress_count\" as an integer\n      And the response body has field \"planned_count\" as an integer\n      And the response body has field \"aggregate_steps\" with:\n        | field          | description                          |\n        | total_steps    | Sum of all steps across all features |\n        | passing_steps  | Sum of all passing steps             |\n        | step_coverage  | Overall passing/total percentage     |\n\n    Scenario: Per-component summary includes step-level detail\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/summary?version=mvp\"\n      Then each component in the summary has:\n        | field              | description                         |\n        | id                 | Component ID                        |\n        | name               | Component name                      |\n        | semver_progress    | Progress from current_version       |\n        | total_steps        | Total Given/When/Then steps         |\n        | passing_steps      | Steps in passing scenarios          |\n        | step_progress      | passing_steps / total_steps * 100   |\n        | combined_progress  | Weighted blend of semver + steps    |\n        | status             | Derived from combined progress      |\n        | feature_count      | Number of feature files             |\n        | scenario_count     | Total scenarios across features     |\n\n    Scenario: Progress summary across all versions\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/overview\"\n      Then the response status is 200\n      And the response body has summaries for versions \"mvp\", \"v1\", and \"v2\"\n      And each version summary includes:\n        | field              | description                      |\n        | version            | Version tag                      |\n        | total_components   | Number of components             |\n        | total_steps        | Sum of steps across all features |\n        | passing_steps      | Sum of passing steps             |\n        | step_coverage      | Overall percentage               |\n        | complete_count     | Components at 100%               |\n        | in_progress_count  | Components at 1-99%              |\n        | planned_count      | Components at 0%                 |\n\n    Scenario: Get progress history for a component version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"history\" has had multiple test runs over time\n      When I send a GET request to \"/api/components/history/versions/mvp/progress-history\"\n      Then the response status is 200\n      And the response body is an array of progress snapshots\n      And each snapshot has fields:\n        | field              | description                    |\n        | timestamp          | When the test run occurred     |\n        | total_steps        | Total steps at that time       |\n        | passing_steps      | Passing steps at that time     |\n        | step_progress      | Step-based percentage          |\n        | semver_progress    | Semver-based percentage        |\n        | combined_progress  | Weighted blend                 |\n\n    Scenario: Progress API provides CSV export\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/summary?version=mvp&format=csv\"\n      Then the response status is 200\n      And the response content type is \"text/csv\"\n      And the CSV has columns: id, name, total_steps, passing_steps, step_progress, semver_progress, combined_progress, status\n\n  # â”€â”€ Automatic Progress Recalculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Progress is recalculated when features or test results change\n\n    Scenario: Uploading a feature triggers step recount and progress update\n      Given component \"recalc-up\" has 20 total steps under version \"v1\" with 20 passing\n      And progress is 100 percent\n      When I upload a new feature with 10 steps under version \"v1\"\n      Then total steps become 30\n      And passing steps remain 20 (new feature has no test results)\n      And progress drops to 67 percent\n\n    Scenario: Deleting a feature triggers step recount and progress update\n      Given component \"recalc-del\" has 30 total steps under version \"v1\"\n      And 20 of those steps are in passing scenarios\n      And progress is 67 percent\n      When I delete a feature with 10 failing steps\n      Then total steps become 20\n      And passing steps remain 20\n      And progress increases to 100 percent\n\n    Scenario: Updating current_version triggers semver progress recalculation\n      Given component \"recalc-ver\" has current_version \"0.5.0\"\n      When I update current_version to \"0.8.0\"\n      Then the semver-derived progress for \"mvp\" becomes 80\n      And the combined progress is recalculated\n\n    Scenario: Recording test results triggers step progress recalculation\n      Given component \"recalc-test\" has step progress at 60 percent\n      When I record new test results with more passing scenarios\n      Then the step progress is recalculated\n      And the combined progress is recalculated\n\n    Scenario: Replacing a feature recounts steps correctly\n      Given component \"recalc-replace\" has feature \"auth.feature\" under version \"v1\" with 8 steps\n      And total steps for version \"v1\" are 20\n      When I upload a new version of \"auth.feature\" under version \"v1\" with 12 steps\n      Then total steps for version \"v1\" become 24 (20 - 8 + 12)\n      And passing steps are recalculated based on latest test results\n\n  # â”€â”€ Progress in Architecture Export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Step-based progress is included in architecture export\n\n    Scenario: Architecture export includes step-based progress per version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components have version-tagged features with test results\n      When I send a GET request to \"/api/architecture\"\n      Then each enriched node includes per-version progress with:\n        | field                       | description                     |\n        | versions.mvp.progress       | Combined progress for MVP       |\n        | versions.mvp.total_steps    | Total steps for MVP features    |\n        | versions.mvp.passing_steps  | Passing steps for MVP           |\n        | versions.v1.progress        | Combined progress for V1        |\n        | versions.v1.total_steps     | Total steps for V1 features     |\n        | versions.v1.passing_steps   | Passing steps for V1            |\n\n    Scenario: JSON export includes step-based progress\n      Given the export use case runs\n      Then the output data.json includes per-version step counts\n      And the web view can display step-level progress bars\n\n  # â”€â”€ Filesystem Feature File Watching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Changes to feature files on disk trigger step recount and progress update\n\n    Scenario: New feature file triggers step recount\n      Given the feature file watcher is running\n      And a new file \"components/test-comp/features/v1-new.feature\" with 8 steps is created\n      When the watcher detects the new file\n      Then the file is stored under version \"v1\" (from filename prefix)\n      And total steps for \"test-comp\" version \"v1\" increase by 8\n      And progress is recalculated\n\n    Scenario: Modified feature file triggers step recount\n      Given the feature file watcher is running\n      And \"components/test-comp/features/mvp-auth.feature\" changes from 5 to 9 steps\n      When the watcher detects the modification\n      Then the step count for that feature updates to 9\n      And total steps for \"test-comp\" version \"mvp\" are recalculated\n      And progress is recalculated\n\n    Scenario: Deleted feature file triggers step recount\n      Given the feature file watcher is running\n      And \"components/test-comp/features/v1-old.feature\" with 6 steps is deleted\n      When the watcher detects the deletion\n      Then total steps for \"test-comp\" version \"v1\" decrease by 6\n      And progress is recalculated\n\n    Scenario: Watcher can be triggered manually via API\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/scan-features\"\n      Then the response status is 200\n      And the response body includes per-version step totals:\n        | field               | description                    |\n        | scanned             | Total files processed          |\n        | added               | New features found             |\n        | updated             | Modified features updated      |\n        | removed             | Deleted features cleaned up    |\n        | step_totals.mvp     | Total MVP steps after scan     |\n        | step_totals.v1      | Total V1 steps after scan      |\n        | step_totals.v2      | Total V2 steps after scan      |\n\n    Scenario: Watcher ignores non-feature files\n      Given the feature file watcher is running\n      And a file \"components/test-comp/features/README.md\" is created\n      When the watcher processes the event\n      Then no feature is added to the database\n      And no step recount occurs\n\n    Scenario: Watcher debounces rapid successive changes\n      Given the feature file watcher is running\n      When 10 feature files are modified within 500 milliseconds\n      Then the watcher batches the changes\n      And triggers a single re-scan after a 1-second debounce period\n      And step counts are recalculated once for all 10 files\n"
          },
          {
            "filename": "v1-neo4j-graph-storage.feature",
            "title": "Neo4j Graph Storage",
            "content": "@wip @v1\nFeature: Neo4j Graph Storage\n  As the roadmap application\n  I want to store all architecture data in Neo4j instead of SQLite\n  So that the graph data model is natively represented, traversals are efficient,\n  and the system is ready for production-scale autonomous coding workflows\n\n  Neo4j replaces SQLite as the persistence layer. The domain entities, repository\n  interfaces, and use cases remain unchanged. Only the infrastructure layer swaps\n  out Drizzle/SQLite repositories for Neo4j Cypher-backed repositories. Connection\n  credentials are stored securely via environment variables, never in code or config\n  files committed to version control.\n\n  # â”€â”€ Connection & Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j connection is configured via environment variables\n\n    Scenario: Connect to Neo4j using environment variables\n      Given the environment variable \"NEO4J_URI\" is set to \"bolt://localhost:7687\"\n      And the environment variable \"NEO4J_USER\" is set to \"neo4j\"\n      And the environment variable \"NEO4J_PASSWORD\" is set to a secure value\n      When the application creates a Neo4j connection\n      Then the connection is established successfully\n      And the driver verifies connectivity with a test query\n\n    Scenario: Connection fails gracefully when NEO4J_URI is missing\n      Given the environment variable \"NEO4J_URI\" is not set\n      When the application attempts to create a Neo4j connection\n      Then a configuration error is thrown with message containing \"NEO4J_URI\"\n      And no connection is established\n\n    Scenario: Connection fails gracefully when NEO4J_PASSWORD is missing\n      Given the environment variable \"NEO4J_URI\" is set to \"bolt://localhost:7687\"\n      And the environment variable \"NEO4J_PASSWORD\" is not set\n      When the application attempts to create a Neo4j connection\n      Then a configuration error is thrown with message containing \"NEO4J_PASSWORD\"\n\n    Scenario: Connection uses encrypted transport in production\n      Given the environment variable \"NODE_ENV\" is set to \"production\"\n      And the environment variable \"NEO4J_URI\" starts with \"neo4j+s://\"\n      When the application creates a Neo4j connection\n      Then the driver uses encrypted transport\n      And the TLS certificate is verified\n\n    Scenario: Connection pool settings are configurable\n      Given the environment variable \"NEO4J_MAX_CONNECTIONS\" is set to \"50\"\n      And the environment variable \"NEO4J_ACQUISITION_TIMEOUT\" is set to \"30000\"\n      When the application creates a Neo4j connection\n      Then the connection pool max size is 50\n      And the acquisition timeout is 30000 milliseconds\n\n    Scenario: Connection retries on transient failure\n      Given the Neo4j server is temporarily unavailable\n      When the application creates a Neo4j connection with retry enabled\n      Then it retries the connection up to 3 times\n      And it waits with exponential backoff between attempts\n      And the final failure is logged with connection details (excluding password)\n\n    Scenario: Credentials are never logged or exposed\n      Given a Neo4j connection is configured\n      When the connection details are logged\n      Then the log output contains the URI\n      And the log output does not contain the password\n      And the log output does not contain the username\n\n  # â”€â”€ Schema Initialisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j schema is initialised with constraints and indexes\n\n    Scenario: Node uniqueness constraint is created\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a uniqueness constraint exists on Node.id\n      And the constraint name is \"unique_node_id\"\n\n    Scenario: Edge composite uniqueness constraint is created\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a uniqueness constraint exists on Edge (source_id, target_id, type)\n      And duplicate edges are rejected by the database\n\n    Scenario: Version composite uniqueness constraint is created\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a uniqueness constraint exists on Version (node_id, version)\n      And duplicate versions are rejected by the database\n\n    Scenario: Indexes are created for common query patterns\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then an index exists on Node.type\n      And an index exists on Node.layer\n      And an index exists on Version.node_id\n      And an index exists on Feature.node_id\n      And an index exists on Feature.version\n\n    Scenario: Schema initialisation is idempotent\n      Given a Neo4j database with existing constraints and indexes\n      When the schema initialisation runs again\n      Then no errors are thrown\n      And the constraints remain unchanged\n      And the indexes remain unchanged\n\n    Scenario: Full-text index on Node name and description\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a full-text index exists on Node.name and Node.description\n      And the index supports case-insensitive search\n\n  # â”€â”€ Node CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j node repository implements INodeRepository\n\n    Scenario: Save a new node\n      Given an empty Neo4j database with schema\n      When I save a node with id \"test-comp\", name \"Test Component\", type \"component\"\n      Then a Neo4j node labelled \"ArchNode\" exists with id \"test-comp\"\n      And the node has property \"name\" with value \"Test Component\"\n      And the node has property \"type\" with value \"component\"\n\n    Scenario: Save a node with all optional fields\n      Given an empty Neo4j database with schema\n      When I save a node with:\n        | field           | value                     |\n        | id              | full-comp                 |\n        | name            | Full Component            |\n        | type            | component                 |\n        | layer           | supervisor-layer          |\n        | color           | #FF5733                   |\n        | icon            | server                    |\n        | description     | A fully specified node    |\n        | tags            | [\"runtime\",\"core\"]        |\n        | sort_order      | 10                        |\n        | current_version | 0.5.0                     |\n      Then the Neo4j node \"full-comp\" has all specified properties\n\n    Scenario: Update an existing node via upsert\n      Given a node \"upsert-comp\" exists in Neo4j with name \"Original\"\n      When I save a node with id \"upsert-comp\" and name \"Updated\"\n      Then the Neo4j node \"upsert-comp\" has property \"name\" with value \"Updated\"\n      And only one node with id \"upsert-comp\" exists\n\n    Scenario: Find all nodes ordered by sort_order\n      Given these nodes exist in Neo4j:\n        | id   | name   | type      | sort_order |\n        | b    | Beta   | component | 20         |\n        | a    | Alpha  | component | 10         |\n        | c    | Gamma  | component | 30         |\n      When I call findAll on the node repository\n      Then the result contains 3 nodes\n      And the nodes are ordered [\"a\", \"b\", \"c\"]\n\n    Scenario: Find node by ID\n      Given a node \"find-me\" exists in Neo4j\n      When I call findById with \"find-me\"\n      Then the result is the node with id \"find-me\"\n\n    Scenario: Find node by ID returns null when not found\n      When I call findById with \"nonexistent\"\n      Then the result is null\n\n    Scenario: Find nodes by type\n      Given these nodes exist in Neo4j:\n        | id      | name      | type      |\n        | layer-1 | Layer One | layer     |\n        | comp-1  | Comp One  | component |\n        | comp-2  | Comp Two  | component |\n      When I call findByType with \"component\"\n      Then the result contains 2 nodes\n      And both nodes have type \"component\"\n\n    Scenario: Find nodes by layer\n      Given these nodes exist in Neo4j:\n        | id     | name     | type      | layer    |\n        | comp-a | Comp A   | component | layer-x  |\n        | comp-b | Comp B   | component | layer-x  |\n        | comp-c | Comp C   | component | layer-y  |\n      When I call findByLayer with \"layer-x\"\n      Then the result contains 2 nodes\n      And both nodes have layer \"layer-x\"\n\n    Scenario: Check node existence\n      Given a node \"exists-comp\" exists in Neo4j\n      When I call exists with \"exists-comp\"\n      Then the result is true\n\n    Scenario: Check node existence returns false for missing\n      When I call exists with \"missing-comp\"\n      Then the result is false\n\n    Scenario: Delete a node\n      Given a node \"delete-me\" exists in Neo4j\n      When I call delete with \"delete-me\"\n      Then no node with id \"delete-me\" exists in Neo4j\n\n    Scenario: Delete cascades to related edges\n      Given a node \"cascade-node\" exists in Neo4j\n      And an edge exists from \"cascade-node\" to \"other-node\" with type \"DEPENDS_ON\"\n      When I call delete with \"cascade-node\"\n      Then no edges reference \"cascade-node\"\n\n    Scenario: Tags are stored as a JSON string property\n      Given I save a node with id \"tag-node\" and tags [\"alpha\", \"beta\"]\n      When I call findById with \"tag-node\"\n      Then the node tags are [\"alpha\", \"beta\"]\n      And the raw Neo4j property \"tags\" is a JSON string\n\n  # â”€â”€ Edge CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j edge repository implements IEdgeRepository\n\n    Scenario: Save a new edge as a Neo4j relationship\n      Given nodes \"source-1\" and \"target-1\" exist in Neo4j\n      When I save an edge from \"source-1\" to \"target-1\" with type \"DEPENDS_ON\"\n      Then a Neo4j relationship of type \"DEPENDS_ON\" exists from \"source-1\" to \"target-1\"\n\n    Scenario: Save an edge with label and metadata\n      Given nodes \"src\" and \"tgt\" exist in Neo4j\n      When I save an edge from \"src\" to \"tgt\" with:\n        | field    | value                 |\n        | type     | CONTROLS              |\n        | label    | spawns                |\n        | metadata | {\"priority\":\"high\"}   |\n      Then the relationship has property \"label\" with value \"spawns\"\n      And the relationship has property \"metadata\" with value '{\"priority\":\"high\"}'\n\n    Scenario: Edge upsert updates label and metadata\n      Given an edge from \"a\" to \"b\" with type \"DEPENDS_ON\" and label \"old\"\n      When I save an edge from \"a\" to \"b\" with type \"DEPENDS_ON\" and label \"new\"\n      Then only one \"DEPENDS_ON\" relationship exists from \"a\" to \"b\"\n      And the label is \"new\"\n\n    Scenario: Find all edges\n      Given 5 edges exist in Neo4j\n      When I call findAll on the edge repository\n      Then the result contains 5 edges\n\n    Scenario: Find edges by source\n      Given edges from \"hub\" to \"spoke-1\", \"spoke-2\", \"spoke-3\" exist\n      When I call findBySource with \"hub\"\n      Then the result contains 3 edges\n      And all edges have source_id \"hub\"\n\n    Scenario: Find edges by target\n      Given edges from \"a\" to \"hub\" and \"b\" to \"hub\" exist\n      When I call findByTarget with \"hub\"\n      Then the result contains 2 edges\n      And all edges have target_id \"hub\"\n\n    Scenario: Find edges by type\n      Given 2 \"CONTAINS\" and 3 \"DEPENDS_ON\" edges exist\n      When I call findByType with \"CONTAINS\"\n      Then the result contains 2 edges\n\n    Scenario: Find relationships excludes CONTAINS edges\n      Given 2 \"CONTAINS\" and 3 \"DEPENDS_ON\" edges exist\n      When I call findRelationships\n      Then the result contains 3 edges\n      And no edge has type \"CONTAINS\"\n\n    Scenario: Delete an edge by ID\n      Given an edge with known ID exists in Neo4j\n      When I call delete with that edge ID\n      Then the edge no longer exists\n\n    Scenario: All 11 edge types are supported\n      Given nodes \"edge-src\" and \"edge-tgt\" exist in Neo4j\n      When I save edges with each of these types:\n        | type           |\n        | CONTAINS       |\n        | CONTROLS       |\n        | DEPENDS_ON     |\n        | READS_FROM     |\n        | WRITES_TO      |\n        | DISPATCHES_TO  |\n        | ESCALATES_TO   |\n        | PROXIES        |\n        | SANITISES      |\n        | GATES          |\n        | SEQUENCE       |\n      Then 11 relationships exist from \"edge-src\" to \"edge-tgt\"\n\n  # â”€â”€ Version CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j version repository implements IVersionRepository\n\n    Scenario: Save a new version\n      Given a node \"ver-node\" exists in Neo4j\n      When I save a version for \"ver-node\" with version \"mvp\" and progress 50\n      Then a Neo4j node labelled \"Version\" exists with node_id \"ver-node\" and version \"mvp\"\n      And the version has progress 50\n\n    Scenario: Version upsert updates content and progress\n      Given a version \"mvp\" exists for node \"ver-node\" with progress 30\n      When I save a version for \"ver-node\" with version \"mvp\" and progress 70\n      Then the version \"mvp\" for \"ver-node\" has progress 70\n      And only one version \"mvp\" exists for \"ver-node\"\n\n    Scenario: Find all versions\n      Given versions exist for multiple nodes\n      When I call findAll on the version repository\n      Then the result contains all versions ordered by node_id and version\n\n    Scenario: Find versions by node\n      Given node \"multi-ver\" has versions \"overview\", \"mvp\", \"v1\", \"v2\"\n      When I call findByNode with \"multi-ver\"\n      Then the result contains 4 versions\n      And all versions have node_id \"multi-ver\"\n\n    Scenario: Find version by node and version tag\n      Given node \"specific-ver\" has version \"v1\" with content \"V1 spec\"\n      When I call findByNodeAndVersion with \"specific-ver\" and \"v1\"\n      Then the result has content \"V1 spec\"\n\n    Scenario: Update progress and status via save\n      Given node \"prog-node\" has version \"mvp\" with progress 0 and status \"planned\"\n      When I call save with node \"prog-node\", version \"mvp\", progress 50, status \"in-progress\"\n      Then the version has progress 50 and status \"in-progress\"\n      And the updated_at timestamp is refreshed\n\n    Scenario: Delete all versions for a node\n      Given node \"del-ver\" has versions \"overview\", \"mvp\", \"v1\"\n      When I call deleteByNode with \"del-ver\"\n      Then no versions exist for \"del-ver\"\n\n  # â”€â”€ Feature CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j feature repository implements IFeatureRepository\n\n    Scenario: Save a new feature\n      Given a node \"feat-node\" exists in Neo4j\n      When I save a feature for \"feat-node\" with filename \"mvp-test.feature\"\n      Then a Neo4j node labelled \"Feature\" exists linked to \"feat-node\"\n      And the feature has filename \"mvp-test.feature\"\n\n    Scenario: Find all features\n      Given features exist for multiple nodes and versions\n      When I call findAll on the feature repository\n      Then the result contains all features ordered by node_id, version, filename\n\n    Scenario: Find features by node\n      Given node \"feat-multi\" has 3 feature files\n      When I call findByNode with \"feat-multi\"\n      Then the result contains 3 features\n\n    Scenario: Find features by node and version\n      Given node \"feat-ver\" has 2 \"mvp\" features and 1 \"v1\" feature\n      When I call findByNodeAndVersion with \"feat-ver\" and \"mvp\"\n      Then the result contains 2 features\n      And both features have version \"mvp\"\n\n    Scenario: Delete all features (re-seed preparation)\n      Given features exist across multiple nodes\n      When I call deleteAll on the feature repository\n      Then no features exist in the database\n\n    Scenario: Delete features by node\n      Given node \"feat-del\" has features and node \"feat-keep\" has features\n      When I call deleteByNode with \"feat-del\"\n      Then no features exist for \"feat-del\"\n      And features still exist for \"feat-keep\"\n\n  # â”€â”€ Native Graph Traversals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j enables native graph traversal queries\n\n    Scenario: Multi-hop dependency traversal\n      Given this dependency chain exists:\n        | source    | target    |\n        | comp-a    | comp-b    |\n        | comp-b    | comp-c    |\n        | comp-c    | comp-d    |\n      When I query for all transitive dependencies of \"comp-a\" up to depth 3\n      Then the result contains \"comp-b\", \"comp-c\", \"comp-d\"\n\n    Scenario: Reverse dependency traversal (dependents)\n      Given this dependency chain exists:\n        | source    | target    |\n        | comp-a    | comp-d    |\n        | comp-b    | comp-d    |\n        | comp-c    | comp-d    |\n      When I query for all transitive dependents of \"comp-d\" up to depth 1\n      Then the result contains \"comp-a\", \"comp-b\", \"comp-c\"\n\n    Scenario: Shortest path between two components\n      Given a graph with multiple paths from \"start\" to \"end\"\n      When I query for the shortest path from \"start\" to \"end\"\n      Then the result contains the path with the fewest hops\n      And each hop includes the edge type and label\n\n    Scenario: Layer containment subtree\n      Given a layer \"supervisor-layer\" containing 4 components\n      When I query the containment subtree of \"supervisor-layer\"\n      Then the result contains 4 child nodes\n      And each child has a CONTAINS relationship from \"supervisor-layer\"\n\n    Scenario: Cycle detection in dependency graph\n      Given this dependency chain exists:\n        | source    | target    |\n        | comp-a    | comp-b    |\n        | comp-b    | comp-c    |\n        | comp-c    | comp-a    |\n      When I check for cycles in the dependency graph\n      Then a cycle is detected involving \"comp-a\", \"comp-b\", \"comp-c\"\n\n    Scenario: Component neighbourhood query\n      Given \"center-comp\" has 3 outbound and 2 inbound edges\n      When I query the 1-hop neighbourhood of \"center-comp\"\n      Then the result contains 5 related components\n      And each result includes the edge type and direction\n\n  # â”€â”€ Data Migration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: SQLite data can be migrated to Neo4j\n\n    Scenario: Migrate all nodes from SQLite to Neo4j\n      Given a SQLite database containing 60 nodes\n      When the migration tool runs\n      Then 60 ArchNode nodes exist in Neo4j\n      And each node has all properties preserved\n\n    Scenario: Migrate all edges from SQLite to Neo4j\n      Given a SQLite database containing 120 edges\n      When the migration tool runs\n      Then 120 relationships exist in Neo4j\n      And each relationship has the correct type, label, and metadata\n\n    Scenario: Migrate all versions from SQLite to Neo4j\n      Given a SQLite database containing 200 versions\n      When the migration tool runs\n      Then 200 Version nodes exist in Neo4j\n      And each version is linked to its parent node\n\n    Scenario: Migrate all features from SQLite to Neo4j\n      Given a SQLite database containing 50 features\n      When the migration tool runs\n      Then 50 Feature nodes exist in Neo4j\n      And each feature is linked to its parent node\n\n    Scenario: Migration is idempotent\n      Given a SQLite database has been migrated once\n      When the migration tool runs again\n      Then no duplicate nodes or relationships are created\n      And updated properties are reflected in Neo4j\n\n    Scenario: Migration validates data integrity\n      Given the migration has completed\n      When a validation check compares SQLite and Neo4j\n      Then the node count matches\n      And the edge count matches\n      And the version count matches\n      And the feature count matches\n      And a sample of 10 nodes have identical properties in both databases\n\n    Scenario: Migration CLI provides progress reporting\n      Given a SQLite database with data\n      When the migration tool runs in verbose mode\n      Then it reports the number of nodes migrated\n      And it reports the number of edges migrated\n      And it reports the number of versions migrated\n      And it reports the number of features migrated\n      And it reports the total elapsed time\n\n  # â”€â”€ Transaction Safety â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j operations use transactions for data integrity\n\n    Scenario: Node save runs within a transaction\n      Given the Neo4j node repository\n      When I save a node and the operation succeeds\n      Then the node is committed to the database\n\n    Scenario: Failed save rolls back the transaction\n      Given the Neo4j node repository\n      When I save a node with an invalid property\n      Then the transaction is rolled back\n      And no partial data exists in the database\n\n    Scenario: Bulk insert uses a single transaction\n      Given 10 nodes to insert\n      When I save all 10 nodes in a batch operation\n      Then either all 10 nodes are committed or none are\n      And the operation uses a single transaction\n\n    Scenario: Concurrent writes are serialised safely\n      Given two concurrent requests to update node \"shared-node\"\n      When both requests execute simultaneously\n      Then both writes complete without data corruption\n      And the final state reflects one of the two writes\n\n  # â”€â”€ Security â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j connection credentials are handled securely\n\n    Scenario: Password is not stored in any configuration file\n      Given the project source directory\n      When I search all files for the Neo4j password\n      Then no file in the repository contains a hardcoded password\n      And credentials are only read from environment variables\n\n    Scenario: Connection string does not leak to client responses\n      Given the API server is running with Neo4j backend\n      When I send a GET request that causes a database error\n      Then the error response does not contain the connection URI\n      And the error response does not contain credentials\n      And the error response contains a generic error message\n\n    Scenario: Database user has minimum required privileges\n      Given the Neo4j user for the application\n      Then the user has read and write access to the application database\n      And the user does not have admin privileges\n      And the user cannot create or drop databases\n\n    Scenario: Environment variables for Neo4j are documented\n      Given the project documentation\n      Then it lists \"NEO4J_URI\" as required\n      And it lists \"NEO4J_USER\" as required\n      And it lists \"NEO4J_PASSWORD\" as required\n      And it lists \"NEO4J_DATABASE\" as optional with default \"neo4j\"\n      And it lists \"NEO4J_MAX_CONNECTIONS\" as optional with default \"100\"\n\n  # â”€â”€ Repository Interface Compliance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j repositories implement the same domain interfaces as SQLite\n\n    Scenario: Neo4j node repository implements INodeRepository\n      Given the Neo4j node repository class\n      Then it implements the INodeRepository interface\n      And it has methods: findAll, findById, findByType, findByLayer, exists, save, delete\n\n    Scenario: Neo4j edge repository implements IEdgeRepository\n      Given the Neo4j edge repository class\n      Then it implements the IEdgeRepository interface\n      And it has methods: findAll, findBySource, findByTarget, findByType, findRelationships, save, delete\n\n    Scenario: Neo4j version repository implements IVersionRepository\n      Given the Neo4j version repository class\n      Then it implements the IVersionRepository interface\n      And it has methods: findAll, findByNode, findByNodeAndVersion, save, deleteByNode\n\n    Scenario: Neo4j feature repository implements IFeatureRepository\n      Given the Neo4j feature repository class\n      Then it implements the IFeatureRepository interface\n      And it has methods: findAll, findByNode, findByNodeAndVersion, save, deleteAll, deleteByNode\n\n    Scenario: Use cases work identically with Neo4j repositories\n      Given the GetArchitecture use case\n      When I inject Neo4j repositories instead of SQLite repositories\n      Then the use case executes without modification\n      And the output structure is identical\n\n    Scenario: Adapter wiring selects storage backend from environment\n      Given the environment variable \"STORAGE_BACKEND\" is set to \"neo4j\"\n      When the API adapter initialises\n      Then it creates Neo4j repository instances\n      And injects them into use cases\n\n    Scenario: Adapter defaults to SQLite when STORAGE_BACKEND is unset\n      Given the environment variable \"STORAGE_BACKEND\" is not set\n      When the API adapter initialises\n      Then it creates SQLite/Drizzle repository instances\n      And injects them into use cases\n"
          },
          {
            "filename": "v1-secure-api.feature",
            "title": "Secure and Rate-Limited API",
            "content": "@wip @v1\nFeature: Secure and Rate-Limited API\n  As the roadmap application\n  I want the REST API to be secured with API key authentication and rate limiting\n  So that only authorised LLM engineers can manage roadmap content headlessly\n  and the system is protected from abuse, brute-force attacks, and accidental overload\n\n  The MVP API has no authentication. V1 adds API key authentication with scoped\n  permissions, per-key rate limiting, request logging, and security headers.\n  API keys are stored as salted hashes. All sensitive operations require\n  appropriate scopes. Rate limits are configurable per key and per endpoint.\n\n  # â”€â”€ API Key Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API keys are generated securely and stored as hashed values\n\n    Scenario: Generate a new API key via CLI\n      Given the API key management CLI\n      When I run the command to generate a new key with name \"ci-bot\"\n      Then a new API key is returned in the format \"rmap_<32 hex characters>\"\n      And the key is displayed once and never stored in plaintext\n      And a salted SHA-256 hash of the key is stored in the database\n\n    Scenario: Generate a key with specific scopes\n      Given the API key management CLI\n      When I run the command to generate a key with name \"reader\" and scopes \"read\"\n      Then the key is created with scope \"read\"\n      And the key cannot be used for write operations\n\n    Scenario: Generate a key with multiple scopes\n      Given the API key management CLI\n      When I run the command to generate a key with name \"engineer\" and scopes \"read,write\"\n      Then the key is created with scopes \"read\" and \"write\"\n\n    Scenario: Generate a key with admin scope\n      Given the API key management CLI\n      When I run the command to generate a key with name \"admin-bot\" and scopes \"read,write,admin\"\n      Then the key is created with scopes \"read\", \"write\", and \"admin\"\n\n    Scenario: API key name must be unique\n      Given a key with name \"existing-bot\" already exists\n      When I run the command to generate a key with name \"existing-bot\"\n      Then an error is returned with message \"Key name already exists: existing-bot\"\n      And no new key is created\n\n    Scenario: API key has an optional expiry date\n      Given the API key management CLI\n      When I run the command to generate a key with name \"temp-key\" and expiry \"2026-12-31\"\n      Then the key is created with expiry date \"2026-12-31T00:00:00Z\"\n\n    Scenario: API key with no expiry never expires\n      Given the API key management CLI\n      When I run the command to generate a key with name \"permanent-key\" and no expiry\n      Then the key is created with a null expiry date\n\n  # â”€â”€ API Key Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API keys are stored securely in the database\n\n    Scenario: Key record contains required fields\n      Given a newly generated API key\n      Then the database record contains:\n        | field        | type     | description                    |\n        | id           | integer  | Auto-incrementing primary key  |\n        | name         | text     | Unique human-readable name     |\n        | key_hash     | text     | Salted SHA-256 hash            |\n        | salt         | text     | Unique per-key salt            |\n        | scopes       | text     | JSON array of scope strings    |\n        | created_at   | text     | ISO 8601 timestamp             |\n        | expires_at   | text     | ISO 8601 timestamp or null     |\n        | last_used_at | text     | ISO 8601 timestamp or null     |\n        | is_active    | integer  | 1 = active, 0 = revoked        |\n\n    Scenario: Raw API key cannot be retrieved from the database\n      Given a key \"stored-key\" exists in the database\n      When I query the api_keys table for \"stored-key\"\n      Then the result contains key_hash but not the raw key\n      And the key_hash cannot be reversed to obtain the raw key\n\n    Scenario: Key verification uses constant-time comparison\n      Given a key \"verify-key\" exists in the database\n      When I verify an API key against the stored hash\n      Then the comparison uses a timing-safe equality check\n      And the verification completes in constant time regardless of match\n\n  # â”€â”€ API Key Authentication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: All API endpoints except health require a valid API key\n\n    Scenario: Health endpoint does not require authentication\n      Given the API server is running\n      When I send a GET request to \"/api/health\" without an API key\n      Then the response status is 200\n      And the response body has field \"status\" with value \"ok\"\n\n    Scenario: Authenticated request with valid key succeeds\n      Given the API server is running\n      And a valid API key \"rmap_abc123\" with scope \"read\" exists\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_abc123\"\n      Then the response status is 200\n\n    Scenario: Request without API key returns 401\n      Given the API server is running\n      When I send a GET request to \"/api/components\" without an API key\n      Then the response status is 401\n      And the response body has field \"error\" with value \"Authentication required\"\n      And the response has header \"WWW-Authenticate\" with value \"Bearer\"\n\n    Scenario: Request with invalid API key returns 401\n      Given the API server is running\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_invalid\"\n      Then the response status is 401\n      And the response body has field \"error\" with value \"Invalid API key\"\n\n    Scenario: Request with expired API key returns 401\n      Given the API server is running\n      And an expired API key \"rmap_expired\" exists\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_expired\"\n      Then the response status is 401\n      And the response body has field \"error\" with value \"API key expired\"\n\n    Scenario: Request with revoked API key returns 401\n      Given the API server is running\n      And a revoked API key \"rmap_revoked\" exists\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_revoked\"\n      Then the response status is 401\n      And the response body has field \"error\" with value \"API key revoked\"\n\n    Scenario: API key is accepted via X-API-Key header as alternative\n      Given the API server is running\n      And a valid API key \"rmap_alt123\" with scope \"read\" exists\n      When I send a GET request to \"/api/components\" with header \"X-API-Key: rmap_alt123\"\n      Then the response status is 200\n\n    Scenario: Last-used timestamp is updated on successful authentication\n      Given the API server is running\n      And a valid API key \"rmap_track\" with scope \"read\" exists\n      When I send a GET request to \"/api/components\" with that key\n      Then the key's last_used_at timestamp is updated to the current time\n\n  # â”€â”€ Scope-Based Authorisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API operations require specific scopes\n\n    Scenario: Read scope allows GET requests\n      Given a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components\" with that key\n      Then the response status is 200\n\n    Scenario: Read scope denies POST requests\n      Given a valid API key with scope \"read\" only\n      When I send a POST request to \"/api/components\" with that key and body:\n        \"\"\"\n        {\"id\":\"new\",\"name\":\"New\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 403\n      And the response body has field \"error\" with value \"Insufficient scope: write required\"\n\n    Scenario: Write scope allows POST requests\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a POST request to \"/api/components\" with that key and body:\n        \"\"\"\n        {\"id\":\"writable\",\"name\":\"Writable\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n\n    Scenario: Write scope allows PUT requests\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a PUT request to \"/api/components/test-comp/features/v1-test.feature\" with that key\n      Then the response status is not 403\n\n    Scenario: Write scope allows PUT requests\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a PUT request to \"/api/components/test-comp/versions/mvp\" with that key\n      Then the response status is not 403\n\n    Scenario: Write scope allows DELETE requests for component data\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a DELETE request to \"/api/components/del-comp\" with that key\n      Then the response status is not 403\n\n    Scenario: Admin scope required for key management endpoints\n      Given a valid API key with scopes \"read\" and \"write\" but not \"admin\"\n      When I send a POST request to \"/api/admin/keys\" with that key\n      Then the response status is 403\n      And the response body has field \"error\" with value \"Insufficient scope: admin required\"\n\n    Scenario: Admin scope allows key management\n      Given a valid API key with scope \"admin\"\n      When I send a GET request to \"/api/admin/keys\" with that key\n      Then the response status is 200\n\n    Scenario: Scope mapping for all HTTP methods\n      Then the following scope mapping applies:\n        | method  | scope  |\n        | GET     | read   |\n        | POST    | write  |\n        | PUT     | write  |\n        | PATCH   | write  |\n        | DELETE  | write  |\n\n  # â”€â”€ API Key Management Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Admin users can manage API keys via the API\n\n    Scenario: List all API keys (admin)\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And 3 API keys exist in the database\n      When I send a GET request to \"/api/admin/keys\" with the admin key\n      Then the response status is 200\n      And the response body is an array of 3 key records\n      And no record contains the raw key or key_hash\n\n    Scenario: Revoke an API key (admin)\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And a key with name \"revoke-me\" exists and is active\n      When I send a DELETE request to \"/api/admin/keys/revoke-me\" with the admin key\n      Then the response status is 200\n      And the key \"revoke-me\" is marked as inactive\n      And subsequent requests with that key return 401\n\n    Scenario: Generate a new key via API (admin)\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/keys\" with the admin key and body:\n        \"\"\"\n        {\"name\":\"api-created\",\"scopes\":[\"read\",\"write\"]}\n        \"\"\"\n      Then the response status is 201\n      And the response body contains the raw key (displayed once)\n      And the response body has field \"name\" with value \"api-created\"\n\n    Scenario: Revoke nonexistent key returns 404\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a DELETE request to \"/api/admin/keys/ghost-key\" with the admin key\n      Then the response status is 404\n      And the response body has field \"error\"\n\n  # â”€â”€ Rate Limiting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API requests are rate-limited per key and per endpoint\n\n    Scenario: Default rate limit is applied per API key\n      Given the API server is running with default rate limit of 100 requests per minute\n      And a valid API key \"rmap_rate\" with scope \"read\"\n      When I send 100 GET requests to \"/api/components\" within 1 minute\n      Then all 100 requests return status 200\n\n    Scenario: Exceeding rate limit returns 429\n      Given the API server is running with default rate limit of 100 requests per minute\n      And a valid API key \"rmap_over\" with scope \"read\"\n      When I send 101 GET requests to \"/api/components\" within 1 minute\n      Then the 101st request returns status 429\n      And the response body has field \"error\" with value \"Rate limit exceeded\"\n      And the response has header \"Retry-After\" with a positive integer value\n\n    Scenario: Rate limit headers are included in every response\n      Given the API server is running\n      And a valid API key \"rmap_headers\" with scope \"read\"\n      When I send a GET request to \"/api/components\" with that key\n      Then the response has header \"X-RateLimit-Limit\"\n      And the response has header \"X-RateLimit-Remaining\"\n      And the response has header \"X-RateLimit-Reset\"\n\n    Scenario: Rate limits reset after the window expires\n      Given the API server is running with rate limit of 10 requests per minute\n      And a valid API key \"rmap_reset\" with scope \"read\"\n      And the key has exhausted its rate limit\n      When 60 seconds have elapsed\n      And I send a GET request to \"/api/components\" with that key\n      Then the response status is 200\n      And X-RateLimit-Remaining reflects the fresh window\n\n    Scenario: Write operations have a stricter rate limit\n      Given the API server is running\n      And write endpoints have a rate limit of 30 requests per minute\n      And a valid API key with scopes \"read\" and \"write\"\n      When I send 31 POST requests to \"/api/components\" within 1 minute\n      Then the 31st request returns status 429\n\n    Scenario: Different keys have independent rate limits\n      Given the API server is running with rate limit of 10 requests per minute\n      And valid API keys \"rmap_key1\" and \"rmap_key2\" exist\n      When \"rmap_key1\" sends 10 requests (exhausting its limit)\n      And \"rmap_key2\" sends 1 request\n      Then \"rmap_key2\" gets status 200\n      And \"rmap_key1\" gets status 429 on its next request\n\n    Scenario: Rate limit can be configured per key\n      Given the API server is running\n      And API key \"rmap_premium\" has a custom rate limit of 500 requests per minute\n      When \"rmap_premium\" sends 200 requests within 1 minute\n      Then all requests return status 200\n      And X-RateLimit-Limit reflects 500\n\n    Scenario: Health endpoint is exempt from rate limiting\n      Given the API server is running\n      When I send 1000 GET requests to \"/api/health\" within 1 minute\n      Then all requests return status 200\n\n    Scenario: Rate limit state is stored in memory (not database)\n      Given the API server is running\n      When the server processes rate-limited requests\n      Then no rate limit data is written to the database\n      And rate limit counters reset on server restart\n\n  # â”€â”€ Security Headers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API responses include security headers\n\n    Scenario: All responses include standard security headers\n      Given the API server is running\n      When I send any request to the API\n      Then the response has header \"X-Content-Type-Options\" with value \"nosniff\"\n      And the response has header \"X-Frame-Options\" with value \"DENY\"\n      And the response has header \"Strict-Transport-Security\" with value \"max-age=31536000; includeSubDomains\"\n      And the response has header \"Cache-Control\" with value \"no-store\"\n      And the response has header \"X-Request-Id\" with a UUID value\n\n    Scenario: CORS is restricted to configured origins\n      Given the environment variable \"ALLOWED_ORIGINS\" is set to \"https://app.example.com\"\n      When I send an OPTIONS request with \"Origin: https://app.example.com\"\n      Then the response has header \"Access-Control-Allow-Origin\" with value \"https://app.example.com\"\n\n    Scenario: CORS rejects unconfigured origins\n      Given the environment variable \"ALLOWED_ORIGINS\" is set to \"https://app.example.com\"\n      When I send an OPTIONS request with \"Origin: https://evil.example.com\"\n      Then the response does not have header \"Access-Control-Allow-Origin\"\n\n    Scenario: CORS allows all origins when not configured (development)\n      Given the environment variable \"ALLOWED_ORIGINS\" is not set\n      When I send an OPTIONS request with \"Origin: http://localhost:3000\"\n      Then the response has header \"Access-Control-Allow-Origin\" with value \"*\"\n\n  # â”€â”€ Request Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API requests are logged for audit and debugging\n\n    Scenario: Successful request is logged\n      Given the API server is running with request logging enabled\n      And a valid API key \"rmap_log\" with scope \"read\"\n      When I send a GET request to \"/api/components\" with that key\n      Then the request log contains an entry with:\n        | field      | value                |\n        | method     | GET                  |\n        | path       | /api/components      |\n        | status     | 200                  |\n        | key_name   | log-key-name         |\n        | duration   | (positive integer)   |\n        | request_id | (UUID)               |\n\n    Scenario: Failed authentication is logged\n      Given the API server is running with request logging enabled\n      When I send a GET request with an invalid API key\n      Then the request log contains an entry with status 401\n      And the log entry does not contain the attempted key value\n\n    Scenario: Rate-limited request is logged\n      Given the API server is running with request logging enabled\n      When a request is rejected due to rate limiting\n      Then the request log contains an entry with status 429\n      And the log entry includes the key name\n\n    Scenario: Request body is not logged for security\n      Given the API server is running with request logging enabled\n      When I send a POST request with a JSON body\n      Then the request log does not contain the request body\n      And the request log does not contain any API key values\n\n    Scenario: Logs include correlation ID for tracing\n      Given the API server is running\n      When I send a request with header \"X-Request-Id: custom-trace-123\"\n      Then the response has header \"X-Request-Id\" with value \"custom-trace-123\"\n      And the request log entry has request_id \"custom-trace-123\"\n\n  # â”€â”€ Input Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: All API inputs are validated and sanitised\n\n    Scenario: Reject request body exceeding maximum size\n      Given the API server is running with max body size of 1MB\n      When I send a POST request with a body larger than 1MB\n      Then the response status is 413\n      And the response body has field \"error\" with value \"Request body too large\"\n\n    Scenario: Reject malformed JSON body\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body \"not json\"\n      Then the response status is 400\n      And the response body has field \"error\" with value \"Invalid JSON body\"\n\n    Scenario: Reject request with path traversal attempt\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components/../../../etc/passwd\"\n      Then the response status is 400\n      And the response body has field \"error\"\n\n    Scenario: Strip HTML from string inputs\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request with name \"<script>alert(1)</script>\"\n      Then the stored name does not contain HTML tags\n      And script content is stripped\n\n    Scenario: Validate component ID format\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request with id \"invalid id with spaces!\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"invalid\"\n\n    Scenario: Component ID must be kebab-case\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"valid-kebab-case\",\"name\":\"Valid\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n\n  # â”€â”€ Error Response Format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Error responses follow a consistent format\n\n    Scenario: All error responses have a consistent structure\n      Given the API server is running\n      When any API request results in an error\n      Then the response body has this structure:\n        \"\"\"\n        {\n          \"error\": \"<human-readable message>\",\n          \"code\": \"<MACHINE_READABLE_CODE>\",\n          \"request_id\": \"<uuid>\"\n        }\n        \"\"\"\n\n    Scenario: Error codes map to HTTP status codes\n      Then the following error codes exist:\n        | code                  | status | description               |\n        | AUTHENTICATION_REQUIRED | 401  | No API key provided       |\n        | INVALID_API_KEY       | 401    | Key not found or invalid  |\n        | API_KEY_EXPIRED       | 401    | Key past expiry date      |\n        | API_KEY_REVOKED       | 401    | Key manually revoked      |\n        | INSUFFICIENT_SCOPE    | 403    | Key lacks required scope  |\n        | RATE_LIMIT_EXCEEDED   | 429    | Too many requests         |\n        | VALIDATION_ERROR      | 400    | Invalid input data        |\n        | NOT_FOUND             | 404    | Resource not found        |\n        | CONFLICT              | 409    | Duplicate resource        |\n        | BODY_TOO_LARGE        | 413    | Request body exceeds limit|\n        | INTERNAL_ERROR        | 500    | Unexpected server error   |\n\n    Scenario: Internal errors do not leak implementation details\n      Given the API server is running\n      When an unexpected error occurs during request handling\n      Then the response status is 500\n      And the response body error message is \"Internal server error\"\n      And the response does not contain stack traces\n      And the response does not contain file paths\n      And the full error is logged server-side with the request_id\n"
          }
        ]
      }
    },
    {
      "id": "observability-dashboard",
      "name": "Observability Dashboard",
      "type": "layer",
      "layer": null,
      "color": "sky",
      "icon": "ðŸ“Š",
      "description": "Runtime Visibility â€” read-only web UI that observes but never mutates. The single pane of glass for both observation and control.",
      "tags": [
        "read-only",
        "web ui",
        "live updates"
      ],
      "sort_order": 10,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Read-only web UI. Process view, goal feed, tool timeline, security events, graph explorers. The dashboard is a thin web UI (React / plain HTML) that reads from two sources: the State Store (goals, tasks, tool logs, escalations, checkpoints) and the Supervisor's health API (process status, heartbeat data, resource usage). It writes nothing â€” pure read-only observer. Optional: SSE/WebSocket push from State Store for live updates without polling. The Human Gate approval actions can be embedded here too, making it the single pane of glass for both observation and control.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "live-dashboard",
      "name": "Live Dashboard",
      "type": "app",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "ðŸ“Š",
      "description": "Real-time view of the entire runtime. Read-only â€” it observes but never mutates. Built as a simple web app (React / plain HTML) that polls the State Store + Supervisor health API. Runs as a separate process managed by the Supervisor. Think: the runtime equivalent of the process tree diagram, but live. Reads from two sources: State Store (goals, tasks, tool logs, escalations, checkpoints) and Supervisor health API (process status, heartbeat data, resource usage). It writes nothing â€” pure read-only observer. Optional: SSE/WebSocket push from State Store for live updates without polling. Human Gate approval actions can be embedded here, making it the single pane of glass for both observation and control.",
      "tags": [
        "read-only",
        "web ui",
        "sse/websocket"
      ],
      "sort_order": 11,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Single-page web app showing process status (up/down) and current goal. Polls Supervisor health API every 5s. Basic goal queue display from State Store. Static HTML + vanilla JS.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Real-time view of the entire runtime. Read-only â€” it observes but never mutates. Built as a simple web app (React / plain HTML) that polls the State Store + Supervisor health API. Runs as a separate process managed by the Supervisor. Think: the runtime equivalent of the process tree diagram, but live. Data sources: State Store â†’ goals, tasks, tool call logs, escalations, checkpoints, fast-path records, injection events. Supervisor Health API â†’ process status, uptime, restart count, memory, current model per instance. No direct process inspection â€” dashboard never connects to OpenCode or proxies directly. Push vs Poll: SSE from State Store for live updates; poll Supervisor health every 5s. Human Gate embedded: approval buttons for gated tasks + escalation responses in same UI.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Full process tree view with live status. Goal & task feed with click-to-inspect. Tool call timeline with filtering. Security events panel. Escalation queue with response actions.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "SSE/WebSocket for real-time push updates. Entity Explorer for User KG. Repo Map for Code Graph. Embedded Human Gate approval UI. Performance metrics and resource graphs.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-dashboard.feature",
            "title": "Live Dashboard (MVP)",
            "content": "Feature: Live Dashboard (MVP)\n  A read-only web UI showing process status and current goal.\n  Polls the Supervisor health API and State Store.\n\n  Background:\n    Given the Dashboard web app is running\n\n  Scenario: Display process status\n    Given the Supervisor health API reports Meta-Agent as \"running\" and Worker as \"running\"\n    When the Dashboard polls the health API\n    Then both processes are shown with \"running\" status indicators\n\n  Scenario: Display current goal\n    Given the State Store contains a goal \"Build user auth\" with status \"in-progress\"\n    When the Dashboard polls the State Store\n    Then the current goal \"Build user auth\" is displayed\n    And its status shows \"in-progress\"\n\n  Scenario: Auto-refresh on interval\n    Given the Dashboard is displaying process status\n    When 5 seconds have elapsed\n    Then the Dashboard polls the health API again\n    And the display updates with fresh data\n\n  Scenario: Show offline state\n    Given the Supervisor health API is unreachable\n    When the Dashboard polls the health API\n    Then a \"Supervisor Unreachable\" indicator is shown\n\n  Scenario: Read-only â€” no mutation endpoints\n    Given the Dashboard is running\n    Then it exposes no POST, PUT, or DELETE endpoints\n    And all data access is via GET requests\n"
          }
        ]
      }
    },
    {
      "id": "live-process-view",
      "name": "Live Process View",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "ðŸ”´",
      "description": "Process tree with real-time status: running, recovering, crashed, paused for every child. Uptime, restart count, current model, memory usage per instance.",
      "tags": [
        "running",
        "recovering",
        "crashed",
        "paused"
      ],
      "sort_order": 12,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Process tree with real-time status: running, recovering, crashed, paused for every child. Uptime, restart count, current model, memory usage per instance.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "goal-task-feed",
      "name": "Goal & Task Feed",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "ðŸ“‹",
      "description": "Live stream of the Goal Queue. Current goal, decomposed sub-tasks, completion status. See what the Meta-Agent is planning and what the Worker is executing. Clickable to inspect full task payloads.",
      "tags": [
        "live stream",
        "clickable",
        "task payloads"
      ],
      "sort_order": 13,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Live stream of the Goal Queue. Current goal, decomposed sub-tasks, completion status. See what the Meta-Agent is planning and what the Worker is executing. Clickable to inspect full task payloads.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-call-timeline",
      "name": "Tool Call Timeline",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "ðŸ”§",
      "description": "Chronological feed of every tool call (both instances). Shows: tool name, args (truncated), response status, latency, sanitiser verdict (pass/block). Filterable by instance, tool, and status. This is your debugging lifeline.",
      "tags": [
        "chronological",
        "filterable",
        "debugging lifeline"
      ],
      "sort_order": 14,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Chronological feed of every tool call (both instances). Shows: tool name, args (truncated), response status, latency, sanitiser verdict (pass/block). Filterable by instance, tool, and status. This is your debugging lifeline.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "security-events",
      "name": "Security Events",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "ðŸ›¡",
      "description": "Sanitiser verdicts, blocked injections with raw payload preview, injection frequency per tool, auto-disable events. Links to full audit log entries. Alerts when injection rate exceeds threshold.",
      "tags": [
        "audit log",
        "injection frequency",
        "auto-disable"
      ],
      "sort_order": 15,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Sanitiser verdicts, blocked injections with raw payload preview, injection frequency per tool, auto-disable events. Links to full audit log entries. Alerts when injection rate exceeds threshold.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "escalation-queue",
      "name": "Escalation Queue",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "â¸",
      "description": "Worker escalation requests waiting for Meta-Agent or human review. Shows the Worker's question, context snapshot, and available actions: respond, override, or abort task.",
      "tags": [
        "respond",
        "override",
        "abort"
      ],
      "sort_order": 16,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Worker escalation requests waiting for Meta-Agent or human review. Shows the Worker's question, context snapshot, and available actions: respond, override, or abort task.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "entity-explorer",
      "name": "Entity Explorer",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "ðŸ‘¤",
      "description": "Browse the User Knowledge Graph. See people, projects, preferences, and their relationships. Understand what the agents \"know about you\".",
      "tags": [
        "new",
        "user kg",
        "browse"
      ],
      "sort_order": 17,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Browse the User Knowledge Graph. See people, projects, preferences, and their relationships. Understand what the agents \"know about you\".",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "repo-map",
      "name": "Repo Map",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "ðŸ—º",
      "description": "Visualise the Code Graph. Module hierarchy, file deps, data flows. See the agent's structural understanding of your codebase.",
      "tags": [
        "new",
        "code graph",
        "visualise"
      ],
      "sort_order": 18,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Visualise the Code Graph. Module hierarchy, file deps, data flows. See the agent's structural understanding of your codebase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "human-gate-dashboard",
      "name": "Human Gate (Dashboard)",
      "type": "component",
      "layer": "observability-dashboard",
      "color": "sky",
      "icon": "â›³",
      "description": "Approval queue + escalation responses. Gate actions embeddable in dashboard UI.",
      "tags": [
        "approval",
        "embedded"
      ],
      "sort_order": 19,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Approval queue + escalation responses. Gate actions embeddable in dashboard UI.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "supervisor-layer",
      "name": "Supervisor",
      "type": "layer",
      "layer": null,
      "color": "purple",
      "icon": "ðŸ‘",
      "description": "The Only Immortal Process â€” process management, signal handling, recovery state machine. No LLM, no planning. If it dies, systemd/Docker restarts it.",
      "tags": [
        "immortal",
        "no llm",
        "process manager"
      ],
      "sort_order": 20,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "The only immortal process. No LLM. Process manager + crash recovery + heartbeat. Exposes health API on :9100. Kill switch: /stop HTTP endpoint + SIGTERM handler. If it dies, systemd/Docker restarts it. Recovery sequence on crash detection: (1) Kill hung process if still alive. (2) Read last checkpoint from State Store. (3) Rebuild config via DSL. (4) Inject resume context via Context Rebuilder. (5) Respawn fresh OpenCode process (new PID, clean slate, resume prompt). (6) Exponential backoff if repeated failures, max 5 retries â†’ alert Human Gate.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "supervisor",
      "name": "Supervisor",
      "type": "app",
      "layer": "supervisor-layer",
      "color": "purple",
      "icon": "ðŸ‘",
      "description": "Manages all child processes. Heartbeat + crash recovery with tiered priority. No LLM, no planning â€” just process management, signal handling, and the recovery state machine. This is what makes it stable: it has almost no reasons to crash. If it does, systemd/Docker restarts it. Exposes a health API (HTTP on :9100) for the dashboard. Kill switch: /stop HTTP endpoint + SIGTERM handler â†’ instant halt of all children. Emergency brake for runaway agents.",
      "tags": [
        "immortal",
        "health api",
        "kill switch",
        "no llm"
      ],
      "sort_order": 21,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Spawn and monitor two child processes (meta-agent, worker). Detect crashes via waitpid(). Restart crashed children with basic retry logic. Expose /health HTTP endpoint returning JSON process status. Handle SIGTERM for graceful shutdown of all children.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Manages all child processes. Heartbeat + crash recovery with tiered priority. No LLM, no planning â€” just process management, signal handling, and the recovery state machine. This is what makes it stable: it has almost no reasons to crash. If it does, systemd/Docker restarts it. Exposes a health API (HTTP on :9100) for the dashboard. Kill switch: /stop HTTP endpoint + SIGTERM handler â†’ instant halt of all children. Why a Supervisor, not a cron job? (1) Instant detection: waitpid() returns the moment a child exits. Cron's worst-case latency = poll interval. (2) Crash loop handling: Supervisor tracks restart count + applies exponential backoff. (3) Multi-step recovery: kill â†’ checkpoint read â†’ config rebuild â†’ context inject â†’ respawn. (4) Lifecycle ownership: Supervisor owns the full process tree â€” PIDs, health, state. (5) Signal handling: catches SIGTERM/SIGCHLD and coordinates graceful shutdown. Stability hierarchy: Tier âˆž (Supervisor) immortal â†’ Tier 0 (Meta-Agent) recovered first â†’ Tier 1 (Worker) expendable. Recovery order: Supervisor â†’ Meta-Agent â†’ Worker. Each tier can recover the one below it.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Add exponential backoff on repeated crashes. Liveness probe (hang detection via output timeout_ms). Recovery state machine with tiered priority (meta-agent first). Checkpoint-aware recovery â€” read last checkpoint before respawn. Human Gate alerting after max 5 retries.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Full config-as-code DSL for spawn configuration. Resource monitoring (memory, CPU per child). Kill switch /stop HTTP endpoint. Dashboard SSE push for process events. Per-instance gate policies. Runtime flag switching for gate modes.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-process-management.feature",
            "title": "Supervisor Process Management (MVP)",
            "content": "Feature: Supervisor Process Management (MVP)\n  The Supervisor spawns and monitors child processes.\n  It detects crashes and restarts children with basic retry logic.\n\n  Background:\n    Given the Supervisor process is running\n\n  Scenario: Spawn child processes on startup\n    When the Supervisor starts\n    Then it spawns the Meta-Agent process\n    And it spawns the Worker process\n    And both processes are in \"running\" state\n\n  Scenario: Detect child crash via waitpid\n    Given the Meta-Agent process is running\n    When the Meta-Agent process exits unexpectedly\n    Then the Supervisor detects the exit within 100ms\n    And the exit is logged with the process ID and exit code\n\n  Scenario: Restart crashed child\n    Given the Worker process has crashed\n    When the Supervisor detects the crash\n    Then it restarts the Worker process\n    And the new process is in \"running\" state\n    And the restart count is incremented\n\n  Scenario: Respect maximum retry limit\n    Given the Worker has crashed 5 times consecutively\n    When the Worker crashes again\n    Then the Supervisor does not restart the Worker\n    And the Worker state is set to \"failed\"\n    And an alert is logged\n\n  Scenario: Health API returns process status\n    Given both child processes are running\n    When a GET request is made to /health\n    Then the response status is 200\n    And the response body contains status for each child process\n    And each status includes \"pid\", \"state\", and \"uptime\"\n\n  Scenario: Graceful shutdown on SIGTERM\n    Given both child processes are running\n    When the Supervisor receives SIGTERM\n    Then it sends SIGTERM to all child processes\n    And it waits for children to exit\n    And it exits with code 0\n"
          }
        ]
      }
    },
    {
      "id": "dual-heartbeat",
      "name": "Dual Heartbeat",
      "type": "component",
      "layer": "supervisor-layer",
      "color": "purple",
      "icon": "ðŸ’“",
      "description": "Monitors both OpenCode instances independently via waitpid() + liveness probes. Instant crash detection (zero latency) â€” waitpid() returns the moment a child exits. Periodic liveness probe for hang detection â€” if no output for timeout_ms, treat as hung. Detects: exit, hang, OOM. If the Worker crashes â†’ recover using Meta-Agent's last plan. If the Meta-Agent crashes â†’ recover it first (higher priority), then it re-dispatches the Worker. Exponential backoff, max 5 retries â†’ alert Human Gate.",
      "tags": [
        "waitpid",
        "zero latency",
        "meta first",
        "worker second",
        "exponential backoff"
      ],
      "sort_order": 22,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "waitpid() loop for crash detection. Basic restart on exit. Retry counter with max limit. Log crash events.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Monitors both OpenCode instances independently via waitpid() + liveness probes. Instant crash detection (zero latency) â€” waitpid() returns the moment a child exits. Periodic liveness probe for hang detection â€” if no output for timeout_ms, treat as hung. Detects: exit, hang, OOM. If Worker crashes â†’ recover using Meta-Agent's last plan. If Meta-Agent crashes â†’ recover it first (higher priority), then it re-dispatches Worker. Exponential backoff, max 5 retries â†’ alert Human Gate.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "human-gate",
      "name": "Human Gate",
      "type": "app",
      "layer": "supervisor-layer",
      "color": "pink",
      "icon": "â›³",
      "description": "Three modes: full-auto, approve-goals, approve-all. Plus write fence: dangerous ops require approval even in full-auto. Also surfaces escalation requests from the Worker. Write fence per-instance: Meta-Agent config mutations and Worker destructive ops can have independent gate policies. Gate mode is a runtime flag â€” switch between modes without restarting any process.",
      "tags": [
        "full-auto",
        "approve-goals",
        "approve-all",
        "write fence",
        "runtime flag"
      ],
      "sort_order": 23,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Basic approval queue. CLI-based approve/reject. Write fence for destructive operations (hardcoded list). Block until approved or timeout.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Three modes: full-auto, approve-goals, approve-all. Plus write fence: dangerous ops require approval even in full-auto. Also surfaces escalation requests from the Worker. Write fence per-instance: Meta-Agent config mutations and Worker destructive ops have independent gate policies. Gate mode is a runtime flag â€” switch between modes without restarting.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Runtime mode switching (full-auto, approve-goals, approve-all). Per-instance gate policies. Escalation forwarding from Meta-Agent. Dashboard-embeddable approval UI.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Configurable write fence per tool category. Approval delegation rules. Audit trail of all gate decisions. Timeout policies with configurable fallback actions.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-human-gate.feature",
            "title": "Human Gate (MVP)",
            "content": "Feature: Human Gate (MVP)\n  The Human Gate provides an approval queue for dangerous\n  operations and a write fence for destructive actions.\n\n  Background:\n    Given the Human Gate is running\n\n  Scenario: Block destructive operation for approval\n    Given the write fence includes \"database drop\" operations\n    When a task requests a database drop\n    Then the task is paused with status \"awaiting_approval\"\n    And the approval request is added to the queue\n\n  Scenario: Approve pending request\n    Given a task is paused awaiting approval\n    When a human approves the request\n    Then the task status changes to \"approved\"\n    And execution resumes\n\n  Scenario: Reject pending request\n    Given a task is paused awaiting approval\n    When a human rejects the request\n    Then the task status changes to \"rejected\"\n    And the task is aborted\n\n  Scenario: Timeout on unanswered request\n    Given a task has been awaiting approval for longer than the timeout\n    When the timeout expires\n    Then the task is aborted\n    And the timeout event is logged\n"
          }
        ]
      }
    },
    {
      "id": "fast-path-router",
      "name": "Fast Path Router",
      "type": "component",
      "layer": "supervisor-layer",
      "color": "lime",
      "icon": "âš¡",
      "description": "Rule engine (no LLM). Classifies tasks as fast, full, or gated. Scores incoming tasks by complexity signals: single-step? (e.g. \"format this file\"), no ambiguity? (clear input/output), no tool mutation needed? (current tools suffice). If all signals pass â†’ direct to Worker, skipping Meta-Agent. Meta-Agent notified after completion via State Store. Cuts latency and cost for simple tasks by ~50%. Configurable: fast_path: \"aggressive\" | \"conservative\" | \"off\". Can query User KG for context (\"does user prefer X for this type of task?\"). If fast-path task fails, re-routed through Meta-Agent.",
      "tags": [
        "fast",
        "full",
        "gated",
        "rule engine",
        "~50% savings",
        "new"
      ],
      "sort_order": 24,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Simple rule engine: match task text against patterns (single verb, no conditionals, target file exists). Three outputs: fast, full, gated. Configurable threshold.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Rule engine (no LLM). Classifies tasks as fast, full, or gated. Scores by complexity signals: single-step, no ambiguity, no tool mutation needed. Configurable: fast_path: \"aggressive\" | \"conservative\" | \"off\". Can query User KG for context. Cuts latency and cost ~50% for simple tasks. Fallback: failed fast-path tasks re-route through Meta-Agent.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "task-router-layer",
      "name": "Task Router",
      "type": "layer",
      "layer": null,
      "color": "lime",
      "icon": "âš¡",
      "description": "Fast Path Decision Point â€” lightweight rule engine (no LLM) routes tasks by complexity: trivial goes direct to Worker, complex goes to Meta-Agent, dangerous requires human approval.",
      "tags": [
        "rule engine",
        "no llm",
        "classifier"
      ],
      "sort_order": 30,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Fast path decision point. Rule engine (no LLM) routes tasks by complexity. Classifier is NOT an LLM â€” it's a rule engine (regex + heuristics on task text). Signals: single verb (\"format\", \"lint\", \"rename\"), no conditional language, target file exists, current tools suffice. Configurable threshold: fast_path: \"aggressive\" | \"conservative\" | \"off\". Meta-Agent stays aware: fast-path completions logged to State Store. Fallback: if fast-path task fails, re-routed through Meta-Agent.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "fast-path",
      "name": "Fast Path",
      "type": "component",
      "layer": "task-router-layer",
      "color": "lime",
      "icon": "âš¡",
      "description": "Rule engine says: single-step, unambiguous, existing tools suffice. Task goes directly to Worker. Meta-Agent notified post-completion via State Store. Flow: task â†’ classifier â†’ FAST â†’ Worker â†’ done â†’ State Store â†’ Meta-Agent reads.",
      "tags": [
        "trivial",
        "direct",
        "skip planner"
      ],
      "sort_order": 31,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Rule engine says: single-step, unambiguous, existing tools suffice. Task goes directly to Worker. Meta-Agent notified post-completion via State Store. Flow: task â†’ classifier â†’ FAST â†’ Worker â†’ done â†’ State Store â†’ Meta-Agent reads.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "full-path",
      "name": "Full Path",
      "type": "component",
      "layer": "task-router-layer",
      "color": "orange",
      "icon": "ðŸ§ ",
      "description": "Classifier says: multi-step, ambiguous, or needs tool changes. Task goes to Meta-Agent for decomposition. Normal planning loop. Flow: task â†’ classifier â†’ FULL â†’ Meta-Agent â†’ plan â†’ dispatch â†’ Worker â†’ State Store.",
      "tags": [
        "complex",
        "decomposition",
        "planning loop"
      ],
      "sort_order": 32,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Classifier says: multi-step, ambiguous, or needs tool changes. Task goes to Meta-Agent for decomposition. Normal planning loop. Flow: task â†’ classifier â†’ FULL â†’ Meta-Agent â†’ plan â†’ dispatch â†’ Worker â†’ State Store.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "gated-path",
      "name": "Gated Path",
      "type": "component",
      "layer": "task-router-layer",
      "color": "purple",
      "icon": "â›³",
      "description": "Classifier or Human Gate flags: destructive, high-cost, or security-sensitive. Task pauses for human approval before any routing. Flow: task â†’ classifier â†’ GATE â†’ Human Gate â†’ approve â†’ (fast or full path).",
      "tags": [
        "dangerous",
        "approval required",
        "security-sensitive"
      ],
      "sort_order": 33,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Classifier or Human Gate flags: destructive, high-cost, or security-sensitive. Task pauses for human approval. Flow: task â†’ classifier â†’ GATE â†’ Human Gate â†’ approve â†’ (fast or full path).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "knowledge-graphs",
      "name": "Knowledge Graphs",
      "type": "layer",
      "layer": null,
      "color": "gold",
      "icon": "ðŸ§ ",
      "description": "Dual graph stores: User Knowledge Graph (domain context â€” people, projects, preferences) + RPG Code Graph (repo structure â€” files, modules, deps, data flows).",
      "tags": [
        "dual stores",
        "domain context",
        "repo structure"
      ],
      "sort_order": 40,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Dual graph stores. User Knowledge Graph holds domain context (people, projects, preferences). RPG Code Graph holds repo structure (files, modules, deps, data flows). Three stores, three purposes: State Store (operational â€” what's happening now, prunable), User KG (domain â€” who you are, long-lived), Code Graph (repo structure â€” what the codebase looks like, disposable and re-derivable from code).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "user-knowledge-graph",
      "name": "User Knowledge Graph",
      "type": "app",
      "layer": "knowledge-graphs",
      "color": "gold",
      "icon": "ðŸ‘¤",
      "description": "A persistent graph of the user's world. Nodes are domain entities â€” people, projects, clients, teams, products, preferences, business rules, conventions, deadlines. Edges are typed relationships with metadata. This is not about code â€” it's about understanding who you are and what you care about so agents make contextually appropriate decisions. Entity types: person, project, org, team, preference, convention, deadline, stack, compliance, product, domain-concept, decision. Relationship types: OWNS, PREFERS, WORKS_WITH, HAS_CLIENT, USES_STACK, REQUIRES, CONVENTION, HAS_DEADLINE, DECIDED, DISLIKES. Populated by: (1) User directly â€” onboarding flow or dashboard edits. (2) Meta-Agent â€” infers entities from conversations and patterns over time. (3) Never by Worker â€” same injection-safety principle. Worker reads, never writes. Confidence layering: user-explicit (1.0) > meta-agent-inferred (0.8) > auto-extracted (0.6).",
      "tags": [
        "new",
        "persistent",
        "entity types",
        "relationship types",
        "confidence layering"
      ],
      "sort_order": 41,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "SQLite-backed entity store. Add/query entities with typed relationships. Basic traversal (1-hop neighbours). Manual entity creation via CLI or dashboard. Simple text search across entities.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "A persistent graph of the user's world. Nodes are domain entities â€” people, projects, clients, teams, products, preferences, business rules, conventions, deadlines. Edges are typed relationships with metadata. This is not about code â€” it's about understanding who you are and what you care about so agents make contextually appropriate decisions. Entity types: person, project, org, team, preference, convention, deadline, stack, compliance, product, domain-concept, decision. Relationship types: OWNS, PREFERS, WORKS_WITH, HAS_CLIENT, USES_STACK, REQUIRES, CONVENTION, HAS_DEADLINE, DECIDED, DISLIKES. Populated by: (1) User directly â€” onboarding flow or dashboard edits. (2) Meta-Agent â€” infers entities from conversations and patterns. (3) Never by Worker â€” injection safety. Confidence layering: user-explicit (1.0) > meta-agent-inferred (0.8) > auto-extracted (0.6). What it solves: Personalisation (\"Alice prefers typed SQL over ORMs\"), project awareness (\"acme-saas uses Next.js + Postgres, client needs SOC2\"), team context (\"Bob is backend lead, prefers PRs\"), decision memory (\"We decided JWT over sessions on Jan 15\"), convention enforcement (\"No ORMs, minimal comments, Tailwind\"), deadline awareness (Meta-Agent prioritises based on known deadlines).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Meta-Agent write access for inferred entities. Confidence layering (user-explicit 1.0 > meta-inferred 0.8). Multi-hop traversal queries. Convention enforcement lookups. Deadline awareness queries.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Full graph query language. Temporal awareness (when was this preference set?). Conflict resolution for contradictory preferences. Export/import for portability. Dashboard entity editor.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-user-kg.feature",
            "title": "User Knowledge Graph (MVP)",
            "content": "Feature: User Knowledge Graph (MVP)\n  A persistent entity-relationship store for domain context:\n  people, projects, preferences, conventions, deadlines.\n\n  Background:\n    Given the User Knowledge Graph SQLite database exists\n\n  Scenario: Add an entity\n    When an entity is added with type \"person\" and name \"Alice\"\n    Then the entity exists in the graph with a unique ID\n    And it has type \"person\" and name \"Alice\"\n\n  Scenario: Add a relationship between entities\n    Given entities \"Alice\" (person) and \"acme-saas\" (project) exist\n    When a relationship \"OWNS\" is added from \"Alice\" to \"acme-saas\"\n    Then the edge exists with type \"OWNS\"\n    And it references both entities\n\n  Scenario: Query 1-hop neighbours\n    Given \"Alice\" has relationships to \"acme-saas\", \"Bob\", and \"minimal-comments\"\n    When querying neighbours of \"Alice\"\n    Then all 3 connected entities are returned\n    And each result includes the relationship type\n\n  Scenario: Search entities by text\n    Given entities \"Alice\", \"Bob\", and \"acme-saas\" exist\n    When searching for \"alice\"\n    Then the entity \"Alice\" is returned\n\n  Scenario: Add entity with metadata\n    When an entity is added with type \"preference\" name \"no-orms\" and metadata '{\"reason\": \"team decision\"}'\n    Then the entity exists with the metadata attached\n"
          }
        ]
      }
    },
    {
      "id": "rpg-code-graph",
      "name": "RPG Code Graph",
      "type": "app",
      "layer": "knowledge-graphs",
      "color": "emerald",
      "icon": "ðŸ—º",
      "description": "An RPG-style structural graph of the current codebase. Encodes file hierarchy, module boundaries, inter-module data flows, function signatures, class inheritance, and import dependencies. Inspired by Microsoft RPG/ZeroRepo (https://arxiv.org/abs/2509.16198). This is a code quality feature â€” it helps the Worker write structurally coherent code by understanding what exists, what depends on what, and where new code should go. Node types: module, file, function, class, interface, package, route, schema, test. Edge types: CONTAINS, IMPORTS, EXPORTS, DATA_FLOW, EXTENDS, IMPLEMENTS, DEPENDS_ON, TESTS, CALLS. Populated by: (1) Static analysis on init â€” AST parse via tree-sitter on repo load (~seconds for repos under 100K LoC). (2) Worker's Checkpointer â€” auto-updates after file edits (re-parse only changed files, diff old vs new, update edges incrementally). (3) Meta-Agent â€” can annotate with higher-level module boundaries and data flow intentions. Lightweight â€” no LLM needed for extraction. Query patterns: topo_order(module), data_flow(A,B), dependents(file), pattern(type,dir), where_to_add(capability).",
      "tags": [
        "new",
        "tree-sitter",
        "ast parsing",
        "disposable",
        "re-derivable",
        "query patterns"
      ],
      "sort_order": 42,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Static analysis on repo load using tree-sitter. Build initial graph from imports, exports, class hierarchy. Basic queries: list files in module, show imports for file. SQLite-backed.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "An RPG-style structural graph of the current codebase. Encodes file hierarchy, module boundaries, inter-module data flows, function signatures, class inheritance, and import dependencies. Inspired by Microsoft RPG/ZeroRepo (arxiv.org/abs/2509.16198). This is a code quality feature â€” it helps the Worker write structurally coherent code. Node types: module, file, function, class, interface, package, route, schema, test. Edge types: CONTAINS, IMPORTS, EXPORTS, DATA_FLOW, EXTENDS, IMPLEMENTS, DEPENDS_ON, TESTS, CALLS. What it solves: Dependency awareness (Worker knows what imports what before editing), placement decisions (\"where should rate limiting go?\" â†’ traverse moduleâ†’fileâ†’function), topological code generation (build in dependency order), data flow understanding (inter-module edges), pattern consistency (existing patterns visible), blast radius estimation (Meta-Agent traverses deps). Implementation: (1) Initial build on repo load â€” tree-sitter AST parse, extract files/imports/exports/classes/functions, infer modules from directory structure, ~seconds for repos under 100K LoC. (2) Incremental update on edit â€” Checkpointer detects file-edit tool calls, re-parses only changed files, diffs old vs new, updates edges incrementally. (3) Query patterns â€” topo_order(module), data_flow(A,B), dependents(file), pattern(type,dir), where_to_add(capability).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Incremental updates via Checkpointer (re-parse only changed files). Dependency traversal (topo_order, dependents). Data flow edges between modules. Pattern queries.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Full where_to_add capability suggestions. Blast radius estimation. Meta-Agent annotations. Multi-language AST support. Dashboard Repo Map visualization.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-code-graph.feature",
            "title": "RPG Code Graph (MVP)",
            "content": "Feature: RPG Code Graph (MVP)\n  Static analysis on repo load builds a structural graph of the\n  codebase: files, imports, exports, classes, functions.\n\n  Background:\n    Given a repository with source files exists\n\n  Scenario: Build initial graph from repo\n    When the Code Graph builder runs on the repository\n    Then nodes are created for each source file\n    And edges are created for import relationships\n    And the graph is stored in SQLite\n\n  Scenario: Extract function exports\n    Given a file \"auth/handler.ts\" exports function \"verifyToken\"\n    When the AST parser processes the file\n    Then a function node \"verifyToken\" exists\n    And an EXPORTS edge connects the file to the function\n\n  Scenario: Extract import relationships\n    Given \"api/routes.ts\" imports from \"auth/handler.ts\"\n    When the AST parser processes both files\n    Then an IMPORTS edge connects \"api/routes.ts\" to \"auth/handler.ts\"\n\n  Scenario: Infer modules from directory structure\n    Given the repository has directories \"auth/\", \"api/\", \"db/\"\n    When the Code Graph builder runs\n    Then module nodes are created for \"auth\", \"api\", \"db\"\n    And CONTAINS edges connect modules to their files\n\n  Scenario: Query files in a module\n    Given the module \"auth\" contains \"handler.ts\" and \"middleware.ts\"\n    When querying files in module \"auth\"\n    Then both files are returned\n"
          }
        ]
      }
    },
    {
      "id": "dual-agents",
      "name": "Dual OpenCode Instances",
      "type": "layer",
      "layer": null,
      "color": "orange",
      "icon": "ðŸ§ ",
      "description": "Two stock OpenCode instances. Meta-Agent (Planner) uses cheap/fast model (Haiku/Sonnet), plans and dispatches. Worker (Executor) uses strong model (Sonnet/Opus), executes one phase at a time. They share no tools â€” isolation by design.",
      "tags": [
        "stock opencode",
        "dual instances",
        "tool isolation"
      ],
      "sort_order": 50,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Two stock OpenCode instances. Meta-Agent (Planner) uses cheap model (Haiku/Sonnet), plans and dispatches. Worker (Executor) uses strong model (Sonnet/Opus), executes one phase at a time. Why two instances? (1) Separation of concerns: planning and execution have different tool sets, models, cost profiles, and risk levels. (2) Blast radius: a prompt injection in the Worker can't reach the Planner â€” they share no tools or MCP connection. (3) Independent recovery: Worker can crash and be re-dispatched without losing Meta-Agent's plan state. (4) Cost optimisation: Planner uses cheap model, Worker uses capable model. (5) Dogfooding: you build one runtime, not two systems. Both instances use the same config DSL, proxy, and checkpoint infra.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "meta-agent",
      "name": "Meta-Agent (Planner)",
      "type": "app",
      "layer": "dual-agents",
      "color": "orange",
      "icon": "ðŸ§ ",
      "description": "Stock OpenCode, planning system prompt, cheap/fast model (Haiku/Sonnet). This instance never touches the codebase or external APIs directly. It only plans, evaluates, and dispatches. Tier-0: recovered first. If this is down, the Worker has no direction. Only internal tools, no injection surface. Traverses User KG to align plans with user preferences, deadlines, team context. Reads Code Graph to understand repo structure before decomposing coding tasks. Also handles escalation responses: reads Worker's request_clarification entries from State Store, reasons about them, and writes guidance back â€” which the Worker receives on its next checkpoint resume or via check_escalation_response tool. System prompt: \"You are a task planner. Use your tools to: read the goal queue, check worker status, decompose goals into tasks, dispatch tasks, evaluate results, and generate follow-up goals. You may also evolve the worker's tools and config when needed.\" The loop emerges from the prompt + tool availability.",
      "tags": [
        "tier-0",
        "10 internal tools",
        "reads/writes user kg",
        "reads code graph",
        "haiku/sonnet",
        "new"
      ],
      "sort_order": 51,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Single OpenCode instance with planning system prompt. Read goal queue, decompose into sub-tasks, dispatch to Worker via State Store. Read Worker progress from checkpoints. Basic goal â†’ task decomposition.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Stock OpenCode, planning system prompt, cheap/fast model (Haiku/Sonnet). This instance never touches the codebase or external APIs directly. It only plans, evaluates, and dispatches. Tier-0: recovered first. If this is down, the Worker has no direction. Only internal tools, no injection surface. Traverses User KG to align plans with user preferences, deadlines, team context. Reads Code Graph to understand repo structure before decomposing coding tasks. Also handles escalation responses: reads Worker's request_clarification entries from State Store, reasons about them, and writes guidance back. System prompt: \"You are a task planner. Use your tools to: read the goal queue, check worker status, decompose goals into tasks, dispatch tasks, evaluate results, and generate follow-up goals. You may also evolve the worker's tools and config when needed.\" The loop emerges from the prompt + tool availability. How Meta-Agent uses both graphs: Before planning â€” \"What stack? Deadlines? Preferences?\" Before decomposing â€” \"Which modules? Dependency order? Blast radius?\" Task dispatch enrichment â€” includes relevant KG + Code Graph context in Worker's task prompt. Tool selection â€” User KG says \"prefers Brave over Google\" â†’ configures proxy. Knowledge curation â€” writes inferred preferences to User KG. Self-evolution: tool discovery via tool_registry.search, config mutation via typed builder DSL (not raw JSON), sub-goal generation, prompt evolution based on observed results. Guardrails: budget limits, scope limits, allowed tool categories, model whitelist.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Phase-locked BDD/TDD dispatch pipeline. Gate verification between phases. Escalation response handling. User KG reads for planning context. Code Graph reads for repo-aware decomposition.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Self-evolution: tool discovery + hot-swap via proxy_admin. Config mutation via DSL. Prompt evolution based on observed results. Knowledge curation â€” write inferred preferences to User KG. Budget and scope guardrails.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-planning-loop.feature",
            "title": "Meta-Agent Planning Loop (MVP)",
            "content": "Feature: Meta-Agent Planning Loop (MVP)\n  The Meta-Agent reads goals, decomposes them into tasks,\n  and dispatches work to the Worker via the State Store.\n\n  Background:\n    Given the Meta-Agent is running with a planning system prompt\n    And the State Store is accessible\n\n  Scenario: Read next goal from queue\n    Given the goal queue contains \"Build user authentication\"\n    When the Meta-Agent checks the goal queue\n    Then it receives the goal \"Build user authentication\"\n    And the goal status is set to \"in-progress\"\n\n  Scenario: Decompose goal into tasks\n    Given the Meta-Agent has received the goal \"Build user authentication\"\n    When it decomposes the goal\n    Then the State Store contains at least 2 sub-tasks\n    And each sub-task has a description and ordering\n\n  Scenario: Dispatch task to Worker\n    Given a sub-task \"Create login endpoint\" exists in the State Store\n    When the Meta-Agent dispatches the task\n    Then the task status is set to \"dispatched\"\n    And the task includes a description and success criteria\n\n  Scenario: Read Worker progress\n    Given a task has been dispatched to the Worker\n    When the Meta-Agent checks Worker progress\n    Then it receives the latest checkpoint for that task\n    And the checkpoint includes tool calls made and their results\n\n  Scenario: Complete goal when all tasks done\n    Given all sub-tasks for a goal are in \"complete\" status\n    When the Meta-Agent evaluates the goal\n    Then the goal status is set to \"complete\"\n    And the Meta-Agent reads the next goal from the queue\n"
          }
        ]
      }
    },
    {
      "id": "worker",
      "name": "Worker (Executor)",
      "type": "app",
      "layer": "dual-agents",
      "color": "cyan",
      "icon": "âš¡",
      "description": "Stock OpenCode, execution system prompt, strong model (Sonnet/Opus). Tier-1, ephemeral, no fork. Lower stability priority â€” if it crashes, the Meta-Agent re-dispatches. Treated as ephemeral and replaceable. On recovery, the agent continues without knowing it crashed â€” the Context Rebuilder injects a resume prompt that makes it look like a natural continuation. External tools, sanitiser required. Reads User KG to respect user preferences during execution (naming conventions, tech choices). Traverses Code Graph to write structurally coherent code (dependency-aware edits, correct placement). Has two escalation tools: request_clarification to pause and ask the planner for guidance, and check_escalation_response to poll for an answer. System prompt: \"If you're uncertain about scope, direction, or trade-offs, use request_clarification. Don't guess â€” ask.\" Receives tasks as structured system prompt injections: phase, task, constraints, forbidden_actions, available_tools, success_criteria. Single-phase isolation: the Worker sees \"Write failing step tests for this feature file. DO NOT implement any production code.\" It literally cannot skip ahead because it doesn't know what \"ahead\" is. The forbidden_actions field explicitly lists what it must not do (e.g. [\"create production files\", \"modify existing src/\", \"run tests in watch mode\"]).",
      "tags": [
        "tier-1",
        "dynamic tools",
        "reads user kg",
        "reads code graph",
        "sanitiser required",
        "ephemeral",
        "no fork",
        "sonnet/opus",
        "new"
      ],
      "sort_order": 52,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Single OpenCode instance with execution system prompt. Receive task from State Store, execute with available tools, report results. Basic tool access via MCP proxy.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Stock OpenCode, execution system prompt, strong model (Sonnet/Opus). Tier-1, ephemeral, no fork. Lower stability priority â€” if it crashes, the Meta-Agent re-dispatches. Treated as ephemeral and replaceable. On recovery, the agent continues without knowing it crashed â€” the Context Rebuilder injects a resume prompt that makes it look like a natural continuation. External tools, sanitiser required. Reads User KG to respect user preferences during execution. Traverses Code Graph for structurally coherent code. Has escalation tools: request_clarification and check_escalation_response. System prompt: \"If you're uncertain about scope, direction, or trade-offs, use request_clarification. Don't guess â€” ask.\" Receives tasks as structured injections: phase, task, constraints, forbidden_actions, available_tools, success_criteria. Single-phase isolation: Worker sees only current phase, never what comes next. Example forbidden_actions: [\"create production files\", \"modify existing src/\", \"run tests in watch mode\"]. The Worker literally cannot skip ahead because it doesn't know what \"ahead\" is.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Phase-locked execution (single phase per dispatch, forbidden_actions enforcement). Escalation tools (request_clarification, check_escalation_response). User KG reads for preference-aware execution. Code Graph reads for structural coherence.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Context-aware resume after crash (transparent to agent). Dynamic tool manifest â€” handles hot-swap mid-session. Confidence scoring on outputs. Full sanitiser integration on all external I/O.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-task-execution.feature",
            "title": "Worker Task Execution (MVP)",
            "content": "Feature: Worker Task Execution (MVP)\n  The Worker receives tasks from the State Store and executes\n  them using available tools via the MCP proxy.\n\n  Background:\n    Given the Worker is running with an execution system prompt\n    And the MCP proxy is accessible with at least one tool\n\n  Scenario: Receive dispatched task\n    Given a task \"Create login endpoint\" is in \"dispatched\" status\n    When the Worker checks for pending tasks\n    Then it receives the task with description and constraints\n\n  Scenario: Execute task with tools\n    Given the Worker has received a task\n    When it executes the task\n    Then it makes at least one tool call via the MCP proxy\n    And each tool call is logged to the State Store\n\n  Scenario: Report task completion\n    Given the Worker has finished executing a task\n    When it reports results\n    Then the task status is set to \"complete\"\n    And the result summary is written to the State Store\n\n  Scenario: Handle tool call failure\n    Given the Worker is executing a task\n    When a tool call returns an error\n    Then the Worker logs the error\n    And it attempts an alternative approach or reports failure\n\n  Scenario: Operate within provided constraints\n    Given the Worker receives a task with forbidden_actions [\"delete files\"]\n    When it executes the task\n    Then it does not call any tool that would delete files\n"
          }
        ]
      }
    },
    {
      "id": "goal-queue",
      "name": "goal_queue",
      "type": "component",
      "layer": "dual-agents",
      "color": "orange",
      "icon": "ðŸ“‹",
      "description": "push, pop, peek, reprioritise â€” manages the persistent goal queue.",
      "tags": [
        "meta-agent tool"
      ],
      "sort_order": 53,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "push, pop, peek, reprioritise â€” manages the persistent goal queue.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "state-reader",
      "name": "state_reader",
      "type": "component",
      "layer": "dual-agents",
      "color": "orange",
      "icon": "ðŸ“–",
      "description": "get_checkpoint, get_task_log, get_escalations â€” reads Worker's progress.",
      "tags": [
        "meta-agent tool"
      ],
      "sort_order": 54,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "get_checkpoint, get_task_log, get_escalations â€” reads Worker's progress.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "worker-control",
      "name": "worker_control",
      "type": "component",
      "layer": "dual-agents",
      "color": "orange",
      "icon": "ðŸ”§",
      "description": "dispatch, abort, respond_escalation â€” sends phase-locked work to Worker. Dispatch includes phase, forbidden_actions, success_criteria.",
      "tags": [
        "meta-agent tool",
        "phase-locked"
      ],
      "sort_order": 55,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "dispatch, abort, respond_escalation â€” sends phase-locked work to Worker. Dispatch includes phase, forbidden_actions, success_criteria.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "proxy-admin",
      "name": "proxy_admin",
      "type": "component",
      "layer": "dual-agents",
      "color": "orange",
      "icon": "ðŸ“¡",
      "description": "register, deregister, list â€” mutates Worker's tool manifest at runtime.",
      "tags": [
        "meta-agent tool",
        "hot-swap"
      ],
      "sort_order": 56,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "register, deregister, list â€” mutates Worker's tool manifest at runtime.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "config-mutator",
      "name": "config_mutator",
      "type": "component",
      "layer": "dual-agents",
      "color": "orange",
      "icon": "âš™",
      "description": "update_prompt, update_model, update_agents â€” evolves Worker's config via typed builder DSL. Validated, versioned, rollback-safe. Not raw JSON editing.",
      "tags": [
        "meta-agent tool",
        "dsl",
        "rollback-safe"
      ],
      "sort_order": 57,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "update_prompt, update_model, update_agents â€” evolves Worker's config via typed builder DSL. Validated, versioned, rollback-safe.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-registry",
      "name": "tool_registry",
      "type": "component",
      "layer": "dual-agents",
      "color": "orange",
      "icon": "ðŸ”",
      "description": "search, inspect, install â€” discovers new MCP servers from a catalogue.",
      "tags": [
        "meta-agent tool",
        "discovery"
      ],
      "sort_order": 58,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "search, inspect, install â€” discovers new MCP servers from a catalogue.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "user-kg-read-meta",
      "name": "user_kg_read (Meta)",
      "type": "component",
      "layer": "dual-agents",
      "color": "gold",
      "icon": "ðŸ‘¤",
      "description": "query, traverse, search â€” Meta-Agent reads User KG for planning context.",
      "tags": [
        "meta-agent tool",
        "new"
      ],
      "sort_order": 59,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "query, traverse, search â€” Meta-Agent reads User KG for planning context.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "escalation-flow",
      "name": "Escalation Flow",
      "type": "layer",
      "layer": null,
      "color": "teal",
      "icon": "ðŸ™‹",
      "description": "Worker â†” Meta-Agent communication via shared state. Worker hits ambiguity â†’ calls request_clarification({question, context, options}) â†’ State Store records escalation â†’ Worker pauses â†’ Meta-Agent picks it up on next cycle â†’ reasons â†’ responds via worker_control.respond_escalation â†’ State Store updates â†’ Worker calls check_escalation_response â†’ receives guidance â†’ resumes. Timeout handling: if Meta-Agent does not respond within escalation_timeout_ms, Worker can (a) proceed with best guess, (b) abort, or (c) escalate to Human Gate. Policy set in Worker system prompt.",
      "tags": [
        "async",
        "no blocking rpc",
        "structured payload"
      ],
      "sort_order": 60,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Worker â†” Meta-Agent communication via shared state. Escalation sequence: Worker hits ambiguity â†’ calls request_clarification({question, context, options}) â†’ State Store records with status pending â†’ Worker pauses (returns control to idle loop) â†’ Meta-Agent's state_reader.get_escalations() picks it up â†’ Meta-Agent reasons â†’ calls worker_control.respond_escalation({task_id, guidance}) â†’ State Store updates to resolved â†’ Worker calls check_escalation_response() â†’ receives guidance â†’ resumes. Timeout: if no response within escalation_timeout_ms, Worker can: (a) proceed_best_guess, (b) abort, or (c) escalate_to_human. Design: Worker stays isolated (writes to State Store, not to Meta-Agent directly). Async by design (no blocking RPC). Structured payload: {question, context_snapshot, suggested_options[], urgency}. Crash safety: Checkpointer snapshots escalation state.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "user-kg-write-meta",
      "name": "user_kg_write (Meta)",
      "type": "component",
      "layer": "dual-agents",
      "color": "gold",
      "icon": "âœ",
      "description": "add_entity, add_edge, annotate â€” Meta-Agent writes inferred entities to User KG.",
      "tags": [
        "meta-agent tool",
        "new"
      ],
      "sort_order": 60,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "add_entity, add_edge, annotate â€” Meta-Agent writes inferred entities to User KG.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "code-graph-read-meta",
      "name": "code_graph_read (Meta)",
      "type": "component",
      "layer": "dual-agents",
      "color": "emerald",
      "icon": "ðŸ—º",
      "description": "expand, path, topo_order â€” Meta-Agent reads Code Graph for repo-aware decomposition.",
      "tags": [
        "meta-agent tool",
        "new"
      ],
      "sort_order": 61,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "expand, path, topo_order â€” Meta-Agent reads Code Graph for repo-aware decomposition.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "code-graph-write-meta",
      "name": "code_graph_write (Meta)",
      "type": "component",
      "layer": "dual-agents",
      "color": "emerald",
      "icon": "âœ",
      "description": "annotate_module, set_data_flow â€” Meta-Agent annotates Code Graph with module boundaries and data flow intentions.",
      "tags": [
        "meta-agent tool",
        "new"
      ],
      "sort_order": 62,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "annotate_module, set_data_flow â€” Meta-Agent annotates Code Graph with module boundaries and data flow intentions.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "request-clarification",
      "name": "request_clarification",
      "type": "component",
      "layer": "dual-agents",
      "color": "teal",
      "icon": "ðŸ™‹",
      "description": "Writes question + context snapshot to State Store. Sets task status to paused:awaiting_guidance. Worker halts current execution and waits.",
      "tags": [
        "worker tool",
        "escalation"
      ],
      "sort_order": 63,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Writes question + context snapshot to State Store. Sets task status to paused:awaiting_guidance. Worker halts current execution and waits.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "check-escalation-response",
      "name": "check_escalation_response",
      "type": "component",
      "layer": "dual-agents",
      "color": "teal",
      "icon": "ðŸ“¨",
      "description": "Polls State Store for Meta-Agent's response. Returns guidance or still_pending. Worker resumes when guidance arrives.",
      "tags": [
        "worker tool",
        "escalation"
      ],
      "sort_order": 64,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Polls State Store for Meta-Agent's response. Returns guidance or still_pending. Worker resumes when guidance arrives.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "user-kg-read-worker",
      "name": "user_kg_read (Worker)",
      "type": "component",
      "layer": "dual-agents",
      "color": "gold",
      "icon": "ðŸ‘¤",
      "description": "Read-only. No writes (injection safety). Worker reads preferences but cannot poison the knowledge graph even if fully compromised.",
      "tags": [
        "worker tool",
        "read-only",
        "new"
      ],
      "sort_order": 65,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Read-only. No writes (injection safety). Worker reads preferences but cannot poison the knowledge graph.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "code-graph-read-worker",
      "name": "code_graph_read (Worker)",
      "type": "component",
      "layer": "dual-agents",
      "color": "emerald",
      "icon": "ðŸ—º",
      "description": "Read-only. Checkpointer writes on Worker's behalf after file edits trigger AST re-parse.",
      "tags": [
        "worker tool",
        "read-only",
        "new"
      ],
      "sort_order": 66,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Read-only. Checkpointer writes on Worker's behalf after file edits trigger AST re-parse.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "shared-state",
      "name": "Shared State Store",
      "type": "layer",
      "layer": null,
      "color": "blue",
      "icon": "ðŸ’¾",
      "description": "The Bridge â€” SQLite WAL / Postgres. Goals, tasks, tool logs, checkpoints, escalations. Both instances communicate by sharing a database, not a connection.",
      "tags": [
        "sqlite wal",
        "postgres",
        "append-only"
      ],
      "sort_order": 70,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "SQLite WAL / Postgres. Goals, tasks, tool logs, checkpoints, escalations. The bridge between both instances â€” they share a database, not a connection. Context preservation: What IS saved â€” task ID + current step index, tool call log (name, args, result hash), plan summary (goal queue snapshot), goal queue pointer, timestamps, escalation state. What is NOT saved â€” LLM hidden state (non-serialisable), full conversation history (too large), in-flight reasoning (ephemeral by nature). Tool results are facts; LLM reasoning can be re-derived.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "state-store",
      "name": "State Store",
      "type": "app",
      "layer": "shared-state",
      "color": "blue",
      "icon": "ðŸ’¾",
      "description": "Append-only log that both instances read/write. The Meta-Agent writes goals and reads results. The Worker's Checkpointer writes tool call logs and progress. This is how the two OpenCode instances communicate without direct coupling â€” they share a database, not a connection. Also stores escalation records (question, context, response, status) and fast-path completion records so the Meta-Agent stays aware of tasks it didn't plan. Dashboard reads everything here.",
      "tags": [
        "checkpointer",
        "context rebuilder",
        "crash recovery"
      ],
      "sort_order": 71,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "SQLite WAL database with tables for goals, tasks, and tool_logs. Basic CRUD operations. Both agents read/write via simple SQL. No pruning, no optimization.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Append-only log that both instances read/write. The Meta-Agent writes goals and reads results. The Worker's Checkpointer writes tool call logs and progress. This is how the two OpenCode instances communicate without direct coupling â€” they share a database, not a connection. Also stores escalation records (question, context, response, status) and fast-path completion records so the Meta-Agent stays aware of tasks it didn't plan. Dashboard reads everything here.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Add checkpoints table, escalation records, fast-path completion records. Context rebuilder queries. Pruning policy (keep last N days). Indexes for common query patterns.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "SSE/WebSocket push for live dashboard updates. Postgres option for multi-machine deployments. Full audit trail with retention policies. Query optimization for dashboard views.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-shared-state.feature",
            "title": "Shared State Store (MVP)",
            "content": "Feature: Shared State Store (MVP)\n  The State Store is a SQLite WAL database that both agents\n  read and write. It stores goals, tasks, and tool logs.\n\n  Background:\n    Given the State Store SQLite database exists\n\n  Scenario: Create a goal\n    When a goal \"Build user authentication\" is inserted\n    Then the goal exists with status \"pending\"\n    And the goal has a created_at timestamp\n\n  Scenario: Create tasks for a goal\n    Given a goal exists with id \"goal-1\"\n    When tasks are inserted for goal \"goal-1\"\n    Then each task references the parent goal\n    And each task has status \"pending\" and an ordering index\n\n  Scenario: Log a tool call\n    Given a task exists with id \"task-1\"\n    When a tool call log is inserted with tool \"filesystem\", args hash, and result hash\n    Then the tool log exists with a timestamp\n    And the tool log references \"task-1\"\n\n  Scenario: Both agents can read/write concurrently\n    Given the Meta-Agent is writing a goal\n    And the Worker is writing a tool log\n    Then both writes succeed without conflict\n    And the WAL journal mode handles concurrent access\n\n  Scenario: Query task status by goal\n    Given a goal has 3 tasks with statuses \"complete\", \"in-progress\", \"pending\"\n    When querying tasks for that goal\n    Then all 3 tasks are returned with their statuses\n"
          }
        ]
      }
    },
    {
      "id": "checkpointer",
      "name": "Checkpointer",
      "type": "app",
      "layer": "shared-state",
      "color": "blue",
      "icon": "ðŸ“¸",
      "description": "Taps Worker's Proxy. Writes after every tool response: task ID, tool name, args, result hash, timestamp, plan summary. Also snapshots escalation state so crash recovery can restore a paused-and-waiting Worker correctly. If tool was a file edit, also triggers AST re-parse and Code Graph update. Strategy: tool results are facts; LLM reasoning can be re-derived. So we save the facts (tool call + result) and let the Context Rebuilder regenerate the reasoning frame on recovery. Runs async â€” doesn't block the agent. The proxy fires-and-forgets to the checkpointer; the Worker never waits for a checkpoint write to complete.",
      "tags": [
        "async",
        "fire-and-forget",
        "non-blocking",
        "code graph update"
      ],
      "sort_order": 72,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Intercept tool responses from Worker proxy. Write task_id, tool_name, args_hash, result_hash, timestamp to State Store. Fire-and-forget (async, non-blocking).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Taps Worker's Proxy. Writes after every tool response: task ID, tool name, args, result hash, timestamp, plan summary. Also snapshots escalation state so crash recovery can restore a paused-and-waiting Worker correctly. If tool was a file edit, triggers AST re-parse and Code Graph update. Strategy: tool results are facts; LLM reasoning can be re-derived. So we save the facts and let the Context Rebuilder regenerate the reasoning frame on recovery. Runs async â€” the proxy fires-and-forgets; the Worker never waits for a checkpoint write to complete.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Escalation state snapshots. File-edit detection triggering Code Graph AST re-parse. Plan summary snapshots for context rebuilder. Idempotency markers for crash recovery.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Configurable checkpoint granularity. Compressed checkpoint storage. Checkpoint pruning with retention policy. Metrics on checkpoint write latency.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-checkpointing.feature",
            "title": "Checkpointer (MVP)",
            "content": "Feature: Checkpointer (MVP)\n  The Checkpointer taps the Worker's MCP proxy and records\n  every tool call for crash recovery and progress tracking.\n\n  Background:\n    Given the Checkpointer is attached to the Worker's MCP proxy\n    And the State Store is accessible\n\n  Scenario: Record tool call after response\n    Given the Worker calls the \"filesystem\" tool with args \"read file.ts\"\n    When the tool returns a successful response\n    Then the Checkpointer writes a record to the State Store\n    And the record includes task_id, tool_name, args_hash, result_hash, and timestamp\n\n  Scenario: Non-blocking operation\n    Given the Worker is executing a task\n    When a tool call completes\n    Then the Checkpointer writes asynchronously\n    And the Worker does not wait for the checkpoint write\n\n  Scenario: Maintain ordering of tool calls\n    Given the Worker makes 3 sequential tool calls\n    When all 3 are checkpointed\n    Then the records are ordered by timestamp\n    And each has a sequential index within the task\n\n  Scenario: Handle write failure gracefully\n    Given the State Store is temporarily unavailable\n    When the Checkpointer attempts to write\n    Then it retries with backoff\n    And the Worker execution is not affected\n"
          }
        ]
      }
    },
    {
      "id": "context-rebuilder",
      "name": "Context Rebuilder",
      "type": "app",
      "layer": "shared-state",
      "color": "blue",
      "icon": "ðŸ“",
      "description": "On crash recovery of either instance: generates resume prompt from compressed checkpoint + relevant graph context. For Worker: \"you were doing X, completed Y, next step Z\". For Meta-Agent: \"current goal is X, worker status is Y, pending goals are Z\". If Worker was in paused:awaiting_guidance state, resume prompt includes the escalation question and any response received while it was down. Lossy by design. You can't clone LLM hidden state â€” it's non-serialisable. This is like a save game, not a VM snapshot. The rebuilt context is \"good enough\" â€” the agent continues without knowing it crashed, picking up from the last checkpoint with a compressed summary of what came before.",
      "tags": [
        "lossy by design",
        "save game",
        "resume prompt"
      ],
      "sort_order": 73,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Read last checkpoint from State Store. Generate basic resume prompt: \"you were doing X, completed Y, next step Z\". Inject as system prompt on respawn.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "On crash recovery of either instance: generates resume prompt from compressed checkpoint + relevant graph context. For Worker: \"you were doing X, completed Y, next step Z\". For Meta-Agent: \"current goal is X, worker status is Y, pending goals are Z\". If Worker was in paused:awaiting_guidance state, resume prompt includes the escalation question and any response received while it was down. Lossy by design. You can't clone LLM hidden state â€” it's non-serialisable. This is like a save game, not a VM snapshot.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Include relevant User KG context in resume prompt. Include Code Graph context for coding tasks. Handle paused:awaiting_guidance state.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Compressed multi-checkpoint summaries. Relevance-ranked context selection. Token budget management for resume prompts.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-context-rebuild.feature",
            "title": "Context Rebuilder (MVP)",
            "content": "Feature: Context Rebuilder (MVP)\n  On crash recovery, the Context Rebuilder generates a resume\n  prompt from the last checkpoint so the agent can continue.\n\n  Background:\n    Given the State Store contains checkpoints for a crashed agent\n\n  Scenario: Generate resume prompt for Worker\n    Given the Worker crashed mid-task on task \"Create login endpoint\"\n    And the last checkpoint shows 3 completed tool calls\n    When the Context Rebuilder generates a resume prompt\n    Then the prompt includes \"you were doing: Create login endpoint\"\n    And it lists the 3 completed tool calls with their results\n    And it states the next expected action\n\n  Scenario: Generate resume prompt for Meta-Agent\n    Given the Meta-Agent crashed while processing goal \"Build auth\"\n    And the goal has 5 tasks, 2 completed and 1 in-progress\n    When the Context Rebuilder generates a resume prompt\n    Then the prompt includes the current goal state\n    And it lists completed and pending tasks\n    And it states the current Worker status\n\n  Scenario: Handle empty checkpoint\n    Given no checkpoints exist for the crashed agent\n    When the Context Rebuilder generates a resume prompt\n    Then it produces a minimal prompt with no prior context\n    And the agent starts fresh\n"
          }
        ]
      }
    },
    {
      "id": "mcp-proxies",
      "name": "MCP Proxies",
      "type": "layer",
      "layer": null,
      "color": "orange",
      "icon": "â‡„",
      "description": "Tool proxy layer for both agent instances. Meta-Agent gets static manifest (10 internal tools, no sanitiser). Worker gets dynamic manifest (hot-swappable, sanitiser required, circuit breaker).",
      "tags": [
        "static manifest",
        "dynamic manifest",
        "hot-swappable"
      ],
      "sort_order": 80,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Tool proxy layer. Meta-Agent gets static manifest (10 internal tools, no sanitiser, low risk). Worker gets dynamic manifest (hot-swappable, sanitiser required, circuit breaker, checkpoint tap).",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "mcp-proxy-meta",
      "name": "MCP Proxy â€” Meta-Agent",
      "type": "app",
      "layer": "mcp-proxies",
      "color": "orange",
      "icon": "â‡„",
      "description": "Hosts 10 planning tools (6 planning + 4 graph). These are your custom MCP servers â€” small, stable, purpose-built. No external API calls, no injection risk.",
      "tags": [
        "static",
        "no sanitiser",
        "low risk"
      ],
      "sort_order": 81,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Static MCP server hosting goal_queue, state_reader, worker_control tools. Simple stdio transport. No hot-swap needed.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Hosts 10 planning tools (6 planning + 4 graph). These are your custom MCP servers â€” small, stable, purpose-built. No external API calls, no injection risk.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "mcp-proxy-worker",
      "name": "MCP Proxy â€” Worker",
      "type": "app",
      "layer": "mcp-proxies",
      "color": "cyan",
      "icon": "â‡„",
      "description": "Hosts all external-facing tools + escalation + graph reads. Dynamic manifest â€” the Meta-Agent's proxy_admin tool adds/removes servers here at runtime. All responses pass through the Sanitiser. Health & circuit breaker: heartbeats downstream servers; dead endpoints auto-removed from manifest, Meta-Agent notified via State Store so it can find replacements.",
      "tags": [
        "sanitiser required",
        "hot-swappable",
        "checkpoint tap",
        "circuit breaker"
      ],
      "sort_order": 82,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "MCP proxy with configurable tool list. Route tool calls to downstream servers. Pass responses through sanitiser. Basic health check on downstream servers.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Hosts all external-facing tools + escalation + graph reads. Dynamic manifest â€” Meta-Agent's proxy_admin adds/removes servers at runtime. All responses pass through Sanitiser. Health & circuit breaker: heartbeats downstream servers; dead endpoints auto-removed from manifest, Meta-Agent notified via State Store so it can find replacements.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "security-sandbox",
      "name": "Security Sandbox",
      "type": "layer",
      "layer": null,
      "color": "red",
      "icon": "ðŸ›¡",
      "description": "3-stage sanitiser pipeline (Worker proxy only). Regex heuristics â†’ structural strip (role tags, cap length) â†’ optional LLM classifier. Fail-closed. Isolated subprocess. Scans inbound responses (injection defence) AND outbound tool args (prevents data exfiltration via tricked agent).",
      "tags": [
        "fail-closed",
        "isolated subprocess",
        "bidirectional"
      ],
      "sort_order": 90,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "3-stage sanitiser: regex heuristics â†’ structural strip â†’ optional LLM classifier. Fail-closed. Isolated subprocess. Scans inbound + outbound. Worker proxy only. Security model: Meta-Agent has no sanitiser (internal tools only). Worker has full 3-stage sanitiser. Cross-instance isolation: Worker cannot reach Meta-Agent's tools even if fully compromised. Escalation tools are safe: they write structured data to State Store, not free-text to Meta-Agent's prompt. Dashboard is read-only â€” cannot be used as attack vector. Graph writes: Worker cannot write to either graph â€” cannot poison knowledge even if fully compromised. In full-auto mode: sanitiser is the only defence against prompt injection. Write fence still applies for destructive ops. Injection feedback loop lets Meta-Agent auto-disable compromised tools.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "sanitiser",
      "name": "3-Stage Sanitiser",
      "type": "app",
      "layer": "security-sandbox",
      "color": "red",
      "icon": "ðŸ›¡",
      "description": "Sits between Worker's Proxy and downstream servers. Stage 1: Heuristic regex for common injection patterns. Stage 2: Structural strip (remove role tags, cap response length). Stage 3: Optional LLM classifier for sophisticated detection. The Meta-Agent's proxy does NOT need a sanitiser â€” its tools are all internal, no external input. Isolated subprocess. Fail-closed. Scans inbound responses (injection defence) and outbound tool args (prevents data exfiltration via a tricked agent â€” e.g. an injected prompt that encodes secrets into a search query). Injection events visible in dashboard Security Events panel.",
      "tags": [
        "3-stage",
        "fail-closed",
        "isolated subprocess",
        "bidirectional"
      ],
      "sort_order": 91,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "mvp": {
          "content": "Regex-based heuristic scanner for common injection patterns. Structural strip (remove role tags, cap response length). Pass/block verdict on each tool response. Logging to State Store.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "overview": {
          "content": "Sits between Worker's Proxy and downstream servers. Stage 1: Heuristic regex for common injection patterns. Stage 2: Structural strip (remove role tags, cap response length). Stage 3: Optional LLM classifier for sophisticated detection. The Meta-Agent's proxy does NOT need a sanitiser â€” its tools are all internal, no external input. Isolated subprocess. Fail-closed. Scans inbound responses (injection defence) and outbound tool args (prevents data exfiltration â€” e.g. an injected prompt encoding secrets into a search query). Injection events visible in dashboard Security Events panel.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v1": {
          "content": "Outbound scanning (prevent data exfiltration via tool args). Configurable rule sets per tool. Injection frequency tracking. Auto-disable tools exceeding threshold. Dashboard integration.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        },
        "v2": {
          "content": "Optional LLM classifier stage. Adaptive rules based on observed attack patterns. Per-tool confidence scoring. Full audit trail with payload samples.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {
        "mvp": [
          {
            "filename": "mvp-sanitiser.feature",
            "title": "3-Stage Sanitiser (MVP)",
            "content": "Feature: 3-Stage Sanitiser (MVP)\n  The Sanitiser sits between the Worker's MCP proxy and downstream\n  tool servers. It scans tool responses for injection attempts.\n\n  Background:\n    Given the Sanitiser is running as an isolated subprocess\n\n  Scenario: Pass clean tool response\n    Given a tool response contains normal text content\n    When the Sanitiser processes the response\n    Then the verdict is \"pass\"\n    And the response is forwarded to the Worker unchanged\n\n  Scenario: Block response with injection pattern\n    Given a tool response contains \"ignore previous instructions\"\n    When the Sanitiser processes the response\n    Then the verdict is \"block\"\n    And the response is not forwarded to the Worker\n    And the injection event is logged to the State Store\n\n  Scenario: Strip role tags from response\n    Given a tool response contains \"<system>\" tags\n    When the Sanitiser applies structural stripping\n    Then the role tags are removed from the response\n    And the cleaned response is forwarded\n\n  Scenario: Cap response length\n    Given a tool response exceeds the maximum allowed length\n    When the Sanitiser processes the response\n    Then the response is truncated to the maximum length\n    And the truncation is noted in the log\n\n  Scenario: Fail closed on processing error\n    Given the Sanitiser encounters an internal error during processing\n    When processing a tool response\n    Then the response is blocked (not forwarded)\n    And the error is logged\n"
          }
        ]
      }
    },
    {
      "id": "alert-pipeline",
      "name": "Alert Pipeline",
      "type": "component",
      "layer": "security-sandbox",
      "color": "red",
      "icon": "ðŸ“‹",
      "description": "Blocked injections logged to State Store. Meta-Agent can auto-disable compromised tools via injection feedback loop â€” reads sanitiser alerts and learns to avoid them. Dashboard shows real-time security events feed.",
      "tags": [
        "auto-disable",
        "injection feedback loop",
        "real-time"
      ],
      "sort_order": 92,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Blocked injections logged to State Store. Meta-Agent can auto-disable compromised tools via injection feedback loop â€” reads sanitiser alerts and learns to avoid them. Dashboard shows real-time security events.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "downstream-tools",
      "name": "Downstream MCP Tool Servers",
      "type": "layer",
      "layer": null,
      "color": "amber",
      "icon": "ðŸ”§",
      "description": "External tool servers â€” search, email, database, filesystem, code execution, custom. Hot-swappable â€” Meta-Agent adds/removes at runtime via proxy_admin.",
      "tags": [
        "external",
        "hot-swappable",
        "runtime managed"
      ],
      "sort_order": 100,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "External MCP tool servers. Search, email, database, filesystem, code execution, custom. Hot-swappable â€” Meta-Agent adds/removes at runtime.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-search",
      "name": "Search",
      "type": "external",
      "layer": "downstream-tools",
      "color": "amber",
      "icon": "ðŸ”",
      "description": "External search MCP server.",
      "tags": [
        "ext"
      ],
      "sort_order": 101,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "External search MCP server.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-email",
      "name": "Email",
      "type": "external",
      "layer": "downstream-tools",
      "color": "amber",
      "icon": "âœ‰",
      "description": "External email MCP server.",
      "tags": [
        "ext"
      ],
      "sort_order": 102,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "External email MCP server.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-database",
      "name": "Database",
      "type": "external",
      "layer": "downstream-tools",
      "color": "amber",
      "icon": "ðŸ—„",
      "description": "External database MCP server.",
      "tags": [
        "write"
      ],
      "sort_order": 103,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "External database MCP server.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-filesystem",
      "name": "Filesystem",
      "type": "external",
      "layer": "downstream-tools",
      "color": "amber",
      "icon": "ðŸ“‚",
      "description": "External filesystem MCP server.",
      "tags": [
        "write"
      ],
      "sort_order": 104,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "External filesystem MCP server.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-code-exec",
      "name": "Code Exec",
      "type": "external",
      "layer": "downstream-tools",
      "color": "amber",
      "icon": "ðŸ’»",
      "description": "External code execution MCP server.",
      "tags": [
        "write"
      ],
      "sort_order": 105,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "External code execution MCP server.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "tool-custom",
      "name": "Custom",
      "type": "external",
      "layer": "downstream-tools",
      "color": "amber",
      "icon": "ðŸ§©",
      "description": "Custom MCP servers â€” hot-swappable, added/removed by Meta-Agent at runtime.",
      "tags": [
        "dynamic"
      ],
      "sort_order": 106,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Custom MCP servers â€” hot-swappable, added/removed by Meta-Agent at runtime.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "bdd-tdd-pipeline",
      "name": "BDD/TDD Phase Pipeline",
      "type": "layer",
      "layer": null,
      "color": "teal",
      "icon": "ðŸ”„",
      "description": "Strict 8-phase pipeline enforced by Meta-Agent, executed by Worker. Each phase is a separate dispatch. Worker sees only the current phase â€” never what comes next. Every phase ends with a git commit creating a clean rollback point. Meta-Agent verifies phase gate before advancing. If gate fails, re-dispatch same phase. Phases â‘¦ and â‘§ are audit-only â€” violations feed back as targeted fix dispatches, then re-audit.",
      "tags": [
        "phase isolation",
        "git commits",
        "gate verification"
      ],
      "sort_order": 110,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Strict 8-phase pipeline enforced by Meta-Agent: Feature â†’ Steps â†’ Units â†’ Red â†’ Green â†’ Refactor â†’ Arch Review â†’ Sec Review. Every phase ends with a git commit. LLM agents are bad at process discipline. Given \"build a login feature\", they'll jump straight to implementation, skip tests, or refactor before proving anything works. Phase isolation prevents this â€” the Worker can't skip ahead because it doesn't know what \"ahead\" is. The forbidden_actions field explicitly blocks forward-leaking behaviour. This turns the Meta-Agent into a process enforcer, not just a task decomposer. Review phases â‘¦ â‘§ are audit-only â€” dispatched as read-only inspections. Worker reports violations but is forbidden from modifying code. Meta-Agent reads the report, dispatches separate fix phases with specific violations as constraints. Prevents the \"fix one thing, break another\" cascade. Phases are extensible: add performance review, accessibility audit, API design review â€” same pattern.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-feature",
      "name": "â‘  Feature",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "gold",
      "icon": "ðŸ“",
      "description": "Write the .feature file. Describe the behaviour in Gherkin. DO NOT write any tests or code. Gate: .feature file exists. Git commit after phase.",
      "tags": [
        "gherkin",
        "gate: .feature exists"
      ],
      "sort_order": 111,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Write the .feature file. Describe the behaviour in Gherkin. DO NOT write any tests or code. Gate: .feature file exists. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-steps",
      "name": "â‘¡ Step Tests",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "cyan",
      "icon": "ðŸ§ª",
      "description": "Write failing step definitions for this feature file. DO NOT implement any production code. Gate: step files exist. Git commit after phase.",
      "tags": [
        "failing tests",
        "gate: step files exist"
      ],
      "sort_order": 112,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Write failing step definitions for this feature file. DO NOT implement any production code. Gate: step files exist. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-units",
      "name": "â‘¢ Unit Tests",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "purple",
      "icon": "ðŸ§ª",
      "description": "Write failing unit tests for the components you'll need. DO NOT implement any production code. Gate: test files exist. Git commit after phase.",
      "tags": [
        "failing tests",
        "gate: test files exist"
      ],
      "sort_order": 113,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Write failing unit tests for the components you'll need. DO NOT implement any production code. Gate: test files exist. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-red",
      "name": "â‘£ Red",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "red",
      "icon": "ðŸ”´",
      "description": "Run all tests. Confirm they fail. Report which tests fail and why. DO NOT fix anything. Gate: tests fail. Git commit after phase.",
      "tags": [
        "confirm failure",
        "gate: tests fail"
      ],
      "sort_order": 114,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Run all tests. Confirm they fail. Report which tests fail and why. DO NOT fix anything. Gate: tests fail. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-green",
      "name": "â‘¤ Green",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "green",
      "icon": "ðŸŸ¢",
      "description": "Write the minimum production code to make all tests pass. DO NOT refactor or optimise. Gate: tests pass. Git commit after phase.",
      "tags": [
        "minimum code",
        "gate: tests pass"
      ],
      "sort_order": 115,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Write the minimum production code to make all tests pass. DO NOT refactor or optimise. Gate: tests pass. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-refactor",
      "name": "â‘¥ Refactor",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "sky",
      "icon": "ðŸ”§",
      "description": "Refactor for clarity, DRY, naming. All tests must still pass. DO NOT add new functionality. Gate: tests still pass. Git commit after phase.",
      "tags": [
        "clarity",
        "dry",
        "gate: tests still pass"
      ],
      "sort_order": 116,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "Refactor for clarity, DRY, naming. All tests must still pass. DO NOT add new functionality. Gate: tests still pass. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-arch-review",
      "name": "â‘¦ Arch Review",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "orange",
      "icon": "ðŸ›",
      "description": "LLM-driven audit agent (not a linter). Audit against Clean Architecture standards. Report violations: dependency direction (inner layers importing outer), layer boundary leaks (business logic in controllers, HTTP in domain), abstraction leaks (SQL in repository interface), use case isolation. Traverses Code Graph DATA_FLOW and IMPORTS edges to verify dependency direction structurally. DO NOT fix â€” only report. Report format: {violations: [{file, line, rule, severity, explanation}], passed: bool}. If violations found, Meta-Agent dispatches targeted fix phases then re-runs this review. Gate: 0 violations. Git commit after phase.",
      "tags": [
        "audit-only",
        "clean architecture",
        "gate: 0 violations"
      ],
      "sort_order": 117,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "LLM-driven audit agent. Audit against Clean Architecture: dependency direction (inner layers importing outer = violation), layer boundaries (business logic in controllers), abstraction leaks (SQL in repository interface), use case isolation. Traverses Code Graph DATA_FLOW and IMPORTS edges structurally. DO NOT fix â€” only report. Report: {violations: [{file, line, rule, severity, explanation}], passed: bool}. Gate: 0 violations. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    },
    {
      "id": "phase-sec-review",
      "name": "â‘§ Sec Review",
      "type": "phase",
      "layer": "bdd-tdd-pipeline",
      "color": "rose",
      "icon": "ðŸ”’",
      "description": "LLM-driven security audit agent. Check: injection vectors (SQL, XSS, command injection, path traversal), auth/authz gaps (endpoints without auth middleware, missing permission checks, privilege escalation), secrets exposure (hardcoded keys, tokens in logs, secrets in error messages, .env leaks), unsafe dependencies (known CVEs, deprecated crypto, insecure defaults). Checks compliance requirements from User KG (e.g. \"client requires SOC2\" â†’ enforce specific controls). DO NOT fix â€” only report. Same report format as Arch Review. Gate: 0 findings. Git commit after phase.",
      "tags": [
        "audit-only",
        "security",
        "gate: 0 findings",
        "user kg compliance"
      ],
      "sort_order": 118,
      "current_version": null,
      "display_state": "Concept",
      "versions": {
        "overview": {
          "content": "LLM-driven security audit. Injection vectors (SQL, XSS, command injection, path traversal), auth/authz gaps (missing middleware, privilege escalation), secrets exposure (hardcoded keys, tokens in logs, .env leaks), unsafe dependencies (CVEs, deprecated crypto). Checks User KG compliance requirements (e.g. SOC2). DO NOT fix â€” only report. Same report format. Gate: 0 findings. Git commit after phase.",
          "progress": 0,
          "status": "planned",
          "updated_at": "2026-02-11 14:27:05"
        }
      },
      "features": {}
    }
  ],
  "edges": [
    {
      "id": 56,
      "source_id": "supervisor",
      "target_id": "meta-agent",
      "type": "CONTROLS",
      "label": "spawns + manages",
      "metadata": null
    },
    {
      "id": 57,
      "source_id": "supervisor",
      "target_id": "worker",
      "type": "CONTROLS",
      "label": "spawns + manages",
      "metadata": null
    },
    {
      "id": 58,
      "source_id": "supervisor",
      "target_id": "live-dashboard",
      "type": "CONTROLS",
      "label": "spawns",
      "metadata": null
    },
    {
      "id": 59,
      "source_id": "meta-agent",
      "target_id": "worker",
      "type": "DISPATCHES_TO",
      "label": "phase-locked tasks",
      "metadata": null
    },
    {
      "id": 60,
      "source_id": "worker",
      "target_id": "meta-agent",
      "type": "ESCALATES_TO",
      "label": "via state store",
      "metadata": null
    },
    {
      "id": 61,
      "source_id": "live-dashboard",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "goals, tasks, logs",
      "metadata": null
    },
    {
      "id": 62,
      "source_id": "live-dashboard",
      "target_id": "supervisor",
      "type": "READS_FROM",
      "label": "health API",
      "metadata": null
    },
    {
      "id": 63,
      "source_id": "live-process-view",
      "target_id": "supervisor",
      "type": "READS_FROM",
      "label": "process status",
      "metadata": null
    },
    {
      "id": 64,
      "source_id": "goal-task-feed",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "goal queue",
      "metadata": null
    },
    {
      "id": 65,
      "source_id": "tool-call-timeline",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "tool logs",
      "metadata": null
    },
    {
      "id": 66,
      "source_id": "security-events",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "injection events",
      "metadata": null
    },
    {
      "id": 67,
      "source_id": "escalation-queue",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "escalations",
      "metadata": null
    },
    {
      "id": 68,
      "source_id": "entity-explorer",
      "target_id": "user-knowledge-graph",
      "type": "READS_FROM",
      "label": "entities",
      "metadata": null
    },
    {
      "id": 69,
      "source_id": "repo-map",
      "target_id": "rpg-code-graph",
      "type": "READS_FROM",
      "label": "code structure",
      "metadata": null
    },
    {
      "id": 70,
      "source_id": "meta-agent",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "worker progress",
      "metadata": null
    },
    {
      "id": 71,
      "source_id": "meta-agent",
      "target_id": "state-store",
      "type": "WRITES_TO",
      "label": "goals, tasks",
      "metadata": null
    },
    {
      "id": 72,
      "source_id": "meta-agent",
      "target_id": "user-knowledge-graph",
      "type": "READS_FROM",
      "label": "preferences",
      "metadata": null
    },
    {
      "id": 73,
      "source_id": "meta-agent",
      "target_id": "user-knowledge-graph",
      "type": "WRITES_TO",
      "label": "inferred entities",
      "metadata": null
    },
    {
      "id": 74,
      "source_id": "meta-agent",
      "target_id": "rpg-code-graph",
      "type": "READS_FROM",
      "label": "repo structure",
      "metadata": null
    },
    {
      "id": 75,
      "source_id": "meta-agent",
      "target_id": "rpg-code-graph",
      "type": "WRITES_TO",
      "label": "annotations",
      "metadata": null
    },
    {
      "id": 76,
      "source_id": "worker",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "task dispatch",
      "metadata": null
    },
    {
      "id": 77,
      "source_id": "worker",
      "target_id": "user-knowledge-graph",
      "type": "READS_FROM",
      "label": "preferences (read-only)",
      "metadata": null
    },
    {
      "id": 78,
      "source_id": "worker",
      "target_id": "rpg-code-graph",
      "type": "READS_FROM",
      "label": "code structure (read-only)",
      "metadata": null
    },
    {
      "id": 79,
      "source_id": "checkpointer",
      "target_id": "state-store",
      "type": "WRITES_TO",
      "label": "tool logs, checkpoints",
      "metadata": null
    },
    {
      "id": 80,
      "source_id": "checkpointer",
      "target_id": "rpg-code-graph",
      "type": "WRITES_TO",
      "label": "incremental AST updates",
      "metadata": null
    },
    {
      "id": 81,
      "source_id": "context-rebuilder",
      "target_id": "state-store",
      "type": "READS_FROM",
      "label": "checkpoints",
      "metadata": null
    },
    {
      "id": 82,
      "source_id": "context-rebuilder",
      "target_id": "user-knowledge-graph",
      "type": "READS_FROM",
      "label": "relevant context",
      "metadata": null
    },
    {
      "id": 83,
      "source_id": "context-rebuilder",
      "target_id": "rpg-code-graph",
      "type": "READS_FROM",
      "label": "relevant context",
      "metadata": null
    },
    {
      "id": 84,
      "source_id": "meta-agent",
      "target_id": "mcp-proxy-meta",
      "type": "DEPENDS_ON",
      "label": "tool access",
      "metadata": null
    },
    {
      "id": 85,
      "source_id": "worker",
      "target_id": "mcp-proxy-worker",
      "type": "DEPENDS_ON",
      "label": "tool access",
      "metadata": null
    },
    {
      "id": 86,
      "source_id": "sanitiser",
      "target_id": "mcp-proxy-worker",
      "type": "SANITISES",
      "label": "all external I/O",
      "metadata": null
    },
    {
      "id": 87,
      "source_id": "mcp-proxy-worker",
      "target_id": "tool-search",
      "type": "PROXIES",
      "label": null,
      "metadata": null
    },
    {
      "id": 88,
      "source_id": "mcp-proxy-worker",
      "target_id": "tool-email",
      "type": "PROXIES",
      "label": null,
      "metadata": null
    },
    {
      "id": 89,
      "source_id": "mcp-proxy-worker",
      "target_id": "tool-database",
      "type": "PROXIES",
      "label": null,
      "metadata": null
    },
    {
      "id": 90,
      "source_id": "mcp-proxy-worker",
      "target_id": "tool-filesystem",
      "type": "PROXIES",
      "label": null,
      "metadata": null
    },
    {
      "id": 91,
      "source_id": "mcp-proxy-worker",
      "target_id": "tool-code-exec",
      "type": "PROXIES",
      "label": null,
      "metadata": null
    },
    {
      "id": 92,
      "source_id": "mcp-proxy-worker",
      "target_id": "tool-custom",
      "type": "PROXIES",
      "label": null,
      "metadata": null
    },
    {
      "id": 93,
      "source_id": "alert-pipeline",
      "target_id": "state-store",
      "type": "WRITES_TO",
      "label": "security events",
      "metadata": null
    },
    {
      "id": 94,
      "source_id": "alert-pipeline",
      "target_id": "meta-agent",
      "type": "DISPATCHES_TO",
      "label": "auto-disable alerts",
      "metadata": null
    },
    {
      "id": 95,
      "source_id": "human-gate",
      "target_id": "fast-path",
      "type": "GATES",
      "label": "approve/reject",
      "metadata": null
    },
    {
      "id": 96,
      "source_id": "human-gate",
      "target_id": "full-path",
      "type": "GATES",
      "label": "approve/reject",
      "metadata": null
    },
    {
      "id": 97,
      "source_id": "human-gate",
      "target_id": "gated-path",
      "type": "GATES",
      "label": "blocks until approved",
      "metadata": null
    },
    {
      "id": 98,
      "source_id": "fast-path",
      "target_id": "worker",
      "type": "DISPATCHES_TO",
      "label": "direct",
      "metadata": null
    },
    {
      "id": 99,
      "source_id": "full-path",
      "target_id": "meta-agent",
      "type": "DISPATCHES_TO",
      "label": "for decomposition",
      "metadata": null
    },
    {
      "id": 100,
      "source_id": "supervisor",
      "target_id": "state-store",
      "type": "DEPENDS_ON",
      "label": "reads checkpoints",
      "metadata": null
    },
    {
      "id": 101,
      "source_id": "checkpointer",
      "target_id": "state-store",
      "type": "DEPENDS_ON",
      "label": "writes checkpoints",
      "metadata": null
    },
    {
      "id": 102,
      "source_id": "context-rebuilder",
      "target_id": "state-store",
      "type": "DEPENDS_ON",
      "label": "reads checkpoints",
      "metadata": null
    },
    {
      "id": 103,
      "source_id": "meta-agent",
      "target_id": "supervisor",
      "type": "DEPENDS_ON",
      "label": "spawned by supervisor",
      "metadata": null
    },
    {
      "id": 104,
      "source_id": "worker",
      "target_id": "supervisor",
      "type": "DEPENDS_ON",
      "label": "spawned by supervisor",
      "metadata": null
    },
    {
      "id": 105,
      "source_id": "mcp-proxy-meta",
      "target_id": "meta-agent",
      "type": "DEPENDS_ON",
      "label": "serves meta-agent tools",
      "metadata": null
    },
    {
      "id": 106,
      "source_id": "mcp-proxy-worker",
      "target_id": "worker",
      "type": "DEPENDS_ON",
      "label": "serves worker tools",
      "metadata": null
    },
    {
      "id": 107,
      "source_id": "sanitiser",
      "target_id": "mcp-proxy-worker",
      "type": "DEPENDS_ON",
      "label": "wraps worker proxy",
      "metadata": null
    },
    {
      "id": 108,
      "source_id": "live-dashboard",
      "target_id": "state-store",
      "type": "DEPENDS_ON",
      "label": "reads all state",
      "metadata": null
    },
    {
      "id": 109,
      "source_id": "live-dashboard",
      "target_id": "supervisor",
      "type": "DEPENDS_ON",
      "label": "reads health API",
      "metadata": null
    },
    {
      "id": 110,
      "source_id": "human-gate",
      "target_id": "state-store",
      "type": "DEPENDS_ON",
      "label": "reads/writes approvals",
      "metadata": null
    },
    {
      "id": 111,
      "source_id": "context-rebuilder",
      "target_id": "user-knowledge-graph",
      "type": "DEPENDS_ON",
      "label": "reads context",
      "metadata": null
    },
    {
      "id": 112,
      "source_id": "context-rebuilder",
      "target_id": "rpg-code-graph",
      "type": "DEPENDS_ON",
      "label": "reads context",
      "metadata": null
    },
    {
      "id": 113,
      "source_id": "phase-feature",
      "target_id": "phase-steps",
      "type": "SEQUENCE",
      "label": "commit â†’ gate",
      "metadata": null
    },
    {
      "id": 114,
      "source_id": "phase-steps",
      "target_id": "phase-units",
      "type": "SEQUENCE",
      "label": "commit â†’ gate",
      "metadata": null
    },
    {
      "id": 115,
      "source_id": "phase-units",
      "target_id": "phase-red",
      "type": "SEQUENCE",
      "label": "commit â†’ gate",
      "metadata": null
    },
    {
      "id": 116,
      "source_id": "phase-red",
      "target_id": "phase-green",
      "type": "SEQUENCE",
      "label": "commit â†’ gate",
      "metadata": null
    },
    {
      "id": 117,
      "source_id": "phase-green",
      "target_id": "phase-refactor",
      "type": "SEQUENCE",
      "label": "commit â†’ gate",
      "metadata": null
    },
    {
      "id": 118,
      "source_id": "phase-refactor",
      "target_id": "phase-arch-review",
      "type": "SEQUENCE",
      "label": "commit â†’ gate",
      "metadata": null
    },
    {
      "id": 119,
      "source_id": "phase-arch-review",
      "target_id": "phase-sec-review",
      "type": "SEQUENCE",
      "label": "commit â†’ gate",
      "metadata": null
    }
  ],
  "progression_tree": {
    "nodes": [
      {
        "id": "roadmap",
        "name": "Roadmap",
        "type": "app",
        "layer": null,
        "color": "cyan",
        "icon": "ðŸ—º",
        "description": "Living documentation for the Open Autonomous Runtime. Self-tracking component with progression tree, versioned specs, and Gherkin feature files. Built with Clean Architecture, TypeScript, SQLite, and Cytoscape.js.",
        "tags": [
          "self-tracking",
          "clean architecture",
          "bdd",
          "progression tree"
        ],
        "sort_order": 1,
        "current_version": "1.0.0",
        "display_state": "v1",
        "versions": {
          "mvp": {
            "content": "Clean Architecture TypeScript codebase. SQLite graph database with 4 tables (nodes, edges, node_versions, features). BDD test suite (Cucumber + Vitest). CI merge gate. Static web view with architecture diagram. Progression tree home page with Cytoscape.js. Self-tracking as a component.",
            "progress": 100,
            "status": "complete",
            "updated_at": "2026-02-11 17:16:36"
          },
          "overview": {
            "content": "Living documentation for the Open Autonomous Runtime. Self-tracking component with progression tree UI, versioned specs, Gherkin feature files, and BDD/TDD pipeline. Built with Clean Architecture (TypeScript), SQLite graph database, and Cytoscape.js for the progression tree visualization.",
            "progress": 0,
            "status": "in-progress",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Secure API with key-based auth (scoped permissions, salted hashes, rate limiting, security headers, request logging, structured errors). Neo4j graph storage replacing SQLite (native traversals, multi-hop queries, shortest path, cycle detection, data migration, transaction safety). Enhanced component management (PATCH updates, filtering, search, full edge CRUD, version lifecycle, bulk operations up to 100 items, layer management). Feature-driven progress tracking (step-level maths from Gherkin Given/When/Then counts, passing_steps/total_steps percentage, test result recording, combined semver+step weighting, progress history, dashboard summary APIs, CSV export, automatic recalculation on feature/test changes). Version-scoped feature publishing (explicit version in URL path, Gherkin validation, batch upload, graph traversal endpoints for dependency trees, topological sort, shortest path, neighbourhood queries, next-implementable components, feature search).",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 23:36:59"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-architecture-export.feature",
              "title": "Architecture Export",
              "content": "Feature: Architecture Export\n  As a web view consumer\n  I want to export the architecture graph to a JSON file\n  So that the static web page can render it without a database connection\n\n  Background:\n    Given a database with architecture data\n\n  Scenario: Export produces a JSON file at the specified path\n    Given an output path \"web/data.json\"\n    When I export the architecture\n    Then the write function is called with path \"web/data.json\"\n    And the written data contains a \"generated_at\" field\n\n  Scenario: Export returns statistics about the exported data\n    Given the database contains 3 nodes, 2 edges, 4 versions, and 1 feature\n    When I export the architecture\n    Then the export result includes stats with total counts\n\n  Scenario: Exported data contains the full architecture structure\n    Given the database contains nodes with versions and features\n    When I export the architecture\n    Then the written data includes layers with children\n    And the written data includes enriched nodes\n    And the written data includes relationship edges\n"
            },
            {
              "filename": "mvp-architecture-graph-assembly.feature",
              "title": "Architecture Graph Assembly",
              "content": "Feature: Architecture Graph Assembly\n  As a roadmap consumer\n  I want the system to assemble a complete architecture graph\n  So that I can see all components, their relationships, versions, and features in one view\n\n  Background:\n    Given a database with architecture data\n\n  Scenario: Assemble all nodes into the graph\n    Given the database contains nodes of types \"layer\", \"component\", and \"store\"\n    When I assemble the architecture graph\n    Then every node appears in the result\n\n  Scenario: Group components under their parent layer\n    Given a layer node \"supervisor-layer\"\n    And a component node \"supervisor\" belonging to layer \"supervisor-layer\"\n    When I assemble the architecture graph\n    Then the layer \"supervisor-layer\" contains child \"supervisor\"\n\n  Scenario: Enrich nodes with their version content\n    Given a component node \"meta-agent\"\n    And versions \"overview\", \"mvp\", \"v1\" exist for node \"meta-agent\"\n    When I assemble the architecture graph\n    Then node \"meta-agent\" has version keys \"overview\", \"mvp\", \"v1\"\n    And each version includes content, progress, and status\n\n  Scenario: Enrich nodes with their feature specs\n    Given a component node \"worker\"\n    And a feature file \"mvp-task-execution.feature\" for node \"worker\" version \"mvp\"\n    When I assemble the architecture graph\n    Then node \"worker\" has features under version \"mvp\"\n    And the feature includes filename and title\n\n  Scenario: Exclude containment edges from relationship output\n    Given a \"CONTAINS\" edge from \"supervisor-layer\" to \"supervisor\"\n    And a \"CONTROLS\" edge from \"supervisor\" to \"meta-agent\"\n    When I assemble the architecture graph\n    Then the edges list includes the \"CONTROLS\" edge\n    And the edges list does not include the \"CONTAINS\" edge\n\n  Scenario: Report accurate statistics\n    Given the database contains 3 nodes, 2 edges, 4 versions, and 1 feature\n    When I assemble the architecture graph\n    Then the stats report 3 total nodes\n    And the stats report 2 total edges\n    And the stats report 4 total versions\n    And the stats report 1 total feature\n\n  Scenario: Include a generation timestamp\n    When I assemble the architecture graph\n    Then the result includes a \"generated_at\" ISO timestamp\n"
            },
            {
              "filename": "mvp-component-management-commands.feature",
              "title": "Component Management Commands",
              "content": "Feature: Component Management Commands\n  As a project maintainer using OpenCode\n  I want slash commands to create, update, delete components and manage their properties\n  So that I can maintain the roadmap without editing SQL files directly\n\n  # â”€â”€ CreateComponent use case â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Create a new component with required fields\n    Given no node with id \"new-service\" exists\n    When I create a component with id \"new-service\" name \"New Service\" type \"app\" and layer \"supervisor-layer\"\n    Then the node \"new-service\" is saved with name \"New Service\" and type \"app\"\n    And a CONTAINS edge from \"supervisor-layer\" to \"new-service\" is created\n\n  Scenario: Create a component with optional description and tags\n    Given no node with id \"audit-log\" exists\n    When I create a component with id \"audit-log\" name \"Audit Log\" type \"component\" layer \"observability-dashboard\" description \"Tracks all changes\" and tags \"logging,audit\"\n    Then the node \"audit-log\" is saved with description \"Tracks all changes\"\n    And the node \"audit-log\" has tags \"logging\" and \"audit\"\n\n  Scenario: Create a component generates default version entries\n    Given no node with id \"new-service\" exists\n    When I create a component with id \"new-service\" name \"New Service\" type \"app\" and layer \"supervisor-layer\"\n    Then versions \"overview\", \"mvp\", \"v1\", \"v2\" are created for node \"new-service\"\n    And all versions have progress 0 and status \"planned\"\n\n  Scenario: Reject creating a component with a duplicate id\n    Given a component node \"supervisor\" exists\n    When I create a component with id \"supervisor\" name \"Duplicate\" type \"app\" and layer \"supervisor-layer\"\n    Then the create operation fails with error \"already exists\"\n\n  Scenario: Reject creating a component with an invalid type\n    When I create a component with id \"bad-type\" name \"Bad\" type \"invalid\" and layer \"supervisor-layer\"\n    Then the create operation fails with error \"Invalid node type\"\n\n  # â”€â”€ DeleteComponent use case â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Delete an existing component and its related data\n    Given a component node \"doomed\" exists\n    And a version \"mvp\" for component \"doomed\" with progress 0\n    And a feature for component \"doomed\"\n    And a \"CONTAINS\" edge from \"supervisor-layer\" to \"doomed\"\n    When I delete the component \"doomed\"\n    Then the node \"doomed\" is removed\n    And all versions for \"doomed\" are removed\n    And all features for \"doomed\" are removed\n    And all edges referencing \"doomed\" are removed\n\n  Scenario: Reject deleting a nonexistent component\n    Given no node with id \"ghost\" exists\n    When I delete the component \"ghost\"\n    Then the delete operation fails with error \"Node not found\"\n\n  # â”€â”€ Publish workflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Publish rebuilds data and exports JSON\n    Given a database with architecture data\n    When I run the publish workflow\n    Then the export produces a JSON file\n    And the exported data contains the latest node data\n\n  # â”€â”€ Individual command files removed in favour of AGENTS.md â”€â”€â”€â”€â”€\n\n  Scenario: Individual component command files have been removed\n    Given the project has an .opencode/commands directory\n    Then no file \"component-create.md\" exists in .opencode/commands\n    And no file \"component-delete.md\" exists in .opencode/commands\n    And no file \"component-update.md\" exists in .opencode/commands\n    And no file \"component-publish.md\" exists in .opencode/commands\n\n  Scenario: Only bdd and ship command files remain\n    Given the project has an .opencode/commands directory\n    Then only command files \"bdd.md\" and \"ship.md\" exist\n\n  # â”€â”€ AGENTS.md documents all API endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: AGENTS.md documents the REST API endpoints\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"REST API\"\n    And the file \"AGENTS.md\" contains \"POST\" \"/api/components\"\n    And the file \"AGENTS.md\" contains \"DELETE\" \"/api/components\"\n    And the file \"AGENTS.md\" contains \"GET\" \"/api/architecture\"\n    And the file \"AGENTS.md\" contains \"GET\" \"/api/health\"\n\n  Scenario: AGENTS.md contains curl examples for API usage\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"curl\"\n    And the file \"AGENTS.md\" contains \"https://roadmap-5vvp.onrender.com\"\n\n  # â”€â”€ Remaining command files use adapter layer, not raw DB access â”€\n\n  Scenario: Remaining command files must not contain raw sqlite3 CLI references\n    Given the project has an .opencode/commands directory\n    Then no command file contains a raw \"sqlite3\" CLI invocation\n\n  # â”€â”€ CLI adapter scripts exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: CLI adapter scripts exist for component management\n    Given the project source directory\n    Then a CLI adapter \"component-create.ts\" exists in src/adapters/cli\n    And a CLI adapter \"component-delete.ts\" exists in src/adapters/cli\n    And a CLI adapter \"export.ts\" exists in src/adapters/cli\n"
            },
            {
              "filename": "mvp-component-version-state.feature",
              "title": "Component Version State",
              "content": "Feature: Component Version State\n  As a user viewing the roadmap\n  I want each component to show its current version as a derived state\n  So that I can see at a glance whether a component is Concept, MVP, or a released version\n\n  Background:\n    Given an architecture graph with nodes and versions\n\n  # â”€â”€â”€ Domain: Node entity gains current_version â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Node with no current_version is Concept\n    Given a node with id \"future-thing\" and no current_version\n    Then the node display state should be \"Concept\"\n\n  Scenario: Node with version less than 1.0.0 is MVP\n    Given a node with id \"early-thing\" and current_version \"0.3.0\"\n    Then the node display state should be \"MVP\"\n    And the node display version should be \"0.3.0\"\n\n  Scenario: Node with version 1.0.0 or above shows major version\n    Given a node with id \"shipped-thing\" and current_version \"1.2.0\"\n    Then the node display state should be \"v1\"\n    And the node display version should be \"1.2.0\"\n\n  Scenario: Node with version 2.0.0 shows v2\n    Given a node with id \"mature-thing\" and current_version \"2.0.0\"\n    Then the node display state should be \"v2\"\n    And the node display version should be \"2.0.0\"\n\n  Scenario: Node type includes app for top-level services\n    Given a node with id \"my-app\" and type \"app\"\n    Then the node type should be \"app\"\n    And the node should be valid\n\n  # â”€â”€â”€ Domain: Version entity accepts flexible version tags â”€\n\n  Scenario: Version entity accepts arbitrary version strings\n    Given a version with version tag \"v3\"\n    Then the version should be valid\n\n  Scenario: Version entity still accepts overview tag\n    Given a version with version tag \"overview\"\n    Then the version should be valid\n\n  # â”€â”€â”€ Domain: Feature entity accepts flexible version strings â”€\n\n  Scenario: Feature derives version from v3 filename prefix\n    Given a feature file named \"v3-advanced-thing.feature\"\n    Then the derived version should be \"v3\"\n\n  Scenario: Feature still derives mvp from unprefixed filename\n    Given a feature file named \"basic-thing.feature\"\n    Then the derived version should be \"mvp\"\n\n  # â”€â”€â”€ Use Case: GetArchitecture includes version state â”€â”€â”€â”€\n\n  Scenario: Exported architecture includes current_version and display state\n    Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n    When I assemble the architecture\n    Then the enriched node \"roadmap\" should have current_version \"0.7.5\"\n    And the enriched node \"roadmap\" should have display_state \"MVP\"\n\n  Scenario: Exported architecture includes current_version null for Concept\n    Given a node \"future-thing\" with no current_version exists in the database\n    When I assemble the architecture\n    Then the enriched node \"future-thing\" should have display_state \"Concept\"\n\n  # â”€â”€â”€ Schema: node_versions accepts flexible version tags â”€\n\n  Scenario: Database accepts version tag beyond mvp/v1/v2\n    Given a node \"test-node\" exists in the database\n    When I save a version with tag \"v3\" for node \"test-node\"\n    Then the version should be persisted successfully\n\n  Scenario: Database stores current_version on nodes\n    Given a node \"test-node\" with current_version \"1.0.0\" is saved\n    When I retrieve the node \"test-node\"\n    Then the node current_version should be \"1.0.0\"\n"
            },
            {
              "filename": "mvp-data-persistence.feature",
              "title": "Data Persistence",
              "content": "Feature: Data Persistence\n  As the system\n  I want SQLite repositories to correctly store and retrieve architecture data\n  So that the database layer fulfils the domain contracts\n\n  Background:\n    Given a fresh in-memory SQLite database with the schema loaded\n\n  Scenario: Save and retrieve a node\n    Given a node with id \"test-comp\", name \"Test Component\", and type \"component\"\n    When I save the node via the repository\n    And I find the node by id \"test-comp\"\n    Then the retrieved node has name \"Test Component\"\n\n  Scenario: Find nodes by type\n    Given a saved layer node \"layer-1\"\n    And a saved component node \"comp-1\" in layer \"layer-1\"\n    And a saved component node \"comp-2\" in layer \"layer-1\"\n    When I find nodes by type \"component\"\n    Then I receive 2 nodes\n\n  Scenario: Find nodes by layer\n    Given a saved layer node \"layer-1\"\n    And a saved component node \"comp-a\" in layer \"layer-1\"\n    And a saved component node \"comp-b\" in layer \"layer-1\"\n    When I find nodes by layer \"layer-1\"\n    Then I receive 2 nodes\n\n  Scenario: Check node existence\n    Given a saved component node \"exists-node\"\n    When I check if node \"exists-node\" exists\n    Then the result is true\n    When I check if node \"missing-node\" exists\n    Then the result is false\n\n  Scenario: Delete a node\n    Given a saved component node \"to-delete\"\n    When I delete node \"to-delete\"\n    And I find the node by id \"to-delete\"\n    Then the retrieved node is null\n\n  Scenario: Save and retrieve edges\n    Given saved nodes \"src-node\" and \"tgt-node\"\n    And an edge from \"src-node\" to \"tgt-node\" of type \"CONTROLS\"\n    When I save the edge via the repository\n    And I find edges by source \"src-node\"\n    Then I receive 1 edge with target \"tgt-node\"\n\n  Scenario: Find relationship edges excluding containment\n    Given saved nodes \"layer-x\" and \"comp-x\" and \"comp-y\"\n    And a saved \"CONTAINS\" edge from \"layer-x\" to \"comp-x\"\n    And a saved \"DEPENDS_ON\" edge from \"comp-x\" to \"comp-y\"\n    When I find relationship edges\n    Then I receive only the \"DEPENDS_ON\" edge\n\n  Scenario: Save and retrieve versions\n    Given a saved component node \"versioned-comp\"\n    And a version \"mvp\" for node \"versioned-comp\" with progress 25 and status \"in-progress\"\n    When I save the version via the repository\n    And I find versions by node \"versioned-comp\"\n    Then I receive 1 version with progress 25\n\n  Scenario: Save version with progress\n    Given a saved component node \"progress-comp\"\n    And a version \"mvp\" for node \"progress-comp\" with progress 75 and status \"in-progress\"\n    When I save the version via the repository\n    And I find the version for node \"progress-comp\" version \"mvp\"\n    Then the version has progress 75 and status \"in-progress\"\n\n  Scenario: Save and retrieve features\n    Given a saved component node \"featured-comp\"\n    And a feature for node \"featured-comp\" version \"mvp\" with filename \"mvp-test.feature\"\n    When I save the feature via the repository\n    And I find features by node \"featured-comp\"\n    Then I receive 1 feature with filename \"mvp-test.feature\"\n\n  Scenario: Delete all features for idempotent re-seeding\n    Given a saved component node \"reseed-comp\"\n    And 3 saved features for node \"reseed-comp\"\n    When I delete all features\n    And I find features by node \"reseed-comp\"\n    Then I receive 0 features\n"
            },
            {
              "filename": "mvp-delete-feature.feature",
              "title": "Delete a single feature file via API",
              "content": "Feature: Delete a single feature file via API\n  As an API consumer\n  I want to delete a specific feature file from a component\n  So that I can remove outdated or incorrect BDD specs without affecting other features\n\n  Background:\n    Given the API server is running\n\n  # â”€â”€ Happy path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Delete an existing feature file returns 204\n    Given a component \"df-comp\" exists in the database\n    And the component \"df-comp\" has a feature \"mvp-example.feature\"\n    When I send a DELETE request to \"/api/components/df-comp/features/mvp-example.feature\"\n    Then the response status is 204\n\n  Scenario: Deleted feature is no longer returned by GET\n    Given a component \"df-verify\" exists in the database\n    And the component \"df-verify\" has a feature \"mvp-gone.feature\"\n    When I send a DELETE request to \"/api/components/df-verify/features/mvp-gone.feature\"\n    And I send a GET request to \"/api/components/df-verify/features\"\n    Then the response status is 200\n    And the response body does not include feature \"mvp-gone.feature\"\n\n  Scenario: Other features for the same component are not affected\n    Given a component \"df-multi\" exists in the database\n    And the component \"df-multi\" has a feature \"mvp-keep.feature\"\n    And the component \"df-multi\" has a feature \"mvp-remove.feature\"\n    When I send a DELETE request to \"/api/components/df-multi/features/mvp-remove.feature\"\n    And I send a GET request to \"/api/components/df-multi/features\"\n    Then the response status is 200\n    And the response body includes feature \"mvp-keep.feature\"\n    And the response body does not include feature \"mvp-remove.feature\"\n\n  # â”€â”€ Error cases â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Delete feature for nonexistent component returns 404\n    When I send a DELETE request to \"/api/components/ghost-comp/features/mvp-test.feature\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  Scenario: Delete nonexistent feature file returns 404\n    Given a component \"df-nofile\" exists in the database\n    When I send a DELETE request to \"/api/components/df-nofile/features/mvp-missing.feature\"\n    Then the response status is 404\n    And the response body has field \"error\"\n"
            },
            {
              "filename": "mvp-docker-render-deployment.feature",
              "title": "Docker-based Render Deployment",
              "content": "Feature: Docker-based Render Deployment\n  As an operator deploying the roadmap application to Render\n  I want the service to use a Docker runtime with sqlite3 installed\n  So that the build step can create the SQLite database\n\n  # â”€â”€ Render Blueprint (Docker) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: render.yaml specifies Docker runtime\n    Given the project source directory\n    Then a file \"render.yaml\" exists in the project\n    And the render.yaml specifies a web service\n    And the render.yaml specifies the Docker runtime\n    And the render.yaml does not specify a Node.js runtime\n\n  # â”€â”€ Dockerfile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Dockerfile exists in project root\n    Given the project source directory\n    Then a file \"Dockerfile\" exists in the project\n\n  Scenario: Dockerfile uses Node.js base image\n    Given the project source directory\n    Then the Dockerfile has a FROM instruction with a Node.js image\n\n  Scenario: Dockerfile installs sqlite3 system package\n    Given the project source directory\n    Then the Dockerfile installs sqlite3 via apt-get\n\n  Scenario: Dockerfile copies package files and installs dependencies\n    Given the project source directory\n    Then the Dockerfile copies package manifest files\n    And the Dockerfile runs npm ci\n\n  Scenario: Dockerfile copies source and builds the project\n    Given the project source directory\n    Then the Dockerfile copies the application source\n    And the Dockerfile runs the build command\n\n  Scenario: Dockerfile exposes the service port\n    Given the project source directory\n    Then the Dockerfile exposes port 3000\n\n  Scenario: Dockerfile specifies the start command\n    Given the project source directory\n    Then the Dockerfile has a CMD or ENTRYPOINT for npm start\n\n  # â”€â”€ .dockerignore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: .dockerignore exists and excludes build artifacts\n    Given the project source directory\n    Then a file \".dockerignore\" exists in the project\n    And the .dockerignore excludes \"node_modules\"\n    And the .dockerignore excludes \"dist\"\n    And the .dockerignore excludes \".git\"\n"
            },
            {
              "filename": "mvp-domain-entities.feature",
              "title": "Domain Entities",
              "content": "Feature: Domain Entities\n  As a developer\n  I want well-defined domain entities with consistent behavior\n  So that the data model is reliable across all layers\n\n  Scenario: Create a Node with required fields\n    Given node properties with id \"test-node\", name \"Test Node\", and type \"component\"\n    When I create the Node entity\n    Then the node has id \"test-node\"\n    And the node has name \"Test Node\"\n    And the node has type \"component\"\n\n  Scenario: Node parses tags from a JSON string\n    Given node properties with tags stored as JSON\n    When I create the Node entity\n    Then the node has parsed tags \"runtime\" and \"core\"\n\n  Scenario: Node accepts tags as an array\n    Given node properties with tags provided as array\n    When I create the Node entity\n    Then the node has parsed tags \"alpha\" and \"beta\"\n\n  Scenario: Node defaults optional fields to null or zero\n    Given node properties with only required fields\n    When I create the Node entity\n    Then the node layer is null\n    And the node color is null\n    And the node icon is null\n    And the node description is null\n    And the node tags are an empty array\n    And the node sort_order is 0\n\n  Scenario: Node identifies itself as a layer\n    Given node properties with type \"layer\"\n    When I create the Node entity\n    Then the node reports it is a layer\n\n  Scenario: Create an Edge with required fields\n    Given edge properties with source \"a\", target \"b\", and type \"CONTROLS\"\n    When I create the Edge entity\n    Then the edge has source_id \"a\" and target_id \"b\"\n    And the edge has type \"CONTROLS\"\n\n  Scenario: Edge identifies containment relationships\n    Given edge properties with type \"CONTAINS\"\n    When I create the Edge entity\n    Then the edge reports it is a containment edge\n\n  Scenario: Edge identifies non-containment relationships\n    Given edge properties with type \"DEPENDS_ON\"\n    When I create the Edge entity\n    Then the edge reports it is not a containment edge\n\n  Scenario: Create a Version with defaults\n    Given version properties with node_id \"comp-1\" and version \"mvp\"\n    When I create the Version entity\n    Then the version progress is 0\n    And the version status is \"planned\"\n    And the version content is null\n\n  Scenario: Version identifies its status\n    Given a version with status \"complete\"\n    When I check the version status\n    Then isComplete returns true\n    And isInProgress returns false\n\n  Scenario: Version identifies in-progress status\n    Given a version with status \"in-progress\"\n    When I check the version status\n    Then isInProgress returns true\n    And isComplete returns false\n\n  Scenario: Feature derives version from filename prefix\n    Then version for filename \"mvp-basic.feature\" is \"mvp\"\n    And version for filename \"v1-advanced.feature\" is \"v1\"\n    And version for filename \"v2-future.feature\" is \"v2\"\n    And version for filename \"other.feature\" is \"mvp\"\n\n  Scenario: Feature extracts title from Gherkin content\n    Then the title extracted from a Feature line is \"My Cool Feature\"\n    And the title falls back to filename \"fallback.feature\" giving \"fallback\"\n"
            },
            {
              "filename": "mvp-drizzle-migration.feature",
              "title": "Drizzle ORM Migration",
              "content": "Feature: Drizzle ORM Migration\n  As a project maintainer\n  I want the infrastructure layer to use Drizzle ORM instead of raw SQL\n  So that the database layer is type-safe, schema is code, and progress survives rebuilds\n\n  The migration replaces hand-written better-sqlite3 queries with Drizzle's\n  type-safe query builder. Domain entities, repository interfaces, and use cases\n  are unchanged â€” only the infrastructure implementations change.\n\n  # â”€â”€ Schema as Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Drizzle schema defines all four tables\n    Given the Drizzle schema module\n    Then it should export a nodes table definition\n    And it should export an edges table definition\n    And it should export a nodeVersions table definition\n    And it should export a features table definition\n\n  Scenario: Nodes table has correct columns\n    Given the Drizzle schema nodes table\n    Then it should have a text primary key column \"id\"\n    And it should have a text column \"name\" that is not null\n    And it should have a text column \"type\" that is not null\n    And it should have optional text columns \"layer\", \"color\", \"icon\", \"description\"\n    And it should have an integer column \"sort_order\" defaulting to 0\n\n  Scenario: Node versions table preserves progress on content upsert\n    Given a Drizzle database with schema applied\n    And a node \"test-comp\" exists\n    And a version \"mvp\" for \"test-comp\" with progress 75 and status \"in-progress\"\n    When I upsert version \"mvp\" for \"test-comp\" with new content \"Updated\" via Drizzle\n    Then the version should have content \"Updated\"\n    And the version should have progress 75\n    And the version should have status \"in-progress\"\n\n  # â”€â”€ Repository Parity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Drizzle node repository implements INodeRepository\n    Given a Drizzle database with schema applied\n    When I save a node via the Drizzle repository\n    Then I can retrieve it by id\n    And I can find it by type\n    And I can find it by layer\n    And I can check it exists\n    And I can delete it\n\n  Scenario: Drizzle edge repository implements IEdgeRepository\n    Given a Drizzle database with schema applied\n    And two nodes exist for edge testing\n    When I save an edge via the Drizzle repository\n    Then I can retrieve it by source\n    And I can retrieve it by target\n    And I can retrieve relationships excluding CONTAINS\n    And I can delete it\n\n  Scenario: Drizzle version repository implements IVersionRepository\n    Given a Drizzle database with schema applied\n    And a node \"comp-1\" exists\n    When I save a version via the Drizzle repository\n    Then I can retrieve versions by node\n    And I can retrieve a specific version by node and version tag\n    And I can update progress and status\n    And I can delete versions by node\n\n  Scenario: Drizzle feature repository implements IFeatureRepository\n    Given a Drizzle database with schema applied\n    And a node \"comp-1\" exists\n    When I save a feature via the Drizzle repository\n    Then I can retrieve features by node\n    And I can retrieve features by node and version\n    And I can delete all features\n    And I can delete features by node\n\n  # â”€â”€ Progress Persistence Through Rebuild â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progress update survives database rebuild via Drizzle upsert\n    Given a Drizzle database with schema and seed data\n    When I update progress for \"roadmap\" version \"mvp\" to 100 with status \"complete\" via Drizzle\n    And I re-run the seed data via Drizzle upsert\n    Then the version \"mvp\" for \"roadmap\" should have progress 100\n    And the version \"mvp\" for \"roadmap\" should have status \"complete\"\n\n  Scenario: Seed content updates propagate while preserving progress\n    Given a Drizzle database with schema and seed data\n    When I update progress for \"roadmap\" version \"mvp\" to 80 with status \"in-progress\" via Drizzle\n    And I re-seed with updated content for \"roadmap\" version \"mvp\"\n    Then the version \"mvp\" for \"roadmap\" should have the updated content\n    And the version \"mvp\" for \"roadmap\" should have progress 80\n    And the version \"mvp\" for \"roadmap\" should have status \"in-progress\"\n\n  Scenario: Build script does not delete the database\n    Given the package.json build scripts\n    Then the build:db script should not contain \"rm -f\"\n    And the build:db script should not contain \"rm db/\"\n"
            },
            {
              "filename": "mvp-feature-seeding.feature",
              "title": "Feature Seeding",
              "content": "Feature: Feature Seeding\n  As a specification author\n  I want feature files on disk to be seeded into the database\n  So that Gherkin specs appear alongside their component in the architecture view\n\n  Background:\n    Given a database with architecture data\n\n  Scenario: Seed a feature file for an existing component\n    Given a component node \"sanitiser\" exists\n    And a feature file \"mvp-sanitiser.feature\" with content:\n      \"\"\"\n      Feature: Sanitiser MVP\n        Scenario: Block injection\n          Given a response containing role tags\n          When the sanitiser processes it\n          Then the injection is blocked\n      \"\"\"\n    When I seed the feature files\n    Then the feature for node \"sanitiser\" is saved with version \"mvp\"\n    And the feature title is \"Sanitiser MVP\"\n\n  Scenario: Derive version from filename prefix\n    Given a component node \"worker\" exists\n    And a feature file \"v1-advanced-tools.feature\" with content:\n      \"\"\"\n      Feature: Advanced tool support\n      \"\"\"\n    When I seed the feature files\n    Then the feature is saved with version \"v1\"\n\n  Scenario: Default to mvp version when no prefix matches\n    Given a component node \"worker\" exists\n    And a feature file \"basic-execution.feature\" with content:\n      \"\"\"\n      Feature: Basic execution\n      \"\"\"\n    When I seed the feature files\n    Then the feature is saved with version \"mvp\"\n\n  Scenario: Skip feature files for unknown nodes\n    Given no node with id \"nonexistent\" exists\n    And a feature file targeting node \"nonexistent\"\n    When I seed the feature files\n    Then the feature is skipped\n    And the result reports 0 seeded and 1 skipped\n\n  Scenario: Clear existing features before re-seeding\n    Given a component node \"sanitiser\" exists\n    And the database already has features for node \"sanitiser\"\n    When I seed the feature files\n    Then all previous features are deleted before new ones are inserted\n\n  Scenario: Extract title from Gherkin Feature line\n    Given a component node \"worker\" exists\n    And a feature file \"mvp-task-execution.feature\" with content:\n      \"\"\"\n      Feature: Task execution under constraints\n        Scenario: Execute with tools\n      \"\"\"\n    When I seed the feature files\n    Then the feature title is \"Task execution under constraints\"\n\n  Scenario: Fall back to filename when no Feature line exists\n    Given a component node \"worker\" exists\n    And a feature file \"mvp-notes.feature\" with content:\n      \"\"\"\n      Some notes without a Feature line\n      \"\"\"\n    When I seed the feature files\n    Then the feature title is \"mvp-notes\"\n"
            },
            {
              "filename": "mvp-package-version-sync.feature",
              "title": "Sync roadmap current_version from package.json",
              "content": "Feature: Sync roadmap current_version from package.json\n  As a project maintainer\n  I want the roadmap component's current_version to be read from package.json\n  So that bumping package.json is the single source of truth for the project version\n\n  The roadmap node is the self-tracking component. Its current_version\n  drives derived progress for all its version phases (MVP, v1, v2).\n  Instead of hardcoding the version in seed.sql, the system should\n  read it from package.json at assembly time.\n\n  Rule: GetArchitecture accepts an optional package version\n\n    Background:\n      Given an architecture graph with nodes and versions\n\n    Scenario: Package version overrides roadmap node's current_version\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the enriched node \"roadmap\" should have current_version \"1.0.0\"\n\n    Scenario: Package version updates derived progress for roadmap\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the version \"mvp\" for node \"roadmap\" should have progress 100\n      And the version \"mvp\" for node \"roadmap\" should have status \"complete\"\n\n    Scenario: Other nodes are not affected by the package version\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a node \"worker\" with current_version \"0.3.0\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"worker\"\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the version \"mvp\" for node \"worker\" should have progress 30\n\n    Scenario: No package version preserves database value\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      When I assemble the architecture\n      Then the enriched node \"roadmap\" should have current_version \"0.7.5\"\n      And the version \"mvp\" for node \"roadmap\" should have progress 75\n\n    Scenario: Package version applies to roadmap in progression tree\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And the package version is \"1.0.0\"\n      When I assemble the architecture with the package version\n      Then the enriched node \"roadmap\" should have display_state \"v1\"\n\n    Scenario: Null package version treated as absent\n      Given a node \"roadmap\" with current_version \"0.7.5\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"roadmap\"\n      And the package version is null\n      When I assemble the architecture with the package version\n      Then the enriched node \"roadmap\" should have current_version \"0.7.5\"\n\n  Rule: seed.sql no longer hardcodes roadmap current_version\n\n    Scenario: seed.sql roadmap node has no hardcoded current_version\n      Given the project source directory\n      Then the roadmap node in seed.sql should not have a hardcoded current_version\n\n  Rule: CLI and API adapters read from package.json\n\n    Scenario: Export adapter reads version from package.json\n      Given the project source directory\n      Then the file \"src/adapters/cli/export.ts\" contains \"package.json\"\n\n    Scenario: API adapter reads version from package.json\n      Given the project source directory\n      Then the file \"src/adapters/api/start.ts\" contains \"package.json\"\n"
            },
            {
              "filename": "mvp-progression-click-dialog.feature",
              "title": "Progression tree click-to-open component dialog",
              "content": "Feature: Progression tree click-to-open component dialog\n  As a user viewing the progression tree\n  I want to click a component node to see its full details in a centered dialog\n  So that I can explore version specs and feature files without leaving the graph view\n\n  # â”€â”€â”€ Dialog structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Web view contains a dialog overlay container\n    Given the web view HTML\n    Then it should contain a dialog overlay element with class \"dialog-overlay\"\n    And the dialog overlay should be hidden by default\n\n  Scenario: Dialog has a close button\n    Given the web view HTML\n    Then the dialog should contain a close button\n\n  # â”€â”€â”€ Click replaces hover â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progression tree uses click events instead of hover for node details\n    Given the web view HTML\n    Then the cytoscape node event should use \"click\" not \"mouseover\"\n    And there should be no \"mouseover\" handler for showing node details\n\n  # â”€â”€â”€ Dialog content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Dialog renders component name and description\n    Given the web view HTML\n    Then the dialog render function should display the node name\n    And the dialog render function should display the node description\n\n  Scenario: Dialog renders version strip with MVP, v1, v2 tabs\n    Given the web view HTML\n    Then the dialog render function should include a version strip\n    And the version strip should support \"mvp\" versions\n    And the version strip should support \"v1\" versions\n    And the version strip should support \"v2\" versions\n\n  Scenario: Dialog renders version content when a version tab is selected\n    Given the web view HTML\n    Then the dialog render function should include version content display\n\n  Scenario: Dialog renders feature files section\n    Given the web view HTML\n    Then the dialog render function should include a features section\n\n  Scenario: Dialog renders progress badge\n    Given the web view HTML\n    Then the dialog render function should include a progress badge\n\n  # â”€â”€â”€ Dialog dismiss behavior â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Dialog can be closed with Escape key\n    Given the web view HTML\n    Then there should be a keydown listener for \"Escape\" to close the dialog\n\n  Scenario: Dialog can be closed by clicking the overlay background\n    Given the web view HTML\n    Then clicking the overlay background should close the dialog\n"
            },
            {
              "filename": "mvp-progression-tree-design-update.feature",
              "title": "Progression tree design update â€” hexagonal nodes and full-width layout",
              "content": "Feature: Progression tree design update â€” hexagonal nodes and full-width layout\n  As a user viewing the roadmap progression tree\n  I want component nodes rendered as hexagons instead of rectangles\n  And the tree to fill the available width without manual zoom controls\n  So that the visual design feels polished and the tree is always fully visible\n\n  # â”€â”€â”€ Hexagonal node shape â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progression tree nodes use hexagonal shape\n    Given the web view HTML\n    Then the cytoscape node shape should be \"hexagon\" not \"roundrectangle\"\n\n  Scenario: Hexagonal nodes have adequate dimensions for labels\n    Given the web view HTML\n    Then the cytoscape node width should be at least 120 pixels\n    And the cytoscape node height should be at least 120 pixels\n\n  # â”€â”€â”€ Zoom removal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: User zooming is disabled on the progression tree\n    Given the web view HTML\n    Then userZoomingEnabled should be false in the cytoscape config\n\n  Scenario: User panning is disabled on the progression tree\n    Given the web view HTML\n    Then userPanningEnabled should be false in the cytoscape config\n\n  # â”€â”€â”€ Full-width fit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Progression tree fits the container width after rendering\n    Given the web view HTML\n    Then the cytoscape instance should call fit after layout completes\n\n  Scenario: Progression tree re-fits on window resize\n    Given the web view HTML\n    Then there should be a resize listener that calls fit on the cytoscape instance\n"
            },
            {
              "filename": "mvp-progression-tree.feature",
              "title": "Progression Tree",
              "content": "Feature: Progression Tree\n  As a user viewing the roadmap\n  I want a game-style progression tree showing apps and their dependencies\n  So that I can see what needs to be built to unlock the next app\n\n  Background:\n    Given an architecture graph with app-type nodes and dependency edges\n\n  # â”€â”€â”€ Data Model: App-type filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Only app-type nodes appear in the progression tree\n    Given nodes of types \"app\", \"component\", \"layer\", \"external\", \"phase\"\n    When I filter for progression tree nodes\n    Then only nodes with type \"app\" should be included\n\n  Scenario: App nodes have dependency edges between them\n    Given app node \"supervisor\" depends on \"state-store\"\n    When I retrieve the dependency graph for apps\n    Then there should be a DEPENDS_ON edge from \"supervisor\" to \"state-store\"\n\n  # â”€â”€â”€ Use Case: GetArchitecture provides progression data â”€\n\n  Scenario: Architecture export includes progression tree data\n    Given app nodes with DEPENDS_ON edges exist\n    When I assemble the architecture\n    Then the result should include a progression_tree section\n    And the progression_tree should contain only app-type nodes\n    And the progression_tree should contain DEPENDS_ON edges between apps\n\n  Scenario: Progression tree nodes include version state\n    Given app node \"supervisor\" with current_version \"0.1.0\"\n    And app node \"state-store\" with no current_version\n    When I assemble the architecture\n    Then progression node \"supervisor\" should have display_state \"MVP\"\n    And progression node \"state-store\" should have display_state \"Concept\"\n\n  # â”€â”€â”€ Visual States â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Concept nodes are visually locked\n    Given an app node with no current_version\n    Then its visual state should be \"locked\"\n\n  Scenario: MVP nodes are visually in-progress\n    Given an app node with current_version \"0.2.0\"\n    Then its visual state should be \"in-progress\"\n\n  Scenario: Released nodes are visually complete\n    Given an app node with current_version \"1.0.0\"\n    Then its visual state should be \"complete\"\n\n  # â”€â”€â”€ Dependency Ordering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Nodes with no dependencies are root nodes\n    Given app node \"state-store\" with no dependencies\n    When I compute the tree layout\n    Then \"state-store\" should be at the top level\n\n  Scenario: Dependent nodes appear below their dependencies\n    Given app node \"supervisor\" depends on \"state-store\"\n    When I compute the tree layout\n    Then \"supervisor\" should appear below \"state-store\"\n\n  # â”€â”€â”€ Tab Structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Web view has two tabs\n    Given the web view is loaded\n    Then there should be a \"Progression\" tab\n    And there should be an \"Architecture\" tab\n    And the \"Progression\" tab should be active by default\n"
            },
            {
              "filename": "mvp-quality-checks-alignment.feature",
              "title": "Quality Checks Alignment",
              "content": "Feature: Quality Checks Alignment\n  As a developer on the roadmap project\n  I want the same quality gates as the open-bot repository\n  So that code quality standards are consistent across all projects\n\n  # â”€â”€â”€ Pre-commit pipeline completeness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Pre-commit pipeline includes coverage enforcement\n    Given the pre-commit script in package.json\n    Then it should include the \"test:coverage\" step\n    And it should not include a bare \"test:unit\" step without coverage\n\n  Scenario: Pre-commit pipeline includes BDD feature tests\n    Given the pre-commit script in package.json\n    Then it should include the \"test:features\" step\n\n  Scenario: Pre-commit pipeline runs 7 stages in order\n    Given the pre-commit script in package.json\n    Then it should run these stages in order:\n      | stage              |\n      | check:code-quality |\n      | lint               |\n      | format:check       |\n      | typecheck          |\n      | build:ts           |\n      | test:coverage      |\n      | test:features      |\n\n  # â”€â”€â”€ Coverage thresholds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Coverage thresholds are set at 90%\n    Given the vitest config file\n    Then the coverage thresholds should be:\n      | metric     | value |\n      | statements | 90    |\n      | branches   | 90    |\n      | functions  | 90    |\n      | lines      | 90    |\n\n  Scenario: Coverage excludes CLI adapter entry points\n    Given the vitest config file\n    Then the coverage exclude list should contain \"src/adapters/cli/**\"\n\n  # â”€â”€â”€ Code quality script: BDD feature coverage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script checks feature files exist\n    Given the code quality script\n    Then it should check that \"features/\" directory contains .feature files\n\n  Scenario: Code quality script checks every feature has scenarios\n    Given the code quality script\n    Then it should verify each feature file has at least one Scenario\n\n  Scenario: Code quality script runs cucumber dry-run\n    Given the code quality script\n    Then it should run a cucumber-js dry-run to detect undefined steps\n\n  Scenario: Code quality script detects orphaned step definitions\n    Given the code quality script\n    Then it should check for orphaned step definitions via usage report\n\n  # â”€â”€â”€ Code quality script: barrel bypass detection â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script detects direct imports bypassing barrels\n    Given the code quality script\n    Then it should check for direct imports that bypass barrel exports in source\n\n  # â”€â”€â”€ Code quality script: domain error checking â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script checks domain layer uses domain-specific errors\n    Given the code quality script\n    Then it should check that the domain layer does not use \"throw new Error(\"\n\n  # â”€â”€â”€ Code quality script: enhanced dead code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Code quality script runs ESLint unused-vars check\n    Given the code quality script\n    Then it should count ESLint \"@typescript-eslint/no-unused-vars\" violations\n\n  # â”€â”€â”€ Knip: unused exports and dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Knip is installed as a devDependency\n    Given the package.json file\n    Then \"knip\" should be in devDependencies\n\n  Scenario: Knip config exists with correct entry points\n    Given the knip config file\n    Then it should specify entry points including \"src/adapters/cli/*.ts\"\n    And it should specify project files including \"src/**/*.ts\"\n\n  Scenario: Knip has an npm script\n    Given the package.json file\n    Then there should be a \"check:knip\" npm script\n\n  Scenario: Code quality script runs knip\n    Given the code quality script\n    Then it should invoke knip to check for unused exports and dependencies\n\n  # â”€â”€â”€ dependency-cruiser: architectural boundaries â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: dependency-cruiser is installed as a devDependency\n    Given the package.json file\n    Then \"dependency-cruiser\" should be in devDependencies\n\n  Scenario: dependency-cruiser config enforces Clean Architecture\n    Given the dependency-cruiser config file\n    Then it should have a rule preventing domain from importing infrastructure\n    And it should have a rule preventing domain from importing adapters\n    And it should have a rule detecting circular dependencies\n\n  Scenario: dependency-cruiser has an npm script\n    Given the package.json file\n    Then there should be a \"check:deps\" npm script\n\n  Scenario: Code quality script runs dependency-cruiser\n    Given the code quality script\n    Then it should invoke dependency-cruiser to validate architecture\n\n  # â”€â”€â”€ AGENTS.md completeness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: AGENTS.md documents all pre-commit stages\n    Given the AGENTS.md file\n    Then it should document \"test:coverage\" as a pre-commit stage\n    And it should document \"test:features\" as a pre-commit stage\n\n  Scenario: AGENTS.md documents the code quality checks\n    Given the AGENTS.md file\n    Then it should document at least 10 code quality script checks\n\n  Scenario: AGENTS.md documents knip and dependency-cruiser\n    Given the AGENTS.md file\n    Then it should document \"knip\" as a quality tool\n    And it should document \"dependency-cruiser\" as a quality tool\n"
            },
            {
              "filename": "mvp-remove-update-progress.feature",
              "title": "Remove manual updateProgress in favour of derived progress",
              "content": "Feature: Remove manual updateProgress in favour of derived progress\n  As a project maintainer\n  I want the manual updateProgress endpoint and use case removed\n  Because derived progress from current_version is the sole source of truth\n  And the manual write path is dead code that silently gets overridden\n\n  # â”€â”€ Use case removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: UpdateProgress use case file does not exist\n    Given the project source directory\n    Then no file \"update-progress.ts\" exists in src/use-cases\n\n  Scenario: UpdateProgress is not exported from use-cases barrel\n    Given the project source directory\n    Then the file \"src/use-cases/index.ts\" does not contain \"UpdateProgress\"\n    And the file \"src/use-cases/index.ts\" does not contain \"update-progress\"\n\n  # â”€â”€ CLI adapter removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: component-update CLI adapter does not exist\n    Given the project source directory\n    Then no file \"component-update.ts\" exists in src/adapters/cli\n\n  # â”€â”€ API route removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: API routes file does not contain updateProgress handler\n    Given the project source directory\n    Then the file \"src/adapters/api/routes.ts\" does not contain \"handleUpdateProgress\"\n    And the file \"src/adapters/api/routes.ts\" does not contain \"UpdateProgress\"\n    And the file \"src/adapters/api/routes.ts\" does not contain \"parseProgressInput\"\n\n  Scenario: No PATCH progress route is registered\n    Given the project source directory\n    Then the file \"src/adapters/api/routes.ts\" does not contain \"PATCH\"\n\n  # â”€â”€ Repository interface cleaned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: IVersionRepository does not declare updateProgress\n    Given the project source directory\n    Then the file \"src/domain/repositories/version-repository.ts\" does not contain \"updateProgress\"\n\n  # â”€â”€ Infrastructure implementations cleaned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: SQLite version repository does not implement updateProgress\n    Given the project source directory\n    Then the file \"src/infrastructure/sqlite/version-repository.ts\" does not contain \"updateProgress\"\n\n  Scenario: Drizzle version repository does not implement updateProgress\n    Given the project source directory\n    Then the file \"src/infrastructure/drizzle/version-repository.ts\" does not contain \"updateProgress\"\n\n  # â”€â”€ OpenCode commands cleaned â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: component-progress command file does not exist\n    Given the project has an .opencode/commands directory\n    Then no file \"component-progress.md\" exists in .opencode/commands\n\n  Scenario: component-update command file has been removed\n    Given the project has an .opencode/commands directory\n    Then no file \"component-update.md\" exists in .opencode/commands\n\n  # â”€â”€ Derived progress remains intact â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Version entity still has deriveProgress method\n    Given the project source directory\n    Then the file \"src/domain/entities/version.ts\" contains \"deriveProgress\"\n\n  Scenario: GetArchitecture still applies derived progress\n    Given the project source directory\n    Then the file \"src/use-cases/get-architecture.ts\" contains \"applyDerivedProgress\"\n\n  # â”€â”€ Documentation updated â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: README does not reference update-progress\n    Given the project README file\n    Then the README does not contain \"update-progress\"\n\n  Scenario: AGENTS.md does not reference update-progress\n    Given the project source directory\n    Then the file \"AGENTS.md\" does not contain \"update-progress\"\n"
            },
            {
              "filename": "mvp-render-api-commands.feature",
              "title": "Render API Commands and README Update",
              "content": "Feature: Render API Commands and README Update\n  As a project maintainer\n  I want the OpenCode commands to use the production Render API URL\n  And the README to reflect the live Render deployment\n  So that commands work against the deployed service and documentation is accurate\n\n  # â”€â”€ API documentation uses Render production URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: AGENTS.md documents API with Render production URL\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"https://roadmap-5vvp.onrender.com\"\n\n  Scenario: AGENTS.md curl examples use the Render production URL\n    Given the project source directory\n    Then the file \"AGENTS.md\" contains \"curl\"\n    And the file \"AGENTS.md\" contains \"https://roadmap-5vvp.onrender.com/api/components\"\n\n  Scenario: Individual component command files no longer exist\n    Given the project has an .opencode/commands directory\n    Then no file \"component-create.md\" exists in .opencode/commands\n    And no file \"component-delete.md\" exists in .opencode/commands\n    And no file \"component-update.md\" exists in .opencode/commands\n    And no file \"component-publish.md\" exists in .opencode/commands\n\n  # â”€â”€ README reflects Render deployment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: README references the Render live URL\n    Given the project README file\n    Then the README contains the Render deployment URL \"https://roadmap-5vvp.onrender.com\"\n\n  Scenario: README does not reference GitHub Pages deployment\n    Given the project README file\n    Then the README does not contain \"github.io/roadmap\"\n    And the README does not contain \"GitHub Pages\"\n\n  Scenario: README does not reference the pages.yml workflow\n    Given the project README file\n    Then the README does not contain \"pages.yml\"\n\n  Scenario: README deployment section describes Render\n    Given the project README file\n    Then the README deployment section mentions \"Render\"\n\n  Scenario: README tech stack references Render instead of GitHub Actions for deployment\n    Given the project README file\n    Then the README does not contain \"CI/CD to GitHub Pages\"\n\n  # â”€â”€ GitHub Pages workflow removed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: GitHub Pages workflow file does not exist\n    Given the project source directory\n    Then no file \"pages.yml\" exists in .github/workflows\n"
            },
            {
              "filename": "mvp-render-deployment.feature",
              "title": "Render Deployment",
              "content": "Feature: Render Deployment\n  As an operator deploying the roadmap application\n  I want the app configured for Render hosting\n  So that I can deploy both the API and web view as a single service\n\n  # â”€â”€ Static File Serving â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: API server serves index.html at root\n    Given the API server is running with static file serving\n    When I request the path \"/\"\n    Then the render response status is 200\n    And the render response content type contains \"text/html\"\n\n  Scenario: API server serves data.json from web directory\n    Given the API server is running with static file serving\n    When I request the path \"/data.json\"\n    Then the render response status is 200\n    And the render response content type contains \"application/json\"\n\n  Scenario: API routes still work alongside static serving\n    Given the API server is running with static file serving\n    When I request the path \"/api/health\"\n    Then the render response status is 200\n    And the render response body has field \"status\" with value \"ok\"\n\n  Scenario: Unknown static file returns 404\n    Given the API server is running with static file serving\n    When I request the path \"/nonexistent-file.xyz\"\n    Then the render response status is 404\n\n  Scenario: Path traversal attempts are rejected\n    Given the API server is running with static file serving\n    When I request the path \"/../package.json\"\n    Then the render response status is 404\n\n  # â”€â”€ Render Blueprint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: render.yaml exists with required fields\n    Given the project source directory\n    Then a file \"render.yaml\" exists in the project\n    And the render.yaml specifies a web service\n    And the render.yaml specifies the Docker runtime\n\n  # â”€â”€ Production Start â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Package.json has a start script for production\n    Given the project source directory\n    Then the package.json has a \"start\" script\n    And the start script runs the compiled server\n\n  Scenario: Server listens on PORT environment variable\n    Given the API server is running with static file serving on a dynamic port\n    When I request the path \"/api/health\"\n    Then the render response status is 200\n\n  # â”€â”€ CORS headers on static files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Static file responses include CORS headers\n    Given the API server is running with static file serving\n    When I request the path \"/data.json\"\n    Then the render response includes CORS headers\n"
            },
            {
              "filename": "mvp-rest-api.feature",
              "title": "REST API Adapter",
              "content": "Feature: REST API Adapter\n  As an LLM-powered CLI coding tool\n  I want a REST API to read and manage the architecture graph and feature files\n  So that I can traverse the component graph, understand dependencies, and manage feature files programmatically\n\n  # â”€â”€ Health Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Health endpoint returns server status\n    Given the API server is running\n    When I send a GET request to \"/api/health\"\n    Then the response status is 200\n    And the response body has field \"status\" with value \"ok\"\n\n  # â”€â”€ Architecture Graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Retrieve full architecture graph\n    Given the API server is running\n    And the database contains architecture data\n    When I send a GET request to \"/api/architecture\"\n    Then the response status is 200\n    And the response body has field \"layers\"\n    And the response body has field \"nodes\"\n    And the response body has field \"edges\"\n    And the response body has field \"progression_tree\"\n    And the response body has field \"stats\"\n\n  # â”€â”€ Component CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: List all components\n    Given the API server is running\n    And the database contains architecture data\n    When I send a GET request to \"/api/components\"\n    Then the response status is 200\n    And the response body is a non-empty array\n\n  Scenario: Get a single component by ID\n    Given the API server is running\n    And a component \"test-comp\" exists in the database\n    When I send a GET request to \"/api/components/test-comp\"\n    Then the response status is 200\n    And the response body has field \"id\" with value \"test-comp\"\n\n  Scenario: Get a nonexistent component returns 404\n    Given the API server is running\n    When I send a GET request to \"/api/components/nonexistent-comp\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  Scenario: Create a new component via POST\n    Given the API server is running\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"new-api-comp\",\"name\":\"New API Component\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n      \"\"\"\n    Then the response status is 201\n    And the response body has field \"id\" with value \"new-api-comp\"\n\n  Scenario: Create a component with duplicate ID returns 409\n    Given the API server is running\n    And a component \"dup-comp\" exists in the database\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"dup-comp\",\"name\":\"Duplicate\",\"type\":\"app\",\"layer\":\"supervisor-layer\"}\n      \"\"\"\n    Then the response status is 409\n    And the response body has field \"error\"\n\n  Scenario: Create a component with invalid type returns 400\n    Given the API server is running\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"bad-type\",\"name\":\"Bad\",\"type\":\"invalid\",\"layer\":\"supervisor-layer\"}\n      \"\"\"\n    Then the response status is 400\n    And the response body has field \"error\"\n\n  Scenario: Create a component with missing required fields returns 400\n    Given the API server is running\n    When I send a POST request to \"/api/components\" with body:\n      \"\"\"\n      {\"id\":\"missing-name\"}\n      \"\"\"\n    Then the response status is 400\n    And the response body has field \"error\"\n\n  Scenario: Delete a component via DELETE\n    Given the API server is running\n    And a component \"delete-me\" exists in the database\n    When I send a DELETE request to \"/api/components/delete-me\"\n    Then the response status is 204\n\n  Scenario: Delete a nonexistent component returns 404\n    Given the API server is running\n    When I send a DELETE request to \"/api/components/ghost-comp\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  # â”€â”€ Feature File Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Get feature files for a component\n    Given the API server is running\n    And a component \"feat-comp\" exists in the database\n    And the component \"feat-comp\" has feature files\n    When I send a GET request to \"/api/components/feat-comp/features\"\n    Then the response status is 200\n    And the response body is a non-empty array\n\n  Scenario: Get feature files for a nonexistent component returns 404\n    Given the API server is running\n    When I send a GET request to \"/api/components/nonexistent-feat/features\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  Scenario: Upload a feature file for a component\n    Given the API server is running\n    And a component \"upload-comp\" exists in the database\n    When I send a PUT request to \"/api/components/upload-comp/features/mvp-test.feature\" with body:\n      \"\"\"\n      Feature: Test Upload\n        Scenario: A test scenario\n          Given something\n          Then something happens\n      \"\"\"\n    Then the response status is 200\n    And the response body has field \"filename\" with value \"mvp-test.feature\"\n\n  Scenario: Upload a feature file for a nonexistent component returns 404\n    Given the API server is running\n    When I send a PUT request to \"/api/components/ghost-upload/features/mvp-test.feature\" with body:\n      \"\"\"\n      Feature: Ghost Upload\n        Scenario: A test\n          Given something\n      \"\"\"\n    Then the response status is 404\n    And the response body has field \"error\"\n\n  # â”€â”€ Graph Traversal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Get edges for a component (dependencies and dependents)\n    Given the API server is running\n    And a component \"graph-comp\" exists in the database\n    And the component \"graph-comp\" has edges\n    When I send a GET request to \"/api/components/graph-comp/edges\"\n    Then the response status is 200\n    And the response body has field \"inbound\"\n    And the response body has field \"outbound\"\n\n  Scenario: Get dependency tree for a component\n    Given the API server is running\n    And a component \"dep-comp\" exists in the database\n    When I send a GET request to \"/api/components/dep-comp/dependencies\"\n    Then the response status is 200\n    And the response body has field \"dependencies\"\n    And the response body has field \"dependents\"\n\n  # â”€â”€ API Server Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: API adapter script exists\n    Given the project source directory\n    Then a file \"src/adapters/api/server.ts\" exists in the project\n    And a file \"src/adapters/api/routes.ts\" exists in the project\n    And a file \"src/adapters/api/index.ts\" exists in the project\n"
            },
            {
              "filename": "mvp-roadmap-component.feature",
              "title": "Roadmap as a Component",
              "content": "Feature: Roadmap as a Component\n  As a developer of the roadmap itself\n  I want the roadmap to track itself as a component in the architecture graph\n  So that it appears in the progression tree with its own features and version\n\n  # â”€â”€â”€ Self-tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Roadmap exists as an app node\n    Given the architecture database is seeded\n    When I look up the node \"roadmap\"\n    Then it should exist with type \"app\"\n    And it should have a current_version\n    And its display state should be \"MVP\"\n\n  Scenario: Roadmap has version specs\n    Given the architecture database is seeded\n    When I retrieve versions for node \"roadmap\"\n    Then there should be an \"overview\" version\n    And there should be an \"mvp\" version\n\n  # â”€â”€â”€ Feature files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Roadmap feature files live under components/roadmap/features\n    Given feature files exist at \"components/roadmap/features/\"\n    When I seed features into the database\n    Then features should be linked to node \"roadmap\"\n\n  Scenario: Roadmap feature files are versioned correctly\n    Given a feature file \"mvp-architecture-graph-assembly.feature\" under \"components/roadmap/features/\"\n    When I seed features into the database\n    Then the feature version should be \"mvp\"\n\n  # â”€â”€â”€ Progression Tree Presence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Roadmap appears in the progression tree\n    Given the architecture is assembled with progression data\n    When I look at the progression tree\n    Then \"roadmap\" should be present as a node\n    And it should show its current version\n\n  # â”€â”€â”€ App Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Scenario: Key runtime components are classified as apps\n    Given the architecture database is seeded\n    Then the following nodes should have type \"app\":\n      | node_id                |\n      | roadmap                |\n      | supervisor             |\n      | meta-agent             |\n      | worker                 |\n      | state-store            |\n      | user-knowledge-graph   |\n      | rpg-code-graph         |\n      | live-dashboard         |\n      | mcp-proxy-meta         |\n      | mcp-proxy-worker       |\n      | sanitiser              |\n      | human-gate             |\n      | checkpointer           |\n      | context-rebuilder      |\n\n  Scenario: Internal tools and layers remain as non-app types\n    Given the architecture database is seeded\n    Then node \"observability-dashboard\" should have type \"layer\"\n    And node \"goal-queue\" should have type \"component\"\n    And node \"phase-feature\" should have type \"phase\"\n    And node \"tool-search\" should have type \"external\"\n"
            },
            {
              "filename": "mvp-version-derived-progress.feature",
              "title": "Version-Derived Progress",
              "content": "Feature: Version-Derived Progress\n  As a project manager\n  I want each version phase (MVP, v1, v2) to derive its progress from the component's current_version number\n  So that version numbers are the single source of truth for build progress\n\n  The mapping rule is:\n    - major 0 = MVP phase, progress = minor * 10 + patch\n    - major 1 = v1 phase, progress = minor * 10 + patch\n    - major 2 = v2 phase, progress = minor * 10 + patch\n    - Once a phase's major version is reached, that phase is 100%\n    - Phases beyond the current major are 0%\n    - No current_version means all phases are 0%\n\n  Examples:\n    current_version 0.5.0 -> MVP 50%, v1 0%, v2 0%\n    current_version 0.7.5 -> MVP 75%, v1 0%, v2 0%\n    current_version 1.0.0 -> MVP 100%, v1 0%, v2 0%\n    current_version 1.3.3 -> MVP 100%, v1 33%, v2 0%\n    current_version 2.7.0 -> MVP 100%, v1 100%, v2 70%\n    current_version 3.0.0 -> MVP 100%, v1 100%, v2 100%\n\n  Rule: Domain â€” Version entity derives progress from current_version\n\n    Scenario: Derive MVP progress from pre-1.0 current_version\n      Given a node with current_version \"0.5.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 50\n\n    Scenario: Derive MVP progress at zero\n      Given a node with current_version \"0.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 0\n\n    Scenario: MVP is 100% once major version reaches 1\n      Given a node with current_version \"1.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 100\n\n    Scenario: MVP stays 100% for higher major versions\n      Given a node with current_version \"2.3.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 100\n\n    Scenario: Derive v1 progress from major version 1\n      Given a node with current_version \"1.3.0\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 30\n\n    Scenario: v1 is 0% when still in MVP phase\n      Given a node with current_version \"0.8.0\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 0\n\n    Scenario: v1 is 100% once major version reaches 2\n      Given a node with current_version \"2.0.0\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 100\n\n    Scenario: Derive v2 progress from major version 2\n      Given a node with current_version \"2.7.0\"\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 70\n\n    Scenario: v2 is 0% when still in v1 phase\n      Given a node with current_version \"1.5.0\"\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 0\n\n    Scenario: v2 is 100% once major version reaches 3\n      Given a node with current_version \"3.0.0\"\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 100\n\n    Scenario: All phases are 0% when no current_version\n      Given a node with no current_version\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 0\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 0\n      When I derive phase progress for version \"v2\"\n      Then the derived progress should be 0\n\n    Scenario: Derive status from progress value\n      Given a node with current_version \"0.5.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 50\n      And the derived status should be \"in-progress\"\n\n    Scenario: Status is planned when progress is 0\n      Given a node with current_version \"0.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 0\n      And the derived status should be \"planned\"\n\n    Scenario: Status is complete when progress is 100\n      Given a node with current_version \"1.0.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 100\n      And the derived status should be \"complete\"\n\n    Scenario: Minor digit 9 gives 90%\n      Given a node with current_version \"0.9.0\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 90\n\n    Scenario: Patch digit adds sub-10% precision\n      Given a node with current_version \"0.7.5\"\n      When I derive phase progress for version \"mvp\"\n      Then the derived progress should be 75\n\n    Scenario: Patch digit works across phases\n      Given a node with current_version \"1.4.5\"\n      When I derive phase progress for version \"v1\"\n      Then the derived progress should be 45\n\n    Scenario: Overview version is unaffected by derivation\n      Given a node with current_version \"1.5.0\"\n      When I derive phase progress for version \"overview\"\n      Then the derived progress should be 0\n\n  Rule: Use Case â€” GetArchitecture uses derived progress\n\n    Background:\n      Given an architecture graph with nodes and versions\n\n    Scenario: Exported architecture uses derived progress for MVP\n      Given a node \"test-comp\" with current_version \"0.7.0\" exists in the database\n      And a version \"mvp\" with manual progress 20 exists for \"test-comp\"\n      When I assemble the architecture\n      Then the version \"mvp\" for node \"test-comp\" should have progress 70\n\n    Scenario: Exported architecture uses derived progress for v1\n      Given a node \"test-comp\" with current_version \"1.4.0\" exists in the database\n      And a version \"v1\" with manual progress 0 exists for \"test-comp\"\n      When I assemble the architecture\n      Then the version \"v1\" for node \"test-comp\" should have progress 40\n\n    Scenario: Exported architecture derives status from progress\n      Given a node \"test-comp\" with current_version \"1.0.0\" exists in the database\n      And a version \"mvp\" with manual progress 0 exists for \"test-comp\"\n      When I assemble the architecture\n      Then the version \"mvp\" for node \"test-comp\" should have progress 100\n      And the version \"mvp\" for node \"test-comp\" should have status \"complete\"\n\n    Scenario: Node without current_version keeps manual progress\n      Given a node \"manual-comp\" with no current_version exists in the database\n      And a version \"mvp\" with manual progress 40 exists for \"manual-comp\"\n      When I assemble the architecture\n      Then the version \"mvp\" for node \"manual-comp\" should have progress 40\n"
            }
          ],
          "v1": [
            {
              "filename": "v1-api-component-management.feature",
              "title": "API Component and Graph Management",
              "content": "@wip @v1\nFeature: API Component and Graph Management\n  As an LLM engineer using the roadmap API headlessly\n  I want comprehensive endpoints to manage components, edges, and versions\n  So that I can programmatically build and maintain the architecture graph\n  without any manual intervention or web UI interaction\n\n  The MVP API provides basic CRUD for components and simple progress updates.\n  V1 extends this with full edge management, version lifecycle, bulk operations,\n  component search, filtering, and batch mutations. Every mutation endpoint\n  validates inputs rigorously and returns structured error responses.\n  All endpoints require appropriate API key scopes (see v1-secure-api.feature).\n\n  # â”€â”€ Component CRUD (Enhanced) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Components can be created, read, updated, and deleted via the API\n\n    Scenario: Create a component with all fields\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\n          \"id\": \"full-component\",\n          \"name\": \"Full Component\",\n          \"type\": \"component\",\n          \"layer\": \"supervisor-layer\",\n          \"description\": \"A fully specified component for testing\",\n          \"tags\": [\"runtime\", \"core\", \"v1\"],\n          \"color\": \"#3498DB\",\n          \"icon\": \"server\",\n          \"sort_order\": 42\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"id\" with value \"full-component\"\n      And the response body has field \"description\"\n      And the response body has field \"tags\" containing \"runtime\"\n      And the response body has field \"color\" with value \"#3498DB\"\n      And the response body has field \"sort_order\" with value \"42\"\n\n    Scenario: Create a component with minimal fields\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"minimal-comp\",\"name\":\"Minimal\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"description\" with value null\n      And the response body has field \"tags\" as an empty array\n      And the response body has field \"sort_order\" with value \"0\"\n\n    Scenario: Create a store-type component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"new-store\",\"name\":\"New Store\",\"type\":\"store\",\"layer\":\"shared-state\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"type\" with value \"store\"\n\n    Scenario: Create an app-type component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"new-app\",\"name\":\"New App\",\"type\":\"app\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"type\" with value \"app\"\n\n    Scenario: Reject component with invalid type\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"bad-type\",\"name\":\"Bad\",\"type\":\"widget\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"type\"\n\n    Scenario: Reject component with invalid layer reference\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"bad-layer\",\"name\":\"Bad\",\"type\":\"component\",\"layer\":\"nonexistent-layer\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"layer\"\n\n    Scenario: Reject component with ID longer than 64 characters\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with an ID of 65 characters\n      Then the response status is 400\n      And the response body has field \"error\" containing \"id\"\n\n    Scenario: Reject component with empty name\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"no-name\",\"name\":\"\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"name\"\n\n  # â”€â”€ Component Update â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Components can be partially updated via PATCH\n\n    Scenario: Update component name\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"patch-comp\" exists\n      When I send a PATCH request to \"/api/components/patch-comp\" with body:\n        \"\"\"\n        {\"name\":\"Updated Name\"}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"name\" with value \"Updated Name\"\n      And the response body has field \"id\" with value \"patch-comp\"\n\n    Scenario: Update component description\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"desc-comp\" exists\n      When I send a PATCH request to \"/api/components/desc-comp\" with body:\n        \"\"\"\n        {\"description\":\"New description for the component\"}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"description\" with value \"New description for the component\"\n\n    Scenario: Update component tags\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"tag-comp\" exists with tags [\"old\"]\n      When I send a PATCH request to \"/api/components/tag-comp\" with body:\n        \"\"\"\n        {\"tags\":[\"new\",\"updated\"]}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"tags\" containing \"new\" and \"updated\"\n\n    Scenario: Update component sort_order\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"sort-comp\" exists with sort_order 10\n      When I send a PATCH request to \"/api/components/sort-comp\" with body:\n        \"\"\"\n        {\"sort_order\":99}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"sort_order\" with value \"99\"\n\n    Scenario: Update component current_version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"ver-comp\" exists with current_version null\n      When I send a PATCH request to \"/api/components/ver-comp\" with body:\n        \"\"\"\n        {\"current_version\":\"0.5.0\"}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"current_version\" with value \"0.5.0\"\n\n    Scenario: Update component current_version triggers progress recalculation\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"recalc-comp\" exists with version \"mvp\" at progress 0\n      When I send a PATCH request to \"/api/components/recalc-comp\" with body:\n        \"\"\"\n        {\"current_version\":\"0.7.5\"}\n        \"\"\"\n      Then the response status is 200\n      And the version \"mvp\" for \"recalc-comp\" now has derived progress 75\n\n    Scenario: Reject update with invalid current_version format\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-ver-comp\" exists\n      When I send a PATCH request to \"/api/components/bad-ver-comp\" with body:\n        \"\"\"\n        {\"current_version\":\"not-semver\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version\"\n\n    Scenario: Update nonexistent component returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PATCH request to \"/api/components/ghost\" with body:\n        \"\"\"\n        {\"name\":\"Ghost\"}\n        \"\"\"\n      Then the response status is 404\n\n    Scenario: Update preserves unmodified fields\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"preserve-comp\" exists with name \"Original\" and description \"Keep me\"\n      When I send a PATCH request to \"/api/components/preserve-comp\" with body:\n        \"\"\"\n        {\"name\":\"Changed\"}\n        \"\"\"\n      Then the response body has field \"name\" with value \"Changed\"\n      And the response body has field \"description\" with value \"Keep me\"\n\n  # â”€â”€ Component Listing and Filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Components can be listed with filtering and search\n\n    Scenario: List all components\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And the database contains 60 components\n      When I send a GET request to \"/api/components\"\n      Then the response status is 200\n      And the response body is an array\n      And layers are excluded from the result\n\n    Scenario: Filter components by type\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?type=store\"\n      Then the response status is 200\n      And every item in the response has type \"store\"\n\n    Scenario: Filter components by layer\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?layer=supervisor-layer\"\n      Then the response status is 200\n      And every item in the response has layer \"supervisor-layer\"\n\n    Scenario: Filter components by tag\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components with tag \"runtime\" exist\n      When I send a GET request to \"/api/components?tag=runtime\"\n      Then the response status is 200\n      And every item in the response has \"runtime\" in its tags\n\n    Scenario: Search components by name (partial match)\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?search=proxy\"\n      Then the response status is 200\n      And every item has \"proxy\" in its name (case-insensitive)\n\n    Scenario: Combine multiple filters\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?type=component&layer=supervisor-layer\"\n      Then the response status is 200\n      And every item has type \"component\" and layer \"supervisor-layer\"\n\n    Scenario: Empty filter result returns empty array\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components?type=nonexistent\"\n      Then the response status is 200\n      And the response body is an empty array\n\n  # â”€â”€ Edge Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Edges can be created, read, and deleted via the API\n\n    Scenario: Create a new edge between components\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"edge-src\" and \"edge-tgt\" exist\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"edge-src\",\"target_id\":\"edge-tgt\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"source_id\" with value \"edge-src\"\n      And the response body has field \"target_id\" with value \"edge-tgt\"\n      And the response body has field \"type\" with value \"DEPENDS_ON\"\n\n    Scenario: Create an edge with label and metadata\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"meta-src\" and \"meta-tgt\" exist\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\n          \"source_id\": \"meta-src\",\n          \"target_id\": \"meta-tgt\",\n          \"type\": \"CONTROLS\",\n          \"label\": \"spawns and monitors\",\n          \"metadata\": {\"restart_policy\": \"always\", \"max_retries\": 5}\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"label\" with value \"spawns and monitors\"\n      And the response body has field \"metadata\"\n\n    Scenario: Reject edge with invalid type\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"a\",\"target_id\":\"b\",\"type\":\"INVALID_TYPE\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"type\"\n\n    Scenario: Reject edge with nonexistent source\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"real-tgt\" exists but \"fake-src\" does not\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"fake-src\",\"target_id\":\"real-tgt\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"source\"\n\n    Scenario: Reject edge with nonexistent target\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"real-src\" exists but \"fake-tgt\" does not\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"real-src\",\"target_id\":\"fake-tgt\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"target\"\n\n    Scenario: Reject duplicate edge (same source, target, type)\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And an edge from \"dup-src\" to \"dup-tgt\" with type \"DEPENDS_ON\" already exists\n      When I send a POST request to \"/api/edges\" with the same edge\n      Then the response status is 409\n      And the response body has field \"error\" containing \"already exists\"\n\n    Scenario: Reject self-referencing edge\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"self-ref\" exists\n      When I send a POST request to \"/api/edges\" with body:\n        \"\"\"\n        {\"source_id\":\"self-ref\",\"target_id\":\"self-ref\",\"type\":\"DEPENDS_ON\"}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"self-referencing\"\n\n    Scenario: List all edges for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"hub-comp\" has 3 inbound and 2 outbound edges\n      When I send a GET request to \"/api/components/hub-comp/edges\"\n      Then the response status is 200\n      And the response body has field \"inbound\" as an array of 3 edges\n      And the response body has field \"outbound\" as an array of 2 edges\n\n    Scenario: Filter edges by type\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/edges?type=DEPENDS_ON\"\n      Then the response status is 200\n      And every edge in the response has type \"DEPENDS_ON\"\n\n    Scenario: List all edges in the graph\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/edges\"\n      Then the response status is 200\n      And the response body is a non-empty array of edge objects\n\n    Scenario: Delete an edge\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And an edge with id 42 exists\n      When I send a DELETE request to \"/api/edges/42\"\n      Then the response status is 204\n      And the edge no longer exists\n\n    Scenario: Delete nonexistent edge returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a DELETE request to \"/api/edges/99999\"\n      Then the response status is 404\n\n  # â”€â”€ Version Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Component versions can be managed via the API\n\n    Scenario: List versions for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"ver-list\" has versions \"overview\", \"mvp\", \"v1\", \"v2\"\n      When I send a GET request to \"/api/components/ver-list/versions\"\n      Then the response status is 200\n      And the response body is an array of 4 version objects\n      And each version has fields: version, content, progress, status, updated_at\n      And each phase version (mvp, v1, v2) includes step-based progress fields:\n        | field          | description                              |\n        | total_steps    | Total Given/When/Then steps for version  |\n        | passing_steps  | Steps in passing scenarios               |\n        | step_progress  | passing_steps / total_steps * 100        |\n\n    Scenario: Get a single version for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"ver-single\" has version \"mvp\" with progress 75\n      When I send a GET request to \"/api/components/ver-single/versions/mvp\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"mvp\"\n      And the response body has field \"progress\" with value \"75\"\n\n    Scenario: Create or update version content\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"ver-upsert\" exists\n      When I send a PUT request to \"/api/components/ver-upsert/versions/v1\" with body:\n        \"\"\"\n        {\n          \"content\": \"V1 adds Neo4j storage, secure API, and feature-driven progress tracking.\",\n          \"progress\": 0,\n          \"status\": \"planned\"\n        }\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"content\"\n\n    Scenario: Delete all versions for a component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"ver-del\" has versions \"overview\", \"mvp\", \"v1\"\n      When I send a DELETE request to \"/api/components/ver-del/versions\"\n      Then the response status is 204\n      And no versions exist for \"ver-del\"\n\n    Scenario: Version progress reflects step-based calculation\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"step-ver\" has version \"v1\" with 40 total steps and 30 passing\n      When I send a GET request to \"/api/components/step-ver/versions/v1\"\n      Then the response status is 200\n      And the response body has field \"total_steps\" with value \"40\"\n      And the response body has field \"passing_steps\" with value \"30\"\n      And the response body has field \"step_progress\" with value \"75\"\n      And the response body has field \"progress\" reflecting the combined weighted value\n\n  # â”€â”€ Bulk Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Bulk operations allow efficient batch mutations\n\n    Scenario: Bulk create multiple components\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/bulk/components\" with body:\n        \"\"\"\n        {\n          \"components\": [\n            {\"id\":\"bulk-1\",\"name\":\"Bulk One\",\"type\":\"component\",\"layer\":\"supervisor-layer\"},\n            {\"id\":\"bulk-2\",\"name\":\"Bulk Two\",\"type\":\"component\",\"layer\":\"supervisor-layer\"},\n            {\"id\":\"bulk-3\",\"name\":\"Bulk Three\",\"type\":\"store\",\"layer\":\"shared-state\"}\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"created\" with value 3\n      And the response body has field \"errors\" as an empty array\n\n    Scenario: Bulk create with partial failure\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"existing-bulk\" already exists\n      When I send a POST request to \"/api/bulk/components\" with body:\n        \"\"\"\n        {\n          \"components\": [\n            {\"id\":\"new-bulk\",\"name\":\"New\",\"type\":\"component\",\"layer\":\"supervisor-layer\"},\n            {\"id\":\"existing-bulk\",\"name\":\"Dup\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n          ]\n        }\n        \"\"\"\n      Then the response status is 207\n      And the response body has field \"created\" with value 1\n      And the response body has field \"errors\" as an array of 1 error\n      And the error references \"existing-bulk\" with status 409\n\n    Scenario: Bulk create edges\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"b-src-1\", \"b-src-2\", \"b-tgt\" exist\n      When I send a POST request to \"/api/bulk/edges\" with body:\n        \"\"\"\n        {\n          \"edges\": [\n            {\"source_id\":\"b-src-1\",\"target_id\":\"b-tgt\",\"type\":\"DEPENDS_ON\"},\n            {\"source_id\":\"b-src-2\",\"target_id\":\"b-tgt\",\"type\":\"DEPENDS_ON\"}\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"created\" with value 2\n\n    Scenario: Bulk operations are limited to 100 items\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/bulk/components\" with 101 components\n      Then the response status is 400\n      And the response body has field \"error\" containing \"maximum 100\"\n\n    Scenario: Bulk delete components\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"del-1\", \"del-2\", \"del-3\" exist\n      When I send a POST request to \"/api/bulk/delete/components\" with body:\n        \"\"\"\n        {\"ids\":[\"del-1\",\"del-2\",\"del-3\"]}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"deleted\" with value 3\n\n  # â”€â”€ Layer Management â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Layers can be managed alongside components\n\n    Scenario: List all layers\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/layers\"\n      Then the response status is 200\n      And the response body is an array of layer objects\n      And each layer has field \"type\" with value \"layer\"\n\n    Scenario: Get a layer with its children\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And layer \"supervisor-layer\" contains 4 components\n      When I send a GET request to \"/api/layers/supervisor-layer\"\n      Then the response status is 200\n      And the response body has field \"id\" with value \"supervisor-layer\"\n      And the response body has field \"children\" as an array of 4 components\n\n    Scenario: Create a new layer\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/layers\" with body:\n        \"\"\"\n        {\"id\":\"new-layer\",\"name\":\"New Layer\",\"color\":\"#E74C3C\",\"icon\":\"layers\"}\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"type\" with value \"layer\"\n\n    Scenario: Move a component to a different layer\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"movable-comp\" is in layer \"old-layer\"\n      When I send a PATCH request to \"/api/components/movable-comp\" with body:\n        \"\"\"\n        {\"layer\":\"new-layer\"}\n        \"\"\"\n      Then the response status is 200\n      And the CONTAINS edge from \"old-layer\" to \"movable-comp\" is removed\n      And a CONTAINS edge from \"new-layer\" to \"movable-comp\" is created\n\n  # â”€â”€ Architecture Graph Endpoint (Enhanced) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: The architecture endpoint returns the full enriched graph\n\n    Scenario: Get full architecture graph\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/architecture\"\n      Then the response status is 200\n      And the response body has field \"generated_at\" as an ISO 8601 timestamp\n      And the response body has field \"layers\" as a non-empty array\n      And the response body has field \"nodes\" as a non-empty array\n      And the response body has field \"edges\" as a non-empty array\n      And the response body has field \"progression_tree\"\n      And the response body has field \"stats\"\n\n    Scenario: Architecture stats are accurate\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/architecture\"\n      Then the stats field has \"total_nodes\" matching the actual node count\n      And the stats field has \"total_edges\" matching the actual edge count\n      And the stats field has \"total_versions\" matching the actual version count\n      And the stats field has \"total_features\" matching the actual feature count\n\n    Scenario: Enriched nodes include versions and features\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"enriched-comp\" has versions and features\n      When I send a GET request to \"/api/architecture\"\n      Then the node \"enriched-comp\" in the response has field \"versions\"\n      And the node \"enriched-comp\" has field \"features\"\n      And the node \"enriched-comp\" has field \"display_state\"\n\n    Scenario: Progression tree contains only app-type nodes\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/architecture\"\n      Then every node in the progression_tree has type \"app\"\n      And every edge in the progression_tree has type \"DEPENDS_ON\"\n\n    Scenario: Architecture response uses derived progress\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"derived-comp\" has current_version \"0.7.5\"\n      When I send a GET request to \"/api/architecture\"\n      Then the version \"mvp\" for node \"derived-comp\" has progress 75 in the response\n"
            },
            {
              "filename": "v1-api-feature-publishing.feature",
              "title": "API Feature File Publishing and Graph Traversal",
              "content": "@wip @v1\nFeature: API Feature File Publishing and Graph Traversal\n  As an LLM engineer working autonomously\n  I want API endpoints to publish, retrieve, and validate Gherkin feature files\n  with an explicit version parameter that categorises every feature\n  and traverse the architecture graph to understand component dependencies\n  So that I can headlessly manage version-tagged BDD specifications for each\n  component, and the system can calculate completion rates using total steps\n  versus passing steps per version tier\n\n  The MVP API supports basic feature upload with version derived from filename.\n  V1 makes version a mandatory part of the upload URL path, ensuring every\n  feature file is explicitly categorised under a version (mvp, v1, v2, etc.).\n  This enables precise step-level progress maths: the app counts total Given/\n  When/Then steps across all features for a version, compares them against\n  passing steps from test results, and derives a completion percentage.\n\n  V1 also adds Gherkin validation, batch publishing with required version\n  fields, version-scoped retrieval and deletion, and rich graph traversal\n  endpoints purpose-built for autonomous coding agents.\n\n  # â”€â”€ Version-Scoped Feature Upload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature uploads require an explicit version in the URL path\n\n    Scenario: Upload a feature file with explicit version in path\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"upload-comp\" exists\n      When I send a PUT request to \"/api/components/upload-comp/versions/v1/features/neo4j-storage.feature\" with body:\n        \"\"\"\n        Feature: Neo4j Storage\n          As a developer\n          I want data stored in Neo4j\n          So that graph traversals are efficient\n\n          Scenario: Save a node\n            Given an empty database\n            When I save a node with id \"test\"\n            Then the node exists in Neo4j\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"filename\" with value \"neo4j-storage.feature\"\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"title\" with value \"Neo4j Storage\"\n      And the response body has field \"node_id\" with value \"upload-comp\"\n      And the response body has field \"step_count\" with value \"3\"\n\n    Scenario: Upload a feature to the MVP version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"mvp-comp\" exists\n      When I send a PUT request to \"/api/components/mvp-comp/versions/mvp/features/basic-crud.feature\" with body:\n        \"\"\"\n        Feature: Basic CRUD\n          Scenario: Create a record\n            Given no records exist\n            When I create a record\n            Then 1 record exists\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"mvp\"\n      And the response body has field \"step_count\" with value \"3\"\n\n    Scenario: Upload a feature to the V2 version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"v2-comp\" exists\n      When I send a PUT request to \"/api/components/v2-comp/versions/v2/features/advanced-search.feature\" with body:\n        \"\"\"\n        Feature: Advanced Search\n          Scenario: Full-text search\n            Given indexed content exists\n            When I search for \"keyword\"\n            Then matching results are returned\n            And results are ranked by relevance\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v2\"\n      And the response body has field \"step_count\" with value \"4\"\n\n    Scenario: Version in path overrides any filename prefix\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"override-comp\" exists\n      When I send a PUT request to \"/api/components/override-comp/versions/v1/features/mvp-legacy-name.feature\" with body:\n        \"\"\"\n        Feature: Legacy Named Feature\n          Scenario: A scenario\n            Given a step\n            When an action\n            Then a result\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v1\"\n      And the feature is stored under version \"v1\" regardless of the \"mvp-\" filename prefix\n\n    Scenario: Reject upload with invalid version value\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-ver\" exists\n      When I send a PUT request to \"/api/components/bad-ver/versions/invalid/features/test.feature\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version\"\n      And the response body has field \"error\" containing \"mvp, v1, v2\"\n\n    Scenario: Reject upload without version in path (old MVP-style URL)\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request to \"/api/components/some-comp/features/test.feature\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version is required\"\n\n    Scenario: Upload extracts title from Feature: line\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"title-comp\" exists\n      When I send a PUT request to \"/api/components/title-comp/versions/v1/features/my-feature.feature\" with body:\n        \"\"\"\n        Feature: My Custom Title Here\n          Scenario: Something\n            Given a step\n            When an action\n            Then a result\n        \"\"\"\n      Then the response body has field \"title\" with value \"My Custom Title Here\"\n\n    Scenario: Upload to nonexistent component returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request to \"/api/components/ghost/versions/v1/features/test.feature\" with body:\n        \"\"\"\n        Feature: Ghost Upload\n          Scenario: Test\n            Given a step\n        \"\"\"\n      Then the response status is 404\n      And the response body has field \"error\" containing \"not found\"\n\n    Scenario: Upload replaces existing feature with same filename and version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"replace-comp\" has feature \"existing.feature\" under version \"v1\" with title \"Old\"\n      When I send a PUT request to \"/api/components/replace-comp/versions/v1/features/existing.feature\" with body:\n        \"\"\"\n        Feature: New Title\n          Scenario: Updated scenario\n            Given a new step\n            When a new action\n            Then a new result\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"title\" with value \"New Title\"\n      And only one feature with filename \"existing.feature\" exists for \"replace-comp\" version \"v1\"\n\n    Scenario: Same filename under different versions creates separate records\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"multi-ver\" exists\n      When I upload \"auth.feature\" to \"multi-ver\" under version \"mvp\" with 3 steps\n      And I upload \"auth.feature\" to \"multi-ver\" under version \"v1\" with 8 steps\n      Then 2 feature records exist for \"multi-ver\" with filename \"auth.feature\"\n      And the \"mvp\" version has step_count 3\n      And the \"v1\" version has step_count 8\n\n    Scenario: Upload preserves features under other versions\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"preserve-comp\" has features under versions \"mvp\" and \"v1\"\n      When I upload a new feature under version \"v2\"\n      Then the \"mvp\" and \"v1\" features are unchanged\n      And \"preserve-comp\" now has features across 3 versions\n\n    Scenario: Response includes step count breakdown\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"step-count-comp\" exists\n      When I send a PUT request to \"/api/components/step-count-comp/versions/v1/features/detailed.feature\" with body:\n        \"\"\"\n        Feature: Detailed Steps\n          Scenario: First scenario\n            Given step one\n            And step two\n            When action one\n            Then result one\n            And result two\n            But not result three\n\n          Scenario: Second scenario\n            Given step three\n            When action two\n            Then result four\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"step_count\" with value \"9\"\n      And the response body has field \"scenario_count\" with value \"2\"\n      And the response body has field \"given_count\" with value \"3\"\n      And the response body has field \"when_count\" with value \"2\"\n      And the response body has field \"then_count\" with value \"4\"\n\n  # â”€â”€ Feature File Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Uploaded feature files are validated for Gherkin syntax\n\n    Scenario: Valid Gherkin is accepted\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"valid-gherkin\" exists\n      When I send a PUT request to \"/api/components/valid-gherkin/versions/v1/features/valid.feature\" with valid Gherkin content\n      Then the response status is 200\n\n    Scenario: Feature file without Feature: keyword is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-gherkin\" exists\n      When I send a PUT request to \"/api/components/bad-gherkin/versions/v1/features/bad.feature\" with body:\n        \"\"\"\n        This is not a valid feature file.\n        It has no Feature: keyword.\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"Feature\"\n\n    Scenario: Feature file without any scenarios is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"no-scenario\" exists\n      When I send a PUT request to \"/api/components/no-scenario/versions/v1/features/empty.feature\" with body:\n        \"\"\"\n        Feature: Empty Feature\n          This feature has a description but no scenarios.\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"scenario\"\n\n    Scenario: Feature file with empty body is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"empty-body\" exists\n      When I send a PUT request to \"/api/components/empty-body/versions/v1/features/empty.feature\" with empty body\n      Then the response status is 400\n      And the response body has field \"error\" containing \"empty\"\n\n    Scenario: Feature file with scenarios but no steps is rejected\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"no-steps\" exists\n      When I send a PUT request to \"/api/components/no-steps/versions/v1/features/stepless.feature\" with body:\n        \"\"\"\n        Feature: Stepless Feature\n          Scenario: Empty scenario\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"steps\"\n\n    Scenario: Feature filename must end with .feature\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-ext\" exists\n      When I send a PUT request to \"/api/components/bad-ext/versions/v1/features/test.txt\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \".feature\"\n\n    Scenario: Feature filename must be kebab-case\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"bad-name\" exists\n      When I send a PUT request to \"/api/components/bad-name/versions/v1/features/under_score.feature\" with Gherkin content\n      Then the response status is 400\n      And the response body has field \"error\" containing \"filename\"\n\n    Scenario: Validation response includes line number for parse errors\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"parse-err\" exists\n      When I send a PUT request to \"/api/components/parse-err/versions/v1/features/broken.feature\" with Gherkin containing a syntax error at line 5\n      Then the response status is 400\n      And the response body has field \"error\" containing line number information\n\n  # â”€â”€ Version-Scoped Feature Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files are retrieved scoped to their explicit version\n\n    Scenario: List all features for a component across all versions\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-list\" has 2 features under \"mvp\", 2 under \"v1\", 1 under \"v2\"\n      When I send a GET request to \"/api/components/feat-list/features\"\n      Then the response status is 200\n      And the response body is an array of 5 feature objects\n      And each object has fields: filename, version, title, content, step_count, updated_at\n\n    Scenario: List features for a specific version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-ver\" has 3 \"mvp\" features and 4 \"v1\" features\n      When I send a GET request to \"/api/components/feat-ver/versions/v1/features\"\n      Then the response status is 200\n      And the response body is an array of 4 features\n      And every feature has version \"v1\"\n\n    Scenario: Get a single feature by version and filename\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-single\" has feature \"auth.feature\" under version \"v1\"\n      When I send a GET request to \"/api/components/feat-single/versions/v1/features/auth.feature\"\n      Then the response status is 200\n      And the response body has field \"filename\" with value \"auth.feature\"\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"content\" containing the full Gherkin text\n      And the response body has field \"step_count\"\n\n    Scenario: Get feature from wrong version returns 404\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"ver-miss\" has feature \"auth.feature\" under version \"mvp\" only\n      When I send a GET request to \"/api/components/ver-miss/versions/v1/features/auth.feature\"\n      Then the response status is 404\n      And the response body has field \"error\" containing \"not found\"\n\n    Scenario: Get nonexistent feature returns 404\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"feat-missing\" exists\n      When I send a GET request to \"/api/components/feat-missing/versions/v1/features/ghost.feature\"\n      Then the response status is 404\n\n    Scenario: Get features for nonexistent component returns 404\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components/nonexistent/versions/v1/features\"\n      Then the response status is 404\n\n    Scenario: Get raw feature content as plain text\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"raw-feat\" has feature \"raw.feature\" under version \"v1\"\n      When I send a GET request to \"/api/components/raw-feat/versions/v1/features/raw.feature\" with header \"Accept: text/plain\"\n      Then the response status is 200\n      And the response content type is \"text/plain\"\n      And the response body is the raw Gherkin text\n\n    Scenario: Feature listing includes step counts for progress maths\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"step-list\" has 3 features under version \"v1\"\n      When I send a GET request to \"/api/components/step-list/versions/v1/features\"\n      Then each feature object has field \"step_count\" as a positive integer\n      And each feature object has field \"scenario_count\" as a positive integer\n      And the response includes a \"totals\" field with:\n        | field               | description                         |\n        | total_features      | Number of features in this version  |\n        | total_scenarios     | Sum of scenarios across features    |\n        | total_steps         | Sum of all steps across features    |\n        | total_given_steps   | Sum of Given/And steps              |\n        | total_when_steps    | Sum of When steps                   |\n        | total_then_steps    | Sum of Then/But steps               |\n\n  # â”€â”€ Version-Scoped Feature Deletion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files can be deleted scoped to their version\n\n    Scenario: Delete a single feature by version and filename\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"del-feat\" has feature \"remove-me.feature\" under version \"v1\"\n      When I send a DELETE request to \"/api/components/del-feat/versions/v1/features/remove-me.feature\"\n      Then the response status is 204\n      And the feature \"remove-me.feature\" under version \"v1\" no longer exists for \"del-feat\"\n\n    Scenario: Delete all features for a specific version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"del-ver\" has 3 \"mvp\" and 2 \"v1\" features\n      When I send a DELETE request to \"/api/components/del-ver/versions/v1/features\"\n      Then the response status is 204\n      And 3 \"mvp\" features still exist for \"del-ver\"\n      And 0 \"v1\" features exist for \"del-ver\"\n\n    Scenario: Delete all features across all versions\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"del-all\" has features under \"mvp\", \"v1\", and \"v2\"\n      When I send a DELETE request to \"/api/components/del-all/features\"\n      Then the response status is 204\n      And no features exist for \"del-all\"\n\n    Scenario: Delete nonexistent feature returns 404\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a DELETE request to \"/api/components/del-feat/versions/v1/features/ghost.feature\"\n      Then the response status is 404\n\n    Scenario: Deleting features triggers progress recalculation\n      Given component \"del-recalc\" has features under version \"v1\" contributing to progress\n      When I delete all features for \"del-recalc\" version \"v1\"\n      Then the step-based progress for \"del-recalc\" version \"v1\" drops to 0 percent\n\n  # â”€â”€ Batch Feature Publishing with Explicit Version â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Batch uploads require an explicit version per feature entry\n\n    Scenario: Batch upload features for a single component and version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"batch-comp\" exists\n      When I send a POST request to \"/api/components/batch-comp/versions/v1/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"filename\": \"first.feature\",\n              \"content\": \"Feature: First\\n  Scenario: S1\\n    Given a step\\n    When an action\\n    Then a result\"\n            },\n            {\n              \"filename\": \"second.feature\",\n              \"content\": \"Feature: Second\\n  Scenario: S2\\n    Given another step\\n    Then another result\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And the response body has field \"uploaded\" with value 2\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"total_steps\" with value \"5\"\n      And the response body has field \"errors\" as an empty array\n\n    Scenario: Batch upload with partial validation failure\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And a component \"batch-partial\" exists\n      When I send a POST request to \"/api/components/batch-partial/versions/v1/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"filename\": \"valid.feature\",\n              \"content\": \"Feature: Valid\\n  Scenario: S1\\n    Given a step\"\n            },\n            {\n              \"filename\": \"invalid.feature\",\n              \"content\": \"This is not valid Gherkin\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 207\n      And the response body has field \"uploaded\" with value 1\n      And the response body has field \"errors\" as an array of 1 error\n      And the error references \"invalid.feature\"\n\n    Scenario: Batch upload limited to 50 features\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a batch upload with 51 features\n      Then the response status is 400\n      And the response body has field \"error\" containing \"maximum 50\"\n\n    Scenario: Cross-component batch publish requires version per entry\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And components \"cross-1\" and \"cross-2\" exist\n      When I send a POST request to \"/api/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"node_id\": \"cross-1\",\n              \"version\": \"v1\",\n              \"filename\": \"a.feature\",\n              \"content\": \"Feature: A\\n  Scenario: S\\n    Given a step\"\n            },\n            {\n              \"node_id\": \"cross-2\",\n              \"version\": \"v2\",\n              \"filename\": \"b.feature\",\n              \"content\": \"Feature: B\\n  Scenario: S\\n    Given a step\\n    Then a result\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 201\n      And \"cross-1\" has feature \"a.feature\" under version \"v1\"\n      And \"cross-2\" has feature \"b.feature\" under version \"v2\"\n\n    Scenario: Cross-component batch rejects entry without version field\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/features/batch\" with body:\n        \"\"\"\n        {\n          \"features\": [\n            {\n              \"node_id\": \"some-comp\",\n              \"filename\": \"no-version.feature\",\n              \"content\": \"Feature: Missing Version\\n  Scenario: S\\n    Given a step\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"version is required\"\n\n    Scenario: Batch upload triggers progress recalculation once\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"batch-recalc\" exists\n      When I batch upload 5 features to \"batch-recalc\" version \"v1\"\n      Then progress recalculation happens once (not 5 times)\n      And the step-based progress reflects all 5 features\n\n  # â”€â”€ Graph Traversal for Autonomous Coding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: LLM engineers can traverse the graph to plan implementation\n\n    Scenario: Get dependency tree for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"dep-root\" has dependencies \"dep-a\" and \"dep-b\"\n      And \"dep-a\" has dependency \"dep-c\"\n      When I send a GET request to \"/api/components/dep-root/dependencies?depth=2\"\n      Then the response status is 200\n      And the response body has field \"dependencies\" as a tree structure\n      And the tree includes \"dep-a\", \"dep-b\" at depth 1\n      And the tree includes \"dep-c\" at depth 2\n\n    Scenario: Get reverse dependencies (dependents)\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components \"consumer-1\" and \"consumer-2\" depend on \"provider\"\n      When I send a GET request to \"/api/components/provider/dependents\"\n      Then the response status is 200\n      And the response body contains \"consumer-1\" and \"consumer-2\"\n\n    Scenario: Get full component context for coding\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"context-comp\" exists with versions, features, and edges\n      When I send a GET request to \"/api/components/context-comp/context\"\n      Then the response status is 200\n      And the response body has field \"component\" with full component details\n      And the response body has field \"versions\" with all version data including step counts\n      And the response body has field \"features\" grouped by version with step counts\n      And the response body has field \"dependencies\" with outbound DEPENDS_ON edges\n      And the response body has field \"dependents\" with inbound DEPENDS_ON edges\n      And the response body has field \"layer\" with the parent layer details\n      And the response body has field \"siblings\" listing other components in the same layer\n      And the response body has field \"progress\" with per-version step-based progress\n\n    Scenario: Get implementation order via topological sort\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a dependency graph with no cycles\n      When I send a GET request to \"/api/graph/implementation-order\"\n      Then the response status is 200\n      And the response body is an array of component IDs\n      And every component appears after all its dependencies\n      And the order is a valid topological sort\n\n    Scenario: Implementation order detects cycles\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a circular dependency exists between \"cycle-a\", \"cycle-b\", \"cycle-c\"\n      When I send a GET request to \"/api/graph/implementation-order\"\n      Then the response status is 409\n      And the response body has field \"error\" containing \"cycle\"\n      And the response body has field \"cycle\" listing the involved components\n\n    Scenario: Get components by completion status\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/graph/components-by-status?version=mvp\"\n      Then the response status is 200\n      And the response body has field \"complete\" as an array of components with 100% step coverage\n      And the response body has field \"in_progress\" as an array with partial step coverage\n      And the response body has field \"planned\" as an array with 0% step coverage\n\n    Scenario: Get next implementable components\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/graph/next-implementable?version=mvp\"\n      Then the response status is 200\n      And the response body is an array of component objects\n      And every component has all its dependencies at 100% step coverage for \"mvp\"\n      And every component itself has step coverage below 100% for \"mvp\"\n\n    Scenario: Get shortest path between two components\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components \"path-start\" and \"path-end\" are connected via intermediate nodes\n      When I send a GET request to \"/api/graph/path?from=path-start&to=path-end\"\n      Then the response status is 200\n      And the response body has field \"path\" as an array of nodes\n      And the response body has field \"edges\" describing each hop\n      And the path is the shortest available route\n\n    Scenario: Path between unconnected components returns empty\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components \"island-1\" and \"island-2\" have no connecting path\n      When I send a GET request to \"/api/graph/path?from=island-1&to=island-2\"\n      Then the response status is 200\n      And the response body has field \"path\" as an empty array\n\n    Scenario: Get component neighbourhood\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"center\" has edges to and from multiple components\n      When I send a GET request to \"/api/components/center/neighbourhood?hops=2\"\n      Then the response status is 200\n      And the response body has field \"nodes\" with all components within 2 hops\n      And the response body has field \"edges\" with all edges between those nodes\n      And the response includes the edge types and directions\n\n    Scenario: Get layer overview for planning\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/graph/layer-overview\"\n      Then the response status is 200\n      And the response body is an array of layer summaries\n      And each summary has field \"layer_id\"\n      And each summary has field \"total_components\"\n      And each summary has field \"completed_mvp\" as a count\n      And each summary has field \"completed_v1\" as a count\n      And each summary has field \"overall_progress\" as a percentage\n\n  # â”€â”€ Feature Seeding from Filesystem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files seeded from the filesystem use filename prefix for version\n\n    Scenario: Trigger feature re-seed via API\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And feature files exist in the \"components/\" directory tree\n      When I send a POST request to \"/api/admin/seed-features\"\n      Then the response status is 200\n      And the response body has field \"seeded\" with a positive integer\n      And the response body has field \"skipped\" with an integer\n      And features with \"mvp-\" prefix are stored under version \"mvp\"\n      And features with \"v1-\" prefix are stored under version \"v1\"\n      And features with \"v2-\" prefix are stored under version \"v2\"\n      And features without a version prefix default to version \"mvp\"\n\n    Scenario: Re-seed is idempotent\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I trigger feature re-seed twice\n      Then the second run produces the same result as the first\n      And no duplicate features exist in the database\n\n    Scenario: Re-seed clears stale features\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And the database has a feature for a deleted file\n      When I trigger feature re-seed\n      Then the stale feature is removed from the database\n      And only features matching filesystem files remain\n\n    Scenario: Seed requires admin scope\n      Given the API server is running\n      And a valid API key with scope \"write\" but not \"admin\"\n      When I send a POST request to \"/api/admin/seed-features\"\n      Then the response status is 403\n\n    Scenario: Seed reports step counts per version\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/seed-features\"\n      Then the response body has field \"step_totals\" as an object with:\n        | version | total_steps | total_scenarios |\n        | mvp     | (integer)   | (integer)       |\n        | v1      | (integer)   | (integer)       |\n        | v2      | (integer)   | (integer)       |\n\n  # â”€â”€ Feature File Export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature files can be exported back to the filesystem\n\n    Scenario: Export all features to the filesystem\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And features exist in the database for components \"comp-a\" and \"comp-b\"\n      When I send a POST request to \"/api/admin/export-features\"\n      Then the response status is 200\n      And feature files are written to \"components/comp-a/features/\"\n      And feature files are written to \"components/comp-b/features/\"\n      And each file contains the Gherkin content from the database\n\n    Scenario: Export creates component directories if missing\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And a feature exists for component \"new-comp\" but no directory exists\n      When I trigger feature export\n      Then the directory \"components/new-comp/features/\" is created\n      And the feature file is written there\n\n    Scenario: Export for a single component\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/export-features?component=specific-comp\"\n      Then the response status is 200\n      And only features for \"specific-comp\" are exported to the filesystem\n\n  # â”€â”€ Feature Content Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Feature content can be searched across all components\n\n    Scenario: Search features by keyword\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And features exist containing the word \"authentication\"\n      When I send a GET request to \"/api/features/search?q=authentication\"\n      Then the response status is 200\n      And the response body is an array of matching features\n      And each result has fields: node_id, filename, version, title, step_count, snippet\n\n    Scenario: Search with no results returns empty array\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/features/search?q=xyznonexistent\"\n      Then the response status is 200\n      And the response body is an empty array\n\n    Scenario: Search is case-insensitive\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a feature contains \"Neo4j\"\n      When I send a GET request to \"/api/features/search?q=neo4j\"\n      Then the response body contains the matching feature\n\n    Scenario: Search can be scoped to a version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/features/search?q=test&version=v1\"\n      Then every result in the response has version \"v1\"\n\n    Scenario: Search returns snippet with highlighted match\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And a feature contains \"rate limiting\"\n      When I send a GET request to \"/api/features/search?q=rate+limiting\"\n      Then each result has a \"snippet\" field showing context around the match\n"
            },
            {
              "filename": "v1-feature-driven-progress.feature",
              "title": "Feature-Driven Progress Tracking",
              "content": "@wip @v1\nFeature: Feature-Driven Progress Tracking\n  As the roadmap application\n  I want to read version-tagged feature files (mvp, v1, v2) and calculate\n  component completion progress using step-level maths\n  So that progress is objective, automatable, and reflects actual BDD\n  specification coverage at the granularity of individual Given/When/Then steps\n\n  The MVP derives progress from semver current_version numbers. V1 adds a\n  step-based progress system that works with explicitly version-tagged feature\n  files. Every feature file is categorised under a version (via the upload API\n  or filesystem prefix). The system counts total steps (Given, When, Then, And,\n  But) across all features for a version, then compares against passing steps\n  from test results to derive a completion percentage.\n\n  Step-based progress formula:\n    completion% = (passing_steps / total_steps) * 100\n\n  Where:\n    - total_steps = count of all Given/When/Then/And/But lines across all\n      feature files tagged with that version for that component\n    - passing_steps = count of steps in scenarios that passed in the most\n      recent test run for that version\n\n  A scenario's steps only count as passing if the entire scenario passed.\n  Partially passing scenarios contribute 0 passing steps (fail-fast semantics).\n\n  # â”€â”€ Step Counting from Feature Files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: The system counts individual steps from Gherkin feature content\n\n    Scenario: Count steps in a simple feature file\n      Given a feature file with content:\n        \"\"\"\n        Feature: Simple Feature\n          Scenario: Basic flow\n            Given a user exists\n            When the user logs in\n            Then the dashboard is displayed\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n      And the given step count is 1\n      And the when step count is 1\n      And the then step count is 1\n\n    Scenario: And/But steps are counted as steps\n      Given a feature file with content:\n        \"\"\"\n        Feature: And/But Steps\n          Scenario: Complex assertions\n            Given a user exists\n            And the user has admin role\n            When the user accesses settings\n            Then the settings page loads\n            And the admin panel is visible\n            But the delete button is hidden\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 6\n\n    Scenario: Count steps across multiple scenarios\n      Given a feature file with content:\n        \"\"\"\n        Feature: Multi-Scenario\n          Scenario: First\n            Given step one\n            When step two\n            Then step three\n\n          Scenario: Second\n            Given step four\n            When step five\n            Then step six\n            And step seven\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 7\n\n    Scenario: Count steps in Scenario Outline (template counts once)\n      Given a feature file with content:\n        \"\"\"\n        Feature: Outline Feature\n          Scenario Outline: Parameterised\n            Given a <role> user\n            When the user performs <action>\n            Then the result is <outcome>\n\n            Examples:\n              | role  | action | outcome |\n              | admin | edit   | success |\n              | guest | edit   | denied  |\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n      And the step count reflects the template, not the expanded examples\n\n    Scenario: Count steps within Rule blocks\n      Given a feature file with content:\n        \"\"\"\n        Feature: Rules Feature\n          Rule: Authentication\n            Scenario: Login\n              Given credentials\n              When I submit them\n              Then I am logged in\n\n          Rule: Authorisation\n            Scenario: Access check\n              Given I am logged in\n              And I have role \"admin\"\n              When I access the resource\n              Then access is granted\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 7\n\n    Scenario: Background steps are counted once per feature\n      Given a feature file with content:\n        \"\"\"\n        Feature: Background Feature\n          Background:\n            Given a database connection\n            And the schema is initialised\n\n          Scenario: Read data\n            When I query the database\n            Then results are returned\n\n          Scenario: Write data\n            When I insert a record\n            Then the record exists\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 6\n      And the background steps count as 2 (not multiplied by scenarios)\n\n    Scenario: Steps with docstrings count as one step each\n      Given a feature file with content:\n        \"\"\"\n        Feature: Docstring Feature\n          Scenario: Upload content\n            Given the API is running\n            When I upload with body:\n              \\\"\\\"\\\"\n              {\"key\": \"value\"}\n              \\\"\\\"\\\"\n            Then the response status is 200\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n\n    Scenario: Steps with data tables count as one step each\n      Given a feature file with content:\n        \"\"\"\n        Feature: Table Feature\n          Scenario: Tabular data\n            Given these users exist:\n              | name  | role  |\n              | Alice | admin |\n              | Bob   | user  |\n            When I list users\n            Then I see 2 users\n        \"\"\"\n      When the step counter processes the file\n      Then the total step count is 3\n\n    Scenario: Empty feature file has 0 steps\n      Given a feature file with no scenarios\n      When the step counter processes the file\n      Then the total step count is 0\n\n  # â”€â”€ Aggregated Step Counts Per Component Per Version â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Step counts are aggregated across all features for a component version\n\n    Scenario: Aggregate steps across multiple features for a version\n      Given component \"agg-comp\" has these features under version \"v1\":\n        | filename              | step_count |\n        | auth.feature          | 12         |\n        | permissions.feature   | 8          |\n        | rate-limiting.feature | 15         |\n      When I query the step totals for \"agg-comp\" version \"v1\"\n      Then the total steps are 35\n      And the feature count is 3\n\n    Scenario: Step counts are independent per version\n      Given component \"ver-steps\" has:\n        | version | total_steps |\n        | mvp     | 20          |\n        | v1      | 45          |\n        | v2      | 30          |\n      When I query the step totals for each version\n      Then the \"mvp\" total is 20 steps\n      And the \"v1\" total is 45 steps\n      And the \"v2\" total is 30 steps\n\n    Scenario: Component with no features for a version has 0 total steps\n      Given component \"no-feat\" has features under \"mvp\" but none under \"v1\"\n      When I query the step totals for \"no-feat\" version \"v1\"\n      Then the total steps are 0\n      And the feature count is 0\n\n    Scenario: Adding a feature updates the aggregated step count\n      Given component \"add-feat\" has 20 total steps under version \"v1\"\n      When a new feature with 8 steps is uploaded under version \"v1\"\n      Then the total steps for \"add-feat\" version \"v1\" become 28\n\n    Scenario: Removing a feature updates the aggregated step count\n      Given component \"rm-feat\" has 30 total steps under version \"v1\" across 3 features\n      When a feature with 10 steps is deleted from version \"v1\"\n      Then the total steps for \"rm-feat\" version \"v1\" become 20\n\n  # â”€â”€ Step-Based Progress Calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Progress is calculated as passing_steps / total_steps * 100\n\n    Scenario: 100% progress when all steps pass\n      Given component \"full-pass\" has 20 total steps under version \"mvp\"\n      And test results show 20 of 20 steps passing\n      When I calculate step-based progress for \"full-pass\" version \"mvp\"\n      Then the progress is 100 percent\n      And the status is \"complete\"\n\n    Scenario: 0% progress when no test results exist\n      Given component \"no-tests\" has 15 total steps under version \"v1\"\n      And no test results exist for \"no-tests\" version \"v1\"\n      When I calculate step-based progress for \"no-tests\" version \"v1\"\n      Then the progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: Partial progress with some scenarios passing\n      Given component \"partial\" has 40 total steps under version \"v1\"\n      And test results show scenarios containing 30 steps passed\n      And scenarios containing 10 steps failed\n      When I calculate step-based progress for \"partial\" version \"v1\"\n      Then the progress is 75 percent\n      And the status is \"in-progress\"\n\n    Scenario: Failed scenario contributes 0 passing steps\n      Given component \"fail-scenario\" has a feature with 2 scenarios:\n        | scenario   | steps | passed |\n        | Scenario A | 5     | yes    |\n        | Scenario B | 5     | no     |\n      When I calculate step-based progress for \"fail-scenario\"\n      Then the passing steps are 5 (only from Scenario A)\n      And the total steps are 10\n      And the progress is 50 percent\n\n    Scenario: 0% progress when all scenarios fail\n      Given component \"all-fail\" has 25 total steps under version \"mvp\"\n      And test results show 0 scenarios passing\n      When I calculate step-based progress for \"all-fail\" version \"mvp\"\n      Then the progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: 0% progress when component has 0 total steps\n      Given component \"empty-comp\" has 0 total steps under version \"v1\"\n      When I calculate step-based progress for \"empty-comp\" version \"v1\"\n      Then the progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: Progress rounds to nearest integer\n      Given component \"round-comp\" has 3 total steps under version \"mvp\"\n      And test results show 2 steps passing\n      When I calculate step-based progress for \"round-comp\" version \"mvp\"\n      Then the progress is 67 percent (rounded from 66.67)\n\n    Scenario: Progress is capped at 100\n      Given step counts and results that could produce over 100\n      When I calculate step-based progress\n      Then the progress is exactly 100\n      And the status is \"complete\"\n\n  # â”€â”€ Recording Step-Level Test Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Test results record per-scenario pass/fail and step counts\n\n    Scenario: Record test results with step counts\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"results-comp\" has features under version \"mvp\" with 15 total steps\n      When I send a POST request to \"/api/components/results-comp/versions/mvp/test-results\" with body:\n        \"\"\"\n        {\n          \"results\": [\n            {\n              \"scenario\": \"Create a node\",\n              \"feature\": \"crud.feature\",\n              \"status\": \"passed\",\n              \"steps\": 5,\n              \"duration_ms\": 120\n            },\n            {\n              \"scenario\": \"Delete a node\",\n              \"feature\": \"crud.feature\",\n              \"status\": \"passed\",\n              \"steps\": 4,\n              \"duration_ms\": 85\n            },\n            {\n              \"scenario\": \"Handle invalid input\",\n              \"feature\": \"validation.feature\",\n              \"status\": \"failed\",\n              \"steps\": 6,\n              \"duration_ms\": 200,\n              \"error\": \"Expected 400, got 500\"\n            }\n          ]\n        }\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"recorded\" with value 3\n      And the response body has field \"total_steps\" with value 15\n      And the response body has field \"passing_steps\" with value 9\n      And the response body has field \"failing_steps\" with value 6\n      And the response body has field \"progress_percent\" with value 60\n\n    Scenario: Test results automatically update step-based progress\n      Given component \"auto-prog\" has 20 total steps under version \"v1\"\n      And current progress for \"auto-prog\" version \"v1\" is 0 percent\n      When I record test results with 15 passing steps\n      Then the progress for \"auto-prog\" version \"v1\" updates to 75 percent\n      And the status updates to \"in-progress\"\n\n    Scenario: New test results overwrite previous results\n      Given component \"overwrite\" has previous test results showing 50% progress\n      When I submit new test results showing 80% progress\n      Then the stored results reflect the new submission\n      And the progress is recalculated to 80 percent\n\n    Scenario: Query test results for a component version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"query-comp\" has recorded test results for version \"v1\"\n      When I send a GET request to \"/api/components/query-comp/versions/v1/test-results\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"v1\"\n      And the response body has field \"total_steps\" as a positive integer\n      And the response body has field \"passing_steps\" as an integer\n      And the response body has field \"failing_steps\" as an integer\n      And the response body has field \"progress_percent\" as a number 0-100\n      And the response body has field \"last_run\" as an ISO 8601 timestamp\n      And the response body has field \"scenarios\" as an array of per-scenario results\n\n    Scenario: Per-scenario result includes step count\n      Given recorded test results for component \"detail-comp\" version \"mvp\"\n      When I query the test results\n      Then each scenario entry has fields:\n        | field       | type     | description                          |\n        | scenario    | string   | Scenario name                        |\n        | feature     | string   | Feature filename                     |\n        | status      | string   | \"passed\" or \"failed\"                 |\n        | steps       | number   | Number of steps in this scenario     |\n        | duration_ms | number   | Execution time in milliseconds       |\n        | error       | string?  | Error message if failed              |\n        | last_run    | string   | ISO 8601 timestamp                   |\n\n    Scenario: Test results can be submitted from CI pipeline (Cucumber JSON)\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components/ci-comp/versions/mvp/test-results/cucumber\" with Cucumber JSON output\n      Then the results are parsed extracting scenario names, step counts, and pass/fail\n      And the step-based progress is recalculated\n\n    Scenario: Test results rejected for nonexistent component\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components/ghost/versions/v1/test-results\" with results\n      Then the response status is 404\n\n    Scenario: Test results rejected for nonexistent version\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"real-comp\" exists but has no features under version \"v2\"\n      When I send a POST request to \"/api/components/real-comp/versions/v2/test-results\" with results\n      Then the response status is 400\n      And the response body has field \"error\" containing \"no features\"\n\n  # â”€â”€ Combined Progress (Semver + Steps) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Final progress blends semver-derived and step-based sources\n\n    Scenario: Combined progress with configurable weights\n      Given the progress configuration has weights:\n        | source        | weight |\n        | semver        | 0.3    |\n        | step_coverage | 0.7    |\n      And component \"weighted\" has:\n        | source                | value |\n        | semver progress       | 50    |\n        | step-based progress   | 80    |\n      When I calculate combined progress for \"weighted\" version \"mvp\"\n      Then the combined progress is 71 percent\n      And the calculation is round((50 * 0.3) + (80 * 0.7)) = 71\n\n    Scenario: Default weights are 50/50\n      Given no custom progress configuration exists\n      And component \"default\" has:\n        | source                | value |\n        | semver progress       | 60    |\n        | step-based progress   | 40    |\n      When I calculate combined progress for \"default\" version \"mvp\"\n      Then the combined progress is 50 percent\n\n    Scenario: No features falls back to semver-only progress\n      Given component \"no-feat\" has current_version \"0.7.0\"\n      And no feature files exist under any version\n      When I calculate combined progress for \"no-feat\" version \"mvp\"\n      Then the combined progress is 70 percent\n      And the progress source is \"semver_only\"\n\n    Scenario: No current_version falls back to step-based-only progress\n      Given component \"no-ver\" has no current_version\n      And step-based progress for \"no-ver\" version \"mvp\" is 60 percent\n      When I calculate combined progress for \"no-ver\" version \"mvp\"\n      Then the combined progress is 60 percent\n      And the progress source is \"step_coverage_only\"\n\n    Scenario: Neither source available gives 0% progress\n      Given component \"empty\" has no current_version and no features\n      When I calculate combined progress for \"empty\" version \"mvp\"\n      Then the combined progress is 0 percent\n      And the status is \"planned\"\n\n    Scenario: Status derived from combined progress\n      Then the following status derivation applies:\n        | combined_progress | status      |\n        | 0                 | planned     |\n        | 1-99              | in-progress |\n        | 100               | complete    |\n\n  # â”€â”€ Progress Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Progress blend weights are configurable per component\n\n    Scenario: Set custom weights\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      And component \"custom-w\" exists\n      When I send a PUT request to \"/api/components/custom-w/progress-config\" with body:\n        \"\"\"\n        {\"semver_weight\": 0.2, \"step_weight\": 0.8}\n        \"\"\"\n      Then the response status is 200\n      And the response body has field \"semver_weight\" with value \"0.2\"\n      And the response body has field \"step_weight\" with value \"0.8\"\n\n    Scenario: Weights must sum to 1.0\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request to \"/api/components/bad-w/progress-config\" with body:\n        \"\"\"\n        {\"semver_weight\": 0.5, \"step_weight\": 0.6}\n        \"\"\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"sum to 1.0\"\n\n    Scenario: Weights must be between 0 and 1\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a PUT request with a negative weight\n      Then the response status is 400\n      And the response body has field \"error\" containing \"between 0 and 1\"\n\n    Scenario: Get current weights for a component\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"config-comp\" has custom weights\n      When I send a GET request to \"/api/components/config-comp/progress-config\"\n      Then the response status is 200\n      And the response body has field \"semver_weight\"\n      And the response body has field \"step_weight\"\n\n    Scenario: Default weights returned when no custom config\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"default-c\" has no custom weights\n      When I send a GET request to \"/api/components/default-c/progress-config\"\n      Then the response status is 200\n      And the response body has field \"semver_weight\" with value \"0.5\"\n      And the response body has field \"step_weight\" with value \"0.5\"\n\n  # â”€â”€ Progress Dashboard Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: The API provides aggregated step-based progress data\n\n    Scenario: Get progress summary for all components at a version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/summary?version=mvp\"\n      Then the response status is 200\n      And the response body has field \"version\" with value \"mvp\"\n      And the response body has field \"total_components\" as a positive integer\n      And the response body has field \"average_progress\" as a number 0-100\n      And the response body has field \"complete_count\" as an integer\n      And the response body has field \"in_progress_count\" as an integer\n      And the response body has field \"planned_count\" as an integer\n      And the response body has field \"aggregate_steps\" with:\n        | field          | description                          |\n        | total_steps    | Sum of all steps across all features |\n        | passing_steps  | Sum of all passing steps             |\n        | step_coverage  | Overall passing/total percentage     |\n\n    Scenario: Per-component summary includes step-level detail\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/summary?version=mvp\"\n      Then each component in the summary has:\n        | field              | description                         |\n        | id                 | Component ID                        |\n        | name               | Component name                      |\n        | semver_progress    | Progress from current_version       |\n        | total_steps        | Total Given/When/Then steps         |\n        | passing_steps      | Steps in passing scenarios          |\n        | step_progress      | passing_steps / total_steps * 100   |\n        | combined_progress  | Weighted blend of semver + steps    |\n        | status             | Derived from combined progress      |\n        | feature_count      | Number of feature files             |\n        | scenario_count     | Total scenarios across features     |\n\n    Scenario: Progress summary across all versions\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/overview\"\n      Then the response status is 200\n      And the response body has summaries for versions \"mvp\", \"v1\", and \"v2\"\n      And each version summary includes:\n        | field              | description                      |\n        | version            | Version tag                      |\n        | total_components   | Number of components             |\n        | total_steps        | Sum of steps across all features |\n        | passing_steps      | Sum of passing steps             |\n        | step_coverage      | Overall percentage               |\n        | complete_count     | Components at 100%               |\n        | in_progress_count  | Components at 1-99%              |\n        | planned_count      | Components at 0%                 |\n\n    Scenario: Get progress history for a component version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And component \"history\" has had multiple test runs over time\n      When I send a GET request to \"/api/components/history/versions/mvp/progress-history\"\n      Then the response status is 200\n      And the response body is an array of progress snapshots\n      And each snapshot has fields:\n        | field              | description                    |\n        | timestamp          | When the test run occurred     |\n        | total_steps        | Total steps at that time       |\n        | passing_steps      | Passing steps at that time     |\n        | step_progress      | Step-based percentage          |\n        | semver_progress    | Semver-based percentage        |\n        | combined_progress  | Weighted blend                 |\n\n    Scenario: Progress API provides CSV export\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/progress/summary?version=mvp&format=csv\"\n      Then the response status is 200\n      And the response content type is \"text/csv\"\n      And the CSV has columns: id, name, total_steps, passing_steps, step_progress, semver_progress, combined_progress, status\n\n  # â”€â”€ Automatic Progress Recalculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Progress is recalculated when features or test results change\n\n    Scenario: Uploading a feature triggers step recount and progress update\n      Given component \"recalc-up\" has 20 total steps under version \"v1\" with 20 passing\n      And progress is 100 percent\n      When I upload a new feature with 10 steps under version \"v1\"\n      Then total steps become 30\n      And passing steps remain 20 (new feature has no test results)\n      And progress drops to 67 percent\n\n    Scenario: Deleting a feature triggers step recount and progress update\n      Given component \"recalc-del\" has 30 total steps under version \"v1\"\n      And 20 of those steps are in passing scenarios\n      And progress is 67 percent\n      When I delete a feature with 10 failing steps\n      Then total steps become 20\n      And passing steps remain 20\n      And progress increases to 100 percent\n\n    Scenario: Updating current_version triggers semver progress recalculation\n      Given component \"recalc-ver\" has current_version \"0.5.0\"\n      When I update current_version to \"0.8.0\"\n      Then the semver-derived progress for \"mvp\" becomes 80\n      And the combined progress is recalculated\n\n    Scenario: Recording test results triggers step progress recalculation\n      Given component \"recalc-test\" has step progress at 60 percent\n      When I record new test results with more passing scenarios\n      Then the step progress is recalculated\n      And the combined progress is recalculated\n\n    Scenario: Replacing a feature recounts steps correctly\n      Given component \"recalc-replace\" has feature \"auth.feature\" under version \"v1\" with 8 steps\n      And total steps for version \"v1\" are 20\n      When I upload a new version of \"auth.feature\" under version \"v1\" with 12 steps\n      Then total steps for version \"v1\" become 24 (20 - 8 + 12)\n      And passing steps are recalculated based on latest test results\n\n  # â”€â”€ Progress in Architecture Export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Step-based progress is included in architecture export\n\n    Scenario: Architecture export includes step-based progress per version\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      And components have version-tagged features with test results\n      When I send a GET request to \"/api/architecture\"\n      Then each enriched node includes per-version progress with:\n        | field                       | description                     |\n        | versions.mvp.progress       | Combined progress for MVP       |\n        | versions.mvp.total_steps    | Total steps for MVP features    |\n        | versions.mvp.passing_steps  | Passing steps for MVP           |\n        | versions.v1.progress        | Combined progress for V1        |\n        | versions.v1.total_steps     | Total steps for V1 features     |\n        | versions.v1.passing_steps   | Passing steps for V1            |\n\n    Scenario: JSON export includes step-based progress\n      Given the export use case runs\n      Then the output data.json includes per-version step counts\n      And the web view can display step-level progress bars\n\n  # â”€â”€ Filesystem Feature File Watching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Changes to feature files on disk trigger step recount and progress update\n\n    Scenario: New feature file triggers step recount\n      Given the feature file watcher is running\n      And a new file \"components/test-comp/features/v1-new.feature\" with 8 steps is created\n      When the watcher detects the new file\n      Then the file is stored under version \"v1\" (from filename prefix)\n      And total steps for \"test-comp\" version \"v1\" increase by 8\n      And progress is recalculated\n\n    Scenario: Modified feature file triggers step recount\n      Given the feature file watcher is running\n      And \"components/test-comp/features/mvp-auth.feature\" changes from 5 to 9 steps\n      When the watcher detects the modification\n      Then the step count for that feature updates to 9\n      And total steps for \"test-comp\" version \"mvp\" are recalculated\n      And progress is recalculated\n\n    Scenario: Deleted feature file triggers step recount\n      Given the feature file watcher is running\n      And \"components/test-comp/features/v1-old.feature\" with 6 steps is deleted\n      When the watcher detects the deletion\n      Then total steps for \"test-comp\" version \"v1\" decrease by 6\n      And progress is recalculated\n\n    Scenario: Watcher can be triggered manually via API\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/scan-features\"\n      Then the response status is 200\n      And the response body includes per-version step totals:\n        | field               | description                    |\n        | scanned             | Total files processed          |\n        | added               | New features found             |\n        | updated             | Modified features updated      |\n        | removed             | Deleted features cleaned up    |\n        | step_totals.mvp     | Total MVP steps after scan     |\n        | step_totals.v1      | Total V1 steps after scan      |\n        | step_totals.v2      | Total V2 steps after scan      |\n\n    Scenario: Watcher ignores non-feature files\n      Given the feature file watcher is running\n      And a file \"components/test-comp/features/README.md\" is created\n      When the watcher processes the event\n      Then no feature is added to the database\n      And no step recount occurs\n\n    Scenario: Watcher debounces rapid successive changes\n      Given the feature file watcher is running\n      When 10 feature files are modified within 500 milliseconds\n      Then the watcher batches the changes\n      And triggers a single re-scan after a 1-second debounce period\n      And step counts are recalculated once for all 10 files\n"
            },
            {
              "filename": "v1-neo4j-graph-storage.feature",
              "title": "Neo4j Graph Storage",
              "content": "@wip @v1\nFeature: Neo4j Graph Storage\n  As the roadmap application\n  I want to store all architecture data in Neo4j instead of SQLite\n  So that the graph data model is natively represented, traversals are efficient,\n  and the system is ready for production-scale autonomous coding workflows\n\n  Neo4j replaces SQLite as the persistence layer. The domain entities, repository\n  interfaces, and use cases remain unchanged. Only the infrastructure layer swaps\n  out Drizzle/SQLite repositories for Neo4j Cypher-backed repositories. Connection\n  credentials are stored securely via environment variables, never in code or config\n  files committed to version control.\n\n  # â”€â”€ Connection & Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j connection is configured via environment variables\n\n    Scenario: Connect to Neo4j using environment variables\n      Given the environment variable \"NEO4J_URI\" is set to \"bolt://localhost:7687\"\n      And the environment variable \"NEO4J_USER\" is set to \"neo4j\"\n      And the environment variable \"NEO4J_PASSWORD\" is set to a secure value\n      When the application creates a Neo4j connection\n      Then the connection is established successfully\n      And the driver verifies connectivity with a test query\n\n    Scenario: Connection fails gracefully when NEO4J_URI is missing\n      Given the environment variable \"NEO4J_URI\" is not set\n      When the application attempts to create a Neo4j connection\n      Then a configuration error is thrown with message containing \"NEO4J_URI\"\n      And no connection is established\n\n    Scenario: Connection fails gracefully when NEO4J_PASSWORD is missing\n      Given the environment variable \"NEO4J_URI\" is set to \"bolt://localhost:7687\"\n      And the environment variable \"NEO4J_PASSWORD\" is not set\n      When the application attempts to create a Neo4j connection\n      Then a configuration error is thrown with message containing \"NEO4J_PASSWORD\"\n\n    Scenario: Connection uses encrypted transport in production\n      Given the environment variable \"NODE_ENV\" is set to \"production\"\n      And the environment variable \"NEO4J_URI\" starts with \"neo4j+s://\"\n      When the application creates a Neo4j connection\n      Then the driver uses encrypted transport\n      And the TLS certificate is verified\n\n    Scenario: Connection pool settings are configurable\n      Given the environment variable \"NEO4J_MAX_CONNECTIONS\" is set to \"50\"\n      And the environment variable \"NEO4J_ACQUISITION_TIMEOUT\" is set to \"30000\"\n      When the application creates a Neo4j connection\n      Then the connection pool max size is 50\n      And the acquisition timeout is 30000 milliseconds\n\n    Scenario: Connection retries on transient failure\n      Given the Neo4j server is temporarily unavailable\n      When the application creates a Neo4j connection with retry enabled\n      Then it retries the connection up to 3 times\n      And it waits with exponential backoff between attempts\n      And the final failure is logged with connection details (excluding password)\n\n    Scenario: Credentials are never logged or exposed\n      Given a Neo4j connection is configured\n      When the connection details are logged\n      Then the log output contains the URI\n      And the log output does not contain the password\n      And the log output does not contain the username\n\n  # â”€â”€ Schema Initialisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j schema is initialised with constraints and indexes\n\n    Scenario: Node uniqueness constraint is created\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a uniqueness constraint exists on Node.id\n      And the constraint name is \"unique_node_id\"\n\n    Scenario: Edge composite uniqueness constraint is created\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a uniqueness constraint exists on Edge (source_id, target_id, type)\n      And duplicate edges are rejected by the database\n\n    Scenario: Version composite uniqueness constraint is created\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a uniqueness constraint exists on Version (node_id, version)\n      And duplicate versions are rejected by the database\n\n    Scenario: Indexes are created for common query patterns\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then an index exists on Node.type\n      And an index exists on Node.layer\n      And an index exists on Version.node_id\n      And an index exists on Feature.node_id\n      And an index exists on Feature.version\n\n    Scenario: Schema initialisation is idempotent\n      Given a Neo4j database with existing constraints and indexes\n      When the schema initialisation runs again\n      Then no errors are thrown\n      And the constraints remain unchanged\n      And the indexes remain unchanged\n\n    Scenario: Full-text index on Node name and description\n      Given a fresh Neo4j database\n      When the schema initialisation runs\n      Then a full-text index exists on Node.name and Node.description\n      And the index supports case-insensitive search\n\n  # â”€â”€ Node CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j node repository implements INodeRepository\n\n    Scenario: Save a new node\n      Given an empty Neo4j database with schema\n      When I save a node with id \"test-comp\", name \"Test Component\", type \"component\"\n      Then a Neo4j node labelled \"ArchNode\" exists with id \"test-comp\"\n      And the node has property \"name\" with value \"Test Component\"\n      And the node has property \"type\" with value \"component\"\n\n    Scenario: Save a node with all optional fields\n      Given an empty Neo4j database with schema\n      When I save a node with:\n        | field           | value                     |\n        | id              | full-comp                 |\n        | name            | Full Component            |\n        | type            | component                 |\n        | layer           | supervisor-layer          |\n        | color           | #FF5733                   |\n        | icon            | server                    |\n        | description     | A fully specified node    |\n        | tags            | [\"runtime\",\"core\"]        |\n        | sort_order      | 10                        |\n        | current_version | 0.5.0                     |\n      Then the Neo4j node \"full-comp\" has all specified properties\n\n    Scenario: Update an existing node via upsert\n      Given a node \"upsert-comp\" exists in Neo4j with name \"Original\"\n      When I save a node with id \"upsert-comp\" and name \"Updated\"\n      Then the Neo4j node \"upsert-comp\" has property \"name\" with value \"Updated\"\n      And only one node with id \"upsert-comp\" exists\n\n    Scenario: Find all nodes ordered by sort_order\n      Given these nodes exist in Neo4j:\n        | id   | name   | type      | sort_order |\n        | b    | Beta   | component | 20         |\n        | a    | Alpha  | component | 10         |\n        | c    | Gamma  | component | 30         |\n      When I call findAll on the node repository\n      Then the result contains 3 nodes\n      And the nodes are ordered [\"a\", \"b\", \"c\"]\n\n    Scenario: Find node by ID\n      Given a node \"find-me\" exists in Neo4j\n      When I call findById with \"find-me\"\n      Then the result is the node with id \"find-me\"\n\n    Scenario: Find node by ID returns null when not found\n      When I call findById with \"nonexistent\"\n      Then the result is null\n\n    Scenario: Find nodes by type\n      Given these nodes exist in Neo4j:\n        | id      | name      | type      |\n        | layer-1 | Layer One | layer     |\n        | comp-1  | Comp One  | component |\n        | comp-2  | Comp Two  | component |\n      When I call findByType with \"component\"\n      Then the result contains 2 nodes\n      And both nodes have type \"component\"\n\n    Scenario: Find nodes by layer\n      Given these nodes exist in Neo4j:\n        | id     | name     | type      | layer    |\n        | comp-a | Comp A   | component | layer-x  |\n        | comp-b | Comp B   | component | layer-x  |\n        | comp-c | Comp C   | component | layer-y  |\n      When I call findByLayer with \"layer-x\"\n      Then the result contains 2 nodes\n      And both nodes have layer \"layer-x\"\n\n    Scenario: Check node existence\n      Given a node \"exists-comp\" exists in Neo4j\n      When I call exists with \"exists-comp\"\n      Then the result is true\n\n    Scenario: Check node existence returns false for missing\n      When I call exists with \"missing-comp\"\n      Then the result is false\n\n    Scenario: Delete a node\n      Given a node \"delete-me\" exists in Neo4j\n      When I call delete with \"delete-me\"\n      Then no node with id \"delete-me\" exists in Neo4j\n\n    Scenario: Delete cascades to related edges\n      Given a node \"cascade-node\" exists in Neo4j\n      And an edge exists from \"cascade-node\" to \"other-node\" with type \"DEPENDS_ON\"\n      When I call delete with \"cascade-node\"\n      Then no edges reference \"cascade-node\"\n\n    Scenario: Tags are stored as a JSON string property\n      Given I save a node with id \"tag-node\" and tags [\"alpha\", \"beta\"]\n      When I call findById with \"tag-node\"\n      Then the node tags are [\"alpha\", \"beta\"]\n      And the raw Neo4j property \"tags\" is a JSON string\n\n  # â”€â”€ Edge CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j edge repository implements IEdgeRepository\n\n    Scenario: Save a new edge as a Neo4j relationship\n      Given nodes \"source-1\" and \"target-1\" exist in Neo4j\n      When I save an edge from \"source-1\" to \"target-1\" with type \"DEPENDS_ON\"\n      Then a Neo4j relationship of type \"DEPENDS_ON\" exists from \"source-1\" to \"target-1\"\n\n    Scenario: Save an edge with label and metadata\n      Given nodes \"src\" and \"tgt\" exist in Neo4j\n      When I save an edge from \"src\" to \"tgt\" with:\n        | field    | value                 |\n        | type     | CONTROLS              |\n        | label    | spawns                |\n        | metadata | {\"priority\":\"high\"}   |\n      Then the relationship has property \"label\" with value \"spawns\"\n      And the relationship has property \"metadata\" with value '{\"priority\":\"high\"}'\n\n    Scenario: Edge upsert updates label and metadata\n      Given an edge from \"a\" to \"b\" with type \"DEPENDS_ON\" and label \"old\"\n      When I save an edge from \"a\" to \"b\" with type \"DEPENDS_ON\" and label \"new\"\n      Then only one \"DEPENDS_ON\" relationship exists from \"a\" to \"b\"\n      And the label is \"new\"\n\n    Scenario: Find all edges\n      Given 5 edges exist in Neo4j\n      When I call findAll on the edge repository\n      Then the result contains 5 edges\n\n    Scenario: Find edges by source\n      Given edges from \"hub\" to \"spoke-1\", \"spoke-2\", \"spoke-3\" exist\n      When I call findBySource with \"hub\"\n      Then the result contains 3 edges\n      And all edges have source_id \"hub\"\n\n    Scenario: Find edges by target\n      Given edges from \"a\" to \"hub\" and \"b\" to \"hub\" exist\n      When I call findByTarget with \"hub\"\n      Then the result contains 2 edges\n      And all edges have target_id \"hub\"\n\n    Scenario: Find edges by type\n      Given 2 \"CONTAINS\" and 3 \"DEPENDS_ON\" edges exist\n      When I call findByType with \"CONTAINS\"\n      Then the result contains 2 edges\n\n    Scenario: Find relationships excludes CONTAINS edges\n      Given 2 \"CONTAINS\" and 3 \"DEPENDS_ON\" edges exist\n      When I call findRelationships\n      Then the result contains 3 edges\n      And no edge has type \"CONTAINS\"\n\n    Scenario: Delete an edge by ID\n      Given an edge with known ID exists in Neo4j\n      When I call delete with that edge ID\n      Then the edge no longer exists\n\n    Scenario: All 11 edge types are supported\n      Given nodes \"edge-src\" and \"edge-tgt\" exist in Neo4j\n      When I save edges with each of these types:\n        | type           |\n        | CONTAINS       |\n        | CONTROLS       |\n        | DEPENDS_ON     |\n        | READS_FROM     |\n        | WRITES_TO      |\n        | DISPATCHES_TO  |\n        | ESCALATES_TO   |\n        | PROXIES        |\n        | SANITISES      |\n        | GATES          |\n        | SEQUENCE       |\n      Then 11 relationships exist from \"edge-src\" to \"edge-tgt\"\n\n  # â”€â”€ Version CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j version repository implements IVersionRepository\n\n    Scenario: Save a new version\n      Given a node \"ver-node\" exists in Neo4j\n      When I save a version for \"ver-node\" with version \"mvp\" and progress 50\n      Then a Neo4j node labelled \"Version\" exists with node_id \"ver-node\" and version \"mvp\"\n      And the version has progress 50\n\n    Scenario: Version upsert updates content and progress\n      Given a version \"mvp\" exists for node \"ver-node\" with progress 30\n      When I save a version for \"ver-node\" with version \"mvp\" and progress 70\n      Then the version \"mvp\" for \"ver-node\" has progress 70\n      And only one version \"mvp\" exists for \"ver-node\"\n\n    Scenario: Find all versions\n      Given versions exist for multiple nodes\n      When I call findAll on the version repository\n      Then the result contains all versions ordered by node_id and version\n\n    Scenario: Find versions by node\n      Given node \"multi-ver\" has versions \"overview\", \"mvp\", \"v1\", \"v2\"\n      When I call findByNode with \"multi-ver\"\n      Then the result contains 4 versions\n      And all versions have node_id \"multi-ver\"\n\n    Scenario: Find version by node and version tag\n      Given node \"specific-ver\" has version \"v1\" with content \"V1 spec\"\n      When I call findByNodeAndVersion with \"specific-ver\" and \"v1\"\n      Then the result has content \"V1 spec\"\n\n    Scenario: Update progress and status via save\n      Given node \"prog-node\" has version \"mvp\" with progress 0 and status \"planned\"\n      When I call save with node \"prog-node\", version \"mvp\", progress 50, status \"in-progress\"\n      Then the version has progress 50 and status \"in-progress\"\n      And the updated_at timestamp is refreshed\n\n    Scenario: Delete all versions for a node\n      Given node \"del-ver\" has versions \"overview\", \"mvp\", \"v1\"\n      When I call deleteByNode with \"del-ver\"\n      Then no versions exist for \"del-ver\"\n\n  # â”€â”€ Feature CRUD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j feature repository implements IFeatureRepository\n\n    Scenario: Save a new feature\n      Given a node \"feat-node\" exists in Neo4j\n      When I save a feature for \"feat-node\" with filename \"mvp-test.feature\"\n      Then a Neo4j node labelled \"Feature\" exists linked to \"feat-node\"\n      And the feature has filename \"mvp-test.feature\"\n\n    Scenario: Find all features\n      Given features exist for multiple nodes and versions\n      When I call findAll on the feature repository\n      Then the result contains all features ordered by node_id, version, filename\n\n    Scenario: Find features by node\n      Given node \"feat-multi\" has 3 feature files\n      When I call findByNode with \"feat-multi\"\n      Then the result contains 3 features\n\n    Scenario: Find features by node and version\n      Given node \"feat-ver\" has 2 \"mvp\" features and 1 \"v1\" feature\n      When I call findByNodeAndVersion with \"feat-ver\" and \"mvp\"\n      Then the result contains 2 features\n      And both features have version \"mvp\"\n\n    Scenario: Delete all features (re-seed preparation)\n      Given features exist across multiple nodes\n      When I call deleteAll on the feature repository\n      Then no features exist in the database\n\n    Scenario: Delete features by node\n      Given node \"feat-del\" has features and node \"feat-keep\" has features\n      When I call deleteByNode with \"feat-del\"\n      Then no features exist for \"feat-del\"\n      And features still exist for \"feat-keep\"\n\n  # â”€â”€ Native Graph Traversals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j enables native graph traversal queries\n\n    Scenario: Multi-hop dependency traversal\n      Given this dependency chain exists:\n        | source    | target    |\n        | comp-a    | comp-b    |\n        | comp-b    | comp-c    |\n        | comp-c    | comp-d    |\n      When I query for all transitive dependencies of \"comp-a\" up to depth 3\n      Then the result contains \"comp-b\", \"comp-c\", \"comp-d\"\n\n    Scenario: Reverse dependency traversal (dependents)\n      Given this dependency chain exists:\n        | source    | target    |\n        | comp-a    | comp-d    |\n        | comp-b    | comp-d    |\n        | comp-c    | comp-d    |\n      When I query for all transitive dependents of \"comp-d\" up to depth 1\n      Then the result contains \"comp-a\", \"comp-b\", \"comp-c\"\n\n    Scenario: Shortest path between two components\n      Given a graph with multiple paths from \"start\" to \"end\"\n      When I query for the shortest path from \"start\" to \"end\"\n      Then the result contains the path with the fewest hops\n      And each hop includes the edge type and label\n\n    Scenario: Layer containment subtree\n      Given a layer \"supervisor-layer\" containing 4 components\n      When I query the containment subtree of \"supervisor-layer\"\n      Then the result contains 4 child nodes\n      And each child has a CONTAINS relationship from \"supervisor-layer\"\n\n    Scenario: Cycle detection in dependency graph\n      Given this dependency chain exists:\n        | source    | target    |\n        | comp-a    | comp-b    |\n        | comp-b    | comp-c    |\n        | comp-c    | comp-a    |\n      When I check for cycles in the dependency graph\n      Then a cycle is detected involving \"comp-a\", \"comp-b\", \"comp-c\"\n\n    Scenario: Component neighbourhood query\n      Given \"center-comp\" has 3 outbound and 2 inbound edges\n      When I query the 1-hop neighbourhood of \"center-comp\"\n      Then the result contains 5 related components\n      And each result includes the edge type and direction\n\n  # â”€â”€ Data Migration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: SQLite data can be migrated to Neo4j\n\n    Scenario: Migrate all nodes from SQLite to Neo4j\n      Given a SQLite database containing 60 nodes\n      When the migration tool runs\n      Then 60 ArchNode nodes exist in Neo4j\n      And each node has all properties preserved\n\n    Scenario: Migrate all edges from SQLite to Neo4j\n      Given a SQLite database containing 120 edges\n      When the migration tool runs\n      Then 120 relationships exist in Neo4j\n      And each relationship has the correct type, label, and metadata\n\n    Scenario: Migrate all versions from SQLite to Neo4j\n      Given a SQLite database containing 200 versions\n      When the migration tool runs\n      Then 200 Version nodes exist in Neo4j\n      And each version is linked to its parent node\n\n    Scenario: Migrate all features from SQLite to Neo4j\n      Given a SQLite database containing 50 features\n      When the migration tool runs\n      Then 50 Feature nodes exist in Neo4j\n      And each feature is linked to its parent node\n\n    Scenario: Migration is idempotent\n      Given a SQLite database has been migrated once\n      When the migration tool runs again\n      Then no duplicate nodes or relationships are created\n      And updated properties are reflected in Neo4j\n\n    Scenario: Migration validates data integrity\n      Given the migration has completed\n      When a validation check compares SQLite and Neo4j\n      Then the node count matches\n      And the edge count matches\n      And the version count matches\n      And the feature count matches\n      And a sample of 10 nodes have identical properties in both databases\n\n    Scenario: Migration CLI provides progress reporting\n      Given a SQLite database with data\n      When the migration tool runs in verbose mode\n      Then it reports the number of nodes migrated\n      And it reports the number of edges migrated\n      And it reports the number of versions migrated\n      And it reports the number of features migrated\n      And it reports the total elapsed time\n\n  # â”€â”€ Transaction Safety â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j operations use transactions for data integrity\n\n    Scenario: Node save runs within a transaction\n      Given the Neo4j node repository\n      When I save a node and the operation succeeds\n      Then the node is committed to the database\n\n    Scenario: Failed save rolls back the transaction\n      Given the Neo4j node repository\n      When I save a node with an invalid property\n      Then the transaction is rolled back\n      And no partial data exists in the database\n\n    Scenario: Bulk insert uses a single transaction\n      Given 10 nodes to insert\n      When I save all 10 nodes in a batch operation\n      Then either all 10 nodes are committed or none are\n      And the operation uses a single transaction\n\n    Scenario: Concurrent writes are serialised safely\n      Given two concurrent requests to update node \"shared-node\"\n      When both requests execute simultaneously\n      Then both writes complete without data corruption\n      And the final state reflects one of the two writes\n\n  # â”€â”€ Security â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j connection credentials are handled securely\n\n    Scenario: Password is not stored in any configuration file\n      Given the project source directory\n      When I search all files for the Neo4j password\n      Then no file in the repository contains a hardcoded password\n      And credentials are only read from environment variables\n\n    Scenario: Connection string does not leak to client responses\n      Given the API server is running with Neo4j backend\n      When I send a GET request that causes a database error\n      Then the error response does not contain the connection URI\n      And the error response does not contain credentials\n      And the error response contains a generic error message\n\n    Scenario: Database user has minimum required privileges\n      Given the Neo4j user for the application\n      Then the user has read and write access to the application database\n      And the user does not have admin privileges\n      And the user cannot create or drop databases\n\n    Scenario: Environment variables for Neo4j are documented\n      Given the project documentation\n      Then it lists \"NEO4J_URI\" as required\n      And it lists \"NEO4J_USER\" as required\n      And it lists \"NEO4J_PASSWORD\" as required\n      And it lists \"NEO4J_DATABASE\" as optional with default \"neo4j\"\n      And it lists \"NEO4J_MAX_CONNECTIONS\" as optional with default \"100\"\n\n  # â”€â”€ Repository Interface Compliance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Neo4j repositories implement the same domain interfaces as SQLite\n\n    Scenario: Neo4j node repository implements INodeRepository\n      Given the Neo4j node repository class\n      Then it implements the INodeRepository interface\n      And it has methods: findAll, findById, findByType, findByLayer, exists, save, delete\n\n    Scenario: Neo4j edge repository implements IEdgeRepository\n      Given the Neo4j edge repository class\n      Then it implements the IEdgeRepository interface\n      And it has methods: findAll, findBySource, findByTarget, findByType, findRelationships, save, delete\n\n    Scenario: Neo4j version repository implements IVersionRepository\n      Given the Neo4j version repository class\n      Then it implements the IVersionRepository interface\n      And it has methods: findAll, findByNode, findByNodeAndVersion, save, deleteByNode\n\n    Scenario: Neo4j feature repository implements IFeatureRepository\n      Given the Neo4j feature repository class\n      Then it implements the IFeatureRepository interface\n      And it has methods: findAll, findByNode, findByNodeAndVersion, save, deleteAll, deleteByNode\n\n    Scenario: Use cases work identically with Neo4j repositories\n      Given the GetArchitecture use case\n      When I inject Neo4j repositories instead of SQLite repositories\n      Then the use case executes without modification\n      And the output structure is identical\n\n    Scenario: Adapter wiring selects storage backend from environment\n      Given the environment variable \"STORAGE_BACKEND\" is set to \"neo4j\"\n      When the API adapter initialises\n      Then it creates Neo4j repository instances\n      And injects them into use cases\n\n    Scenario: Adapter defaults to SQLite when STORAGE_BACKEND is unset\n      Given the environment variable \"STORAGE_BACKEND\" is not set\n      When the API adapter initialises\n      Then it creates SQLite/Drizzle repository instances\n      And injects them into use cases\n"
            },
            {
              "filename": "v1-secure-api.feature",
              "title": "Secure and Rate-Limited API",
              "content": "@wip @v1\nFeature: Secure and Rate-Limited API\n  As the roadmap application\n  I want the REST API to be secured with API key authentication and rate limiting\n  So that only authorised LLM engineers can manage roadmap content headlessly\n  and the system is protected from abuse, brute-force attacks, and accidental overload\n\n  The MVP API has no authentication. V1 adds API key authentication with scoped\n  permissions, per-key rate limiting, request logging, and security headers.\n  API keys are stored as salted hashes. All sensitive operations require\n  appropriate scopes. Rate limits are configurable per key and per endpoint.\n\n  # â”€â”€ API Key Generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API keys are generated securely and stored as hashed values\n\n    Scenario: Generate a new API key via CLI\n      Given the API key management CLI\n      When I run the command to generate a new key with name \"ci-bot\"\n      Then a new API key is returned in the format \"rmap_<32 hex characters>\"\n      And the key is displayed once and never stored in plaintext\n      And a salted SHA-256 hash of the key is stored in the database\n\n    Scenario: Generate a key with specific scopes\n      Given the API key management CLI\n      When I run the command to generate a key with name \"reader\" and scopes \"read\"\n      Then the key is created with scope \"read\"\n      And the key cannot be used for write operations\n\n    Scenario: Generate a key with multiple scopes\n      Given the API key management CLI\n      When I run the command to generate a key with name \"engineer\" and scopes \"read,write\"\n      Then the key is created with scopes \"read\" and \"write\"\n\n    Scenario: Generate a key with admin scope\n      Given the API key management CLI\n      When I run the command to generate a key with name \"admin-bot\" and scopes \"read,write,admin\"\n      Then the key is created with scopes \"read\", \"write\", and \"admin\"\n\n    Scenario: API key name must be unique\n      Given a key with name \"existing-bot\" already exists\n      When I run the command to generate a key with name \"existing-bot\"\n      Then an error is returned with message \"Key name already exists: existing-bot\"\n      And no new key is created\n\n    Scenario: API key has an optional expiry date\n      Given the API key management CLI\n      When I run the command to generate a key with name \"temp-key\" and expiry \"2026-12-31\"\n      Then the key is created with expiry date \"2026-12-31T00:00:00Z\"\n\n    Scenario: API key with no expiry never expires\n      Given the API key management CLI\n      When I run the command to generate a key with name \"permanent-key\" and no expiry\n      Then the key is created with a null expiry date\n\n  # â”€â”€ API Key Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API keys are stored securely in the database\n\n    Scenario: Key record contains required fields\n      Given a newly generated API key\n      Then the database record contains:\n        | field        | type     | description                    |\n        | id           | integer  | Auto-incrementing primary key  |\n        | name         | text     | Unique human-readable name     |\n        | key_hash     | text     | Salted SHA-256 hash            |\n        | salt         | text     | Unique per-key salt            |\n        | scopes       | text     | JSON array of scope strings    |\n        | created_at   | text     | ISO 8601 timestamp             |\n        | expires_at   | text     | ISO 8601 timestamp or null     |\n        | last_used_at | text     | ISO 8601 timestamp or null     |\n        | is_active    | integer  | 1 = active, 0 = revoked        |\n\n    Scenario: Raw API key cannot be retrieved from the database\n      Given a key \"stored-key\" exists in the database\n      When I query the api_keys table for \"stored-key\"\n      Then the result contains key_hash but not the raw key\n      And the key_hash cannot be reversed to obtain the raw key\n\n    Scenario: Key verification uses constant-time comparison\n      Given a key \"verify-key\" exists in the database\n      When I verify an API key against the stored hash\n      Then the comparison uses a timing-safe equality check\n      And the verification completes in constant time regardless of match\n\n  # â”€â”€ API Key Authentication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: All API endpoints except health require a valid API key\n\n    Scenario: Health endpoint does not require authentication\n      Given the API server is running\n      When I send a GET request to \"/api/health\" without an API key\n      Then the response status is 200\n      And the response body has field \"status\" with value \"ok\"\n\n    Scenario: Authenticated request with valid key succeeds\n      Given the API server is running\n      And a valid API key \"rmap_abc123\" with scope \"read\" exists\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_abc123\"\n      Then the response status is 200\n\n    Scenario: Request without API key returns 401\n      Given the API server is running\n      When I send a GET request to \"/api/components\" without an API key\n      Then the response status is 401\n      And the response body has field \"error\" with value \"Authentication required\"\n      And the response has header \"WWW-Authenticate\" with value \"Bearer\"\n\n    Scenario: Request with invalid API key returns 401\n      Given the API server is running\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_invalid\"\n      Then the response status is 401\n      And the response body has field \"error\" with value \"Invalid API key\"\n\n    Scenario: Request with expired API key returns 401\n      Given the API server is running\n      And an expired API key \"rmap_expired\" exists\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_expired\"\n      Then the response status is 401\n      And the response body has field \"error\" with value \"API key expired\"\n\n    Scenario: Request with revoked API key returns 401\n      Given the API server is running\n      And a revoked API key \"rmap_revoked\" exists\n      When I send a GET request to \"/api/components\" with header \"Authorization: Bearer rmap_revoked\"\n      Then the response status is 401\n      And the response body has field \"error\" with value \"API key revoked\"\n\n    Scenario: API key is accepted via X-API-Key header as alternative\n      Given the API server is running\n      And a valid API key \"rmap_alt123\" with scope \"read\" exists\n      When I send a GET request to \"/api/components\" with header \"X-API-Key: rmap_alt123\"\n      Then the response status is 200\n\n    Scenario: Last-used timestamp is updated on successful authentication\n      Given the API server is running\n      And a valid API key \"rmap_track\" with scope \"read\" exists\n      When I send a GET request to \"/api/components\" with that key\n      Then the key's last_used_at timestamp is updated to the current time\n\n  # â”€â”€ Scope-Based Authorisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API operations require specific scopes\n\n    Scenario: Read scope allows GET requests\n      Given a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components\" with that key\n      Then the response status is 200\n\n    Scenario: Read scope denies POST requests\n      Given a valid API key with scope \"read\" only\n      When I send a POST request to \"/api/components\" with that key and body:\n        \"\"\"\n        {\"id\":\"new\",\"name\":\"New\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 403\n      And the response body has field \"error\" with value \"Insufficient scope: write required\"\n\n    Scenario: Write scope allows POST requests\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a POST request to \"/api/components\" with that key and body:\n        \"\"\"\n        {\"id\":\"writable\",\"name\":\"Writable\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n\n    Scenario: Write scope allows PUT requests\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a PUT request to \"/api/components/test-comp/features/v1-test.feature\" with that key\n      Then the response status is not 403\n\n    Scenario: Write scope allows PUT requests\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a PUT request to \"/api/components/test-comp/versions/mvp\" with that key\n      Then the response status is not 403\n\n    Scenario: Write scope allows DELETE requests for component data\n      Given a valid API key with scopes \"read\" and \"write\"\n      When I send a DELETE request to \"/api/components/del-comp\" with that key\n      Then the response status is not 403\n\n    Scenario: Admin scope required for key management endpoints\n      Given a valid API key with scopes \"read\" and \"write\" but not \"admin\"\n      When I send a POST request to \"/api/admin/keys\" with that key\n      Then the response status is 403\n      And the response body has field \"error\" with value \"Insufficient scope: admin required\"\n\n    Scenario: Admin scope allows key management\n      Given a valid API key with scope \"admin\"\n      When I send a GET request to \"/api/admin/keys\" with that key\n      Then the response status is 200\n\n    Scenario: Scope mapping for all HTTP methods\n      Then the following scope mapping applies:\n        | method  | scope  |\n        | GET     | read   |\n        | POST    | write  |\n        | PUT     | write  |\n        | PATCH   | write  |\n        | DELETE  | write  |\n\n  # â”€â”€ API Key Management Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Admin users can manage API keys via the API\n\n    Scenario: List all API keys (admin)\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And 3 API keys exist in the database\n      When I send a GET request to \"/api/admin/keys\" with the admin key\n      Then the response status is 200\n      And the response body is an array of 3 key records\n      And no record contains the raw key or key_hash\n\n    Scenario: Revoke an API key (admin)\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      And a key with name \"revoke-me\" exists and is active\n      When I send a DELETE request to \"/api/admin/keys/revoke-me\" with the admin key\n      Then the response status is 200\n      And the key \"revoke-me\" is marked as inactive\n      And subsequent requests with that key return 401\n\n    Scenario: Generate a new key via API (admin)\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a POST request to \"/api/admin/keys\" with the admin key and body:\n        \"\"\"\n        {\"name\":\"api-created\",\"scopes\":[\"read\",\"write\"]}\n        \"\"\"\n      Then the response status is 201\n      And the response body contains the raw key (displayed once)\n      And the response body has field \"name\" with value \"api-created\"\n\n    Scenario: Revoke nonexistent key returns 404\n      Given the API server is running\n      And a valid API key with scope \"admin\"\n      When I send a DELETE request to \"/api/admin/keys/ghost-key\" with the admin key\n      Then the response status is 404\n      And the response body has field \"error\"\n\n  # â”€â”€ Rate Limiting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API requests are rate-limited per key and per endpoint\n\n    Scenario: Default rate limit is applied per API key\n      Given the API server is running with default rate limit of 100 requests per minute\n      And a valid API key \"rmap_rate\" with scope \"read\"\n      When I send 100 GET requests to \"/api/components\" within 1 minute\n      Then all 100 requests return status 200\n\n    Scenario: Exceeding rate limit returns 429\n      Given the API server is running with default rate limit of 100 requests per minute\n      And a valid API key \"rmap_over\" with scope \"read\"\n      When I send 101 GET requests to \"/api/components\" within 1 minute\n      Then the 101st request returns status 429\n      And the response body has field \"error\" with value \"Rate limit exceeded\"\n      And the response has header \"Retry-After\" with a positive integer value\n\n    Scenario: Rate limit headers are included in every response\n      Given the API server is running\n      And a valid API key \"rmap_headers\" with scope \"read\"\n      When I send a GET request to \"/api/components\" with that key\n      Then the response has header \"X-RateLimit-Limit\"\n      And the response has header \"X-RateLimit-Remaining\"\n      And the response has header \"X-RateLimit-Reset\"\n\n    Scenario: Rate limits reset after the window expires\n      Given the API server is running with rate limit of 10 requests per minute\n      And a valid API key \"rmap_reset\" with scope \"read\"\n      And the key has exhausted its rate limit\n      When 60 seconds have elapsed\n      And I send a GET request to \"/api/components\" with that key\n      Then the response status is 200\n      And X-RateLimit-Remaining reflects the fresh window\n\n    Scenario: Write operations have a stricter rate limit\n      Given the API server is running\n      And write endpoints have a rate limit of 30 requests per minute\n      And a valid API key with scopes \"read\" and \"write\"\n      When I send 31 POST requests to \"/api/components\" within 1 minute\n      Then the 31st request returns status 429\n\n    Scenario: Different keys have independent rate limits\n      Given the API server is running with rate limit of 10 requests per minute\n      And valid API keys \"rmap_key1\" and \"rmap_key2\" exist\n      When \"rmap_key1\" sends 10 requests (exhausting its limit)\n      And \"rmap_key2\" sends 1 request\n      Then \"rmap_key2\" gets status 200\n      And \"rmap_key1\" gets status 429 on its next request\n\n    Scenario: Rate limit can be configured per key\n      Given the API server is running\n      And API key \"rmap_premium\" has a custom rate limit of 500 requests per minute\n      When \"rmap_premium\" sends 200 requests within 1 minute\n      Then all requests return status 200\n      And X-RateLimit-Limit reflects 500\n\n    Scenario: Health endpoint is exempt from rate limiting\n      Given the API server is running\n      When I send 1000 GET requests to \"/api/health\" within 1 minute\n      Then all requests return status 200\n\n    Scenario: Rate limit state is stored in memory (not database)\n      Given the API server is running\n      When the server processes rate-limited requests\n      Then no rate limit data is written to the database\n      And rate limit counters reset on server restart\n\n  # â”€â”€ Security Headers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API responses include security headers\n\n    Scenario: All responses include standard security headers\n      Given the API server is running\n      When I send any request to the API\n      Then the response has header \"X-Content-Type-Options\" with value \"nosniff\"\n      And the response has header \"X-Frame-Options\" with value \"DENY\"\n      And the response has header \"Strict-Transport-Security\" with value \"max-age=31536000; includeSubDomains\"\n      And the response has header \"Cache-Control\" with value \"no-store\"\n      And the response has header \"X-Request-Id\" with a UUID value\n\n    Scenario: CORS is restricted to configured origins\n      Given the environment variable \"ALLOWED_ORIGINS\" is set to \"https://app.example.com\"\n      When I send an OPTIONS request with \"Origin: https://app.example.com\"\n      Then the response has header \"Access-Control-Allow-Origin\" with value \"https://app.example.com\"\n\n    Scenario: CORS rejects unconfigured origins\n      Given the environment variable \"ALLOWED_ORIGINS\" is set to \"https://app.example.com\"\n      When I send an OPTIONS request with \"Origin: https://evil.example.com\"\n      Then the response does not have header \"Access-Control-Allow-Origin\"\n\n    Scenario: CORS allows all origins when not configured (development)\n      Given the environment variable \"ALLOWED_ORIGINS\" is not set\n      When I send an OPTIONS request with \"Origin: http://localhost:3000\"\n      Then the response has header \"Access-Control-Allow-Origin\" with value \"*\"\n\n  # â”€â”€ Request Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: API requests are logged for audit and debugging\n\n    Scenario: Successful request is logged\n      Given the API server is running with request logging enabled\n      And a valid API key \"rmap_log\" with scope \"read\"\n      When I send a GET request to \"/api/components\" with that key\n      Then the request log contains an entry with:\n        | field      | value                |\n        | method     | GET                  |\n        | path       | /api/components      |\n        | status     | 200                  |\n        | key_name   | log-key-name         |\n        | duration   | (positive integer)   |\n        | request_id | (UUID)               |\n\n    Scenario: Failed authentication is logged\n      Given the API server is running with request logging enabled\n      When I send a GET request with an invalid API key\n      Then the request log contains an entry with status 401\n      And the log entry does not contain the attempted key value\n\n    Scenario: Rate-limited request is logged\n      Given the API server is running with request logging enabled\n      When a request is rejected due to rate limiting\n      Then the request log contains an entry with status 429\n      And the log entry includes the key name\n\n    Scenario: Request body is not logged for security\n      Given the API server is running with request logging enabled\n      When I send a POST request with a JSON body\n      Then the request log does not contain the request body\n      And the request log does not contain any API key values\n\n    Scenario: Logs include correlation ID for tracing\n      Given the API server is running\n      When I send a request with header \"X-Request-Id: custom-trace-123\"\n      Then the response has header \"X-Request-Id\" with value \"custom-trace-123\"\n      And the request log entry has request_id \"custom-trace-123\"\n\n  # â”€â”€ Input Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: All API inputs are validated and sanitised\n\n    Scenario: Reject request body exceeding maximum size\n      Given the API server is running with max body size of 1MB\n      When I send a POST request with a body larger than 1MB\n      Then the response status is 413\n      And the response body has field \"error\" with value \"Request body too large\"\n\n    Scenario: Reject malformed JSON body\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body \"not json\"\n      Then the response status is 400\n      And the response body has field \"error\" with value \"Invalid JSON body\"\n\n    Scenario: Reject request with path traversal attempt\n      Given the API server is running\n      And a valid API key with scope \"read\"\n      When I send a GET request to \"/api/components/../../../etc/passwd\"\n      Then the response status is 400\n      And the response body has field \"error\"\n\n    Scenario: Strip HTML from string inputs\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request with name \"<script>alert(1)</script>\"\n      Then the stored name does not contain HTML tags\n      And script content is stripped\n\n    Scenario: Validate component ID format\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request with id \"invalid id with spaces!\"\n      Then the response status is 400\n      And the response body has field \"error\" containing \"invalid\"\n\n    Scenario: Component ID must be kebab-case\n      Given the API server is running\n      And a valid API key with scope \"write\"\n      When I send a POST request to \"/api/components\" with body:\n        \"\"\"\n        {\"id\":\"valid-kebab-case\",\"name\":\"Valid\",\"type\":\"component\",\"layer\":\"supervisor-layer\"}\n        \"\"\"\n      Then the response status is 201\n\n  # â”€â”€ Error Response Format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  Rule: Error responses follow a consistent format\n\n    Scenario: All error responses have a consistent structure\n      Given the API server is running\n      When any API request results in an error\n      Then the response body has this structure:\n        \"\"\"\n        {\n          \"error\": \"<human-readable message>\",\n          \"code\": \"<MACHINE_READABLE_CODE>\",\n          \"request_id\": \"<uuid>\"\n        }\n        \"\"\"\n\n    Scenario: Error codes map to HTTP status codes\n      Then the following error codes exist:\n        | code                  | status | description               |\n        | AUTHENTICATION_REQUIRED | 401  | No API key provided       |\n        | INVALID_API_KEY       | 401    | Key not found or invalid  |\n        | API_KEY_EXPIRED       | 401    | Key past expiry date      |\n        | API_KEY_REVOKED       | 401    | Key manually revoked      |\n        | INSUFFICIENT_SCOPE    | 403    | Key lacks required scope  |\n        | RATE_LIMIT_EXCEEDED   | 429    | Too many requests         |\n        | VALIDATION_ERROR      | 400    | Invalid input data        |\n        | NOT_FOUND             | 404    | Resource not found        |\n        | CONFLICT              | 409    | Duplicate resource        |\n        | BODY_TOO_LARGE        | 413    | Request body exceeds limit|\n        | INTERNAL_ERROR        | 500    | Unexpected server error   |\n\n    Scenario: Internal errors do not leak implementation details\n      Given the API server is running\n      When an unexpected error occurs during request handling\n      Then the response status is 500\n      And the response body error message is \"Internal server error\"\n      And the response does not contain stack traces\n      And the response does not contain file paths\n      And the full error is logged server-side with the request_id\n"
            }
          ]
        }
      },
      {
        "id": "live-dashboard",
        "name": "Live Dashboard",
        "type": "app",
        "layer": "observability-dashboard",
        "color": "sky",
        "icon": "ðŸ“Š",
        "description": "Real-time view of the entire runtime. Read-only â€” it observes but never mutates. Built as a simple web app (React / plain HTML) that polls the State Store + Supervisor health API. Runs as a separate process managed by the Supervisor. Think: the runtime equivalent of the process tree diagram, but live. Reads from two sources: State Store (goals, tasks, tool logs, escalations, checkpoints) and Supervisor health API (process status, heartbeat data, resource usage). It writes nothing â€” pure read-only observer. Optional: SSE/WebSocket push from State Store for live updates without polling. Human Gate approval actions can be embedded here, making it the single pane of glass for both observation and control.",
        "tags": [
          "read-only",
          "web ui",
          "sse/websocket"
        ],
        "sort_order": 11,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Single-page web app showing process status (up/down) and current goal. Polls Supervisor health API every 5s. Basic goal queue display from State Store. Static HTML + vanilla JS.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Real-time view of the entire runtime. Read-only â€” it observes but never mutates. Built as a simple web app (React / plain HTML) that polls the State Store + Supervisor health API. Runs as a separate process managed by the Supervisor. Think: the runtime equivalent of the process tree diagram, but live. Data sources: State Store â†’ goals, tasks, tool call logs, escalations, checkpoints, fast-path records, injection events. Supervisor Health API â†’ process status, uptime, restart count, memory, current model per instance. No direct process inspection â€” dashboard never connects to OpenCode or proxies directly. Push vs Poll: SSE from State Store for live updates; poll Supervisor health every 5s. Human Gate embedded: approval buttons for gated tasks + escalation responses in same UI.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Full process tree view with live status. Goal & task feed with click-to-inspect. Tool call timeline with filtering. Security events panel. Escalation queue with response actions.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "SSE/WebSocket for real-time push updates. Entity Explorer for User KG. Repo Map for Code Graph. Embedded Human Gate approval UI. Performance metrics and resource graphs.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-dashboard.feature",
              "title": "Live Dashboard (MVP)",
              "content": "Feature: Live Dashboard (MVP)\n  A read-only web UI showing process status and current goal.\n  Polls the Supervisor health API and State Store.\n\n  Background:\n    Given the Dashboard web app is running\n\n  Scenario: Display process status\n    Given the Supervisor health API reports Meta-Agent as \"running\" and Worker as \"running\"\n    When the Dashboard polls the health API\n    Then both processes are shown with \"running\" status indicators\n\n  Scenario: Display current goal\n    Given the State Store contains a goal \"Build user auth\" with status \"in-progress\"\n    When the Dashboard polls the State Store\n    Then the current goal \"Build user auth\" is displayed\n    And its status shows \"in-progress\"\n\n  Scenario: Auto-refresh on interval\n    Given the Dashboard is displaying process status\n    When 5 seconds have elapsed\n    Then the Dashboard polls the health API again\n    And the display updates with fresh data\n\n  Scenario: Show offline state\n    Given the Supervisor health API is unreachable\n    When the Dashboard polls the health API\n    Then a \"Supervisor Unreachable\" indicator is shown\n\n  Scenario: Read-only â€” no mutation endpoints\n    Given the Dashboard is running\n    Then it exposes no POST, PUT, or DELETE endpoints\n    And all data access is via GET requests\n"
            }
          ]
        }
      },
      {
        "id": "supervisor",
        "name": "Supervisor",
        "type": "app",
        "layer": "supervisor-layer",
        "color": "purple",
        "icon": "ðŸ‘",
        "description": "Manages all child processes. Heartbeat + crash recovery with tiered priority. No LLM, no planning â€” just process management, signal handling, and the recovery state machine. This is what makes it stable: it has almost no reasons to crash. If it does, systemd/Docker restarts it. Exposes a health API (HTTP on :9100) for the dashboard. Kill switch: /stop HTTP endpoint + SIGTERM handler â†’ instant halt of all children. Emergency brake for runaway agents.",
        "tags": [
          "immortal",
          "health api",
          "kill switch",
          "no llm"
        ],
        "sort_order": 21,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Spawn and monitor two child processes (meta-agent, worker). Detect crashes via waitpid(). Restart crashed children with basic retry logic. Expose /health HTTP endpoint returning JSON process status. Handle SIGTERM for graceful shutdown of all children.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Manages all child processes. Heartbeat + crash recovery with tiered priority. No LLM, no planning â€” just process management, signal handling, and the recovery state machine. This is what makes it stable: it has almost no reasons to crash. If it does, systemd/Docker restarts it. Exposes a health API (HTTP on :9100) for the dashboard. Kill switch: /stop HTTP endpoint + SIGTERM handler â†’ instant halt of all children. Why a Supervisor, not a cron job? (1) Instant detection: waitpid() returns the moment a child exits. Cron's worst-case latency = poll interval. (2) Crash loop handling: Supervisor tracks restart count + applies exponential backoff. (3) Multi-step recovery: kill â†’ checkpoint read â†’ config rebuild â†’ context inject â†’ respawn. (4) Lifecycle ownership: Supervisor owns the full process tree â€” PIDs, health, state. (5) Signal handling: catches SIGTERM/SIGCHLD and coordinates graceful shutdown. Stability hierarchy: Tier âˆž (Supervisor) immortal â†’ Tier 0 (Meta-Agent) recovered first â†’ Tier 1 (Worker) expendable. Recovery order: Supervisor â†’ Meta-Agent â†’ Worker. Each tier can recover the one below it.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Add exponential backoff on repeated crashes. Liveness probe (hang detection via output timeout_ms). Recovery state machine with tiered priority (meta-agent first). Checkpoint-aware recovery â€” read last checkpoint before respawn. Human Gate alerting after max 5 retries.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Full config-as-code DSL for spawn configuration. Resource monitoring (memory, CPU per child). Kill switch /stop HTTP endpoint. Dashboard SSE push for process events. Per-instance gate policies. Runtime flag switching for gate modes.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-process-management.feature",
              "title": "Supervisor Process Management (MVP)",
              "content": "Feature: Supervisor Process Management (MVP)\n  The Supervisor spawns and monitors child processes.\n  It detects crashes and restarts children with basic retry logic.\n\n  Background:\n    Given the Supervisor process is running\n\n  Scenario: Spawn child processes on startup\n    When the Supervisor starts\n    Then it spawns the Meta-Agent process\n    And it spawns the Worker process\n    And both processes are in \"running\" state\n\n  Scenario: Detect child crash via waitpid\n    Given the Meta-Agent process is running\n    When the Meta-Agent process exits unexpectedly\n    Then the Supervisor detects the exit within 100ms\n    And the exit is logged with the process ID and exit code\n\n  Scenario: Restart crashed child\n    Given the Worker process has crashed\n    When the Supervisor detects the crash\n    Then it restarts the Worker process\n    And the new process is in \"running\" state\n    And the restart count is incremented\n\n  Scenario: Respect maximum retry limit\n    Given the Worker has crashed 5 times consecutively\n    When the Worker crashes again\n    Then the Supervisor does not restart the Worker\n    And the Worker state is set to \"failed\"\n    And an alert is logged\n\n  Scenario: Health API returns process status\n    Given both child processes are running\n    When a GET request is made to /health\n    Then the response status is 200\n    And the response body contains status for each child process\n    And each status includes \"pid\", \"state\", and \"uptime\"\n\n  Scenario: Graceful shutdown on SIGTERM\n    Given both child processes are running\n    When the Supervisor receives SIGTERM\n    Then it sends SIGTERM to all child processes\n    And it waits for children to exit\n    And it exits with code 0\n"
            }
          ]
        }
      },
      {
        "id": "human-gate",
        "name": "Human Gate",
        "type": "app",
        "layer": "supervisor-layer",
        "color": "pink",
        "icon": "â›³",
        "description": "Three modes: full-auto, approve-goals, approve-all. Plus write fence: dangerous ops require approval even in full-auto. Also surfaces escalation requests from the Worker. Write fence per-instance: Meta-Agent config mutations and Worker destructive ops can have independent gate policies. Gate mode is a runtime flag â€” switch between modes without restarting any process.",
        "tags": [
          "full-auto",
          "approve-goals",
          "approve-all",
          "write fence",
          "runtime flag"
        ],
        "sort_order": 23,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Basic approval queue. CLI-based approve/reject. Write fence for destructive operations (hardcoded list). Block until approved or timeout.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Three modes: full-auto, approve-goals, approve-all. Plus write fence: dangerous ops require approval even in full-auto. Also surfaces escalation requests from the Worker. Write fence per-instance: Meta-Agent config mutations and Worker destructive ops have independent gate policies. Gate mode is a runtime flag â€” switch between modes without restarting.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Runtime mode switching (full-auto, approve-goals, approve-all). Per-instance gate policies. Escalation forwarding from Meta-Agent. Dashboard-embeddable approval UI.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Configurable write fence per tool category. Approval delegation rules. Audit trail of all gate decisions. Timeout policies with configurable fallback actions.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-human-gate.feature",
              "title": "Human Gate (MVP)",
              "content": "Feature: Human Gate (MVP)\n  The Human Gate provides an approval queue for dangerous\n  operations and a write fence for destructive actions.\n\n  Background:\n    Given the Human Gate is running\n\n  Scenario: Block destructive operation for approval\n    Given the write fence includes \"database drop\" operations\n    When a task requests a database drop\n    Then the task is paused with status \"awaiting_approval\"\n    And the approval request is added to the queue\n\n  Scenario: Approve pending request\n    Given a task is paused awaiting approval\n    When a human approves the request\n    Then the task status changes to \"approved\"\n    And execution resumes\n\n  Scenario: Reject pending request\n    Given a task is paused awaiting approval\n    When a human rejects the request\n    Then the task status changes to \"rejected\"\n    And the task is aborted\n\n  Scenario: Timeout on unanswered request\n    Given a task has been awaiting approval for longer than the timeout\n    When the timeout expires\n    Then the task is aborted\n    And the timeout event is logged\n"
            }
          ]
        }
      },
      {
        "id": "user-knowledge-graph",
        "name": "User Knowledge Graph",
        "type": "app",
        "layer": "knowledge-graphs",
        "color": "gold",
        "icon": "ðŸ‘¤",
        "description": "A persistent graph of the user's world. Nodes are domain entities â€” people, projects, clients, teams, products, preferences, business rules, conventions, deadlines. Edges are typed relationships with metadata. This is not about code â€” it's about understanding who you are and what you care about so agents make contextually appropriate decisions. Entity types: person, project, org, team, preference, convention, deadline, stack, compliance, product, domain-concept, decision. Relationship types: OWNS, PREFERS, WORKS_WITH, HAS_CLIENT, USES_STACK, REQUIRES, CONVENTION, HAS_DEADLINE, DECIDED, DISLIKES. Populated by: (1) User directly â€” onboarding flow or dashboard edits. (2) Meta-Agent â€” infers entities from conversations and patterns over time. (3) Never by Worker â€” same injection-safety principle. Worker reads, never writes. Confidence layering: user-explicit (1.0) > meta-agent-inferred (0.8) > auto-extracted (0.6).",
        "tags": [
          "new",
          "persistent",
          "entity types",
          "relationship types",
          "confidence layering"
        ],
        "sort_order": 41,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "SQLite-backed entity store. Add/query entities with typed relationships. Basic traversal (1-hop neighbours). Manual entity creation via CLI or dashboard. Simple text search across entities.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "A persistent graph of the user's world. Nodes are domain entities â€” people, projects, clients, teams, products, preferences, business rules, conventions, deadlines. Edges are typed relationships with metadata. This is not about code â€” it's about understanding who you are and what you care about so agents make contextually appropriate decisions. Entity types: person, project, org, team, preference, convention, deadline, stack, compliance, product, domain-concept, decision. Relationship types: OWNS, PREFERS, WORKS_WITH, HAS_CLIENT, USES_STACK, REQUIRES, CONVENTION, HAS_DEADLINE, DECIDED, DISLIKES. Populated by: (1) User directly â€” onboarding flow or dashboard edits. (2) Meta-Agent â€” infers entities from conversations and patterns. (3) Never by Worker â€” injection safety. Confidence layering: user-explicit (1.0) > meta-agent-inferred (0.8) > auto-extracted (0.6). What it solves: Personalisation (\"Alice prefers typed SQL over ORMs\"), project awareness (\"acme-saas uses Next.js + Postgres, client needs SOC2\"), team context (\"Bob is backend lead, prefers PRs\"), decision memory (\"We decided JWT over sessions on Jan 15\"), convention enforcement (\"No ORMs, minimal comments, Tailwind\"), deadline awareness (Meta-Agent prioritises based on known deadlines).",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Meta-Agent write access for inferred entities. Confidence layering (user-explicit 1.0 > meta-inferred 0.8). Multi-hop traversal queries. Convention enforcement lookups. Deadline awareness queries.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Full graph query language. Temporal awareness (when was this preference set?). Conflict resolution for contradictory preferences. Export/import for portability. Dashboard entity editor.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-user-kg.feature",
              "title": "User Knowledge Graph (MVP)",
              "content": "Feature: User Knowledge Graph (MVP)\n  A persistent entity-relationship store for domain context:\n  people, projects, preferences, conventions, deadlines.\n\n  Background:\n    Given the User Knowledge Graph SQLite database exists\n\n  Scenario: Add an entity\n    When an entity is added with type \"person\" and name \"Alice\"\n    Then the entity exists in the graph with a unique ID\n    And it has type \"person\" and name \"Alice\"\n\n  Scenario: Add a relationship between entities\n    Given entities \"Alice\" (person) and \"acme-saas\" (project) exist\n    When a relationship \"OWNS\" is added from \"Alice\" to \"acme-saas\"\n    Then the edge exists with type \"OWNS\"\n    And it references both entities\n\n  Scenario: Query 1-hop neighbours\n    Given \"Alice\" has relationships to \"acme-saas\", \"Bob\", and \"minimal-comments\"\n    When querying neighbours of \"Alice\"\n    Then all 3 connected entities are returned\n    And each result includes the relationship type\n\n  Scenario: Search entities by text\n    Given entities \"Alice\", \"Bob\", and \"acme-saas\" exist\n    When searching for \"alice\"\n    Then the entity \"Alice\" is returned\n\n  Scenario: Add entity with metadata\n    When an entity is added with type \"preference\" name \"no-orms\" and metadata '{\"reason\": \"team decision\"}'\n    Then the entity exists with the metadata attached\n"
            }
          ]
        }
      },
      {
        "id": "rpg-code-graph",
        "name": "RPG Code Graph",
        "type": "app",
        "layer": "knowledge-graphs",
        "color": "emerald",
        "icon": "ðŸ—º",
        "description": "An RPG-style structural graph of the current codebase. Encodes file hierarchy, module boundaries, inter-module data flows, function signatures, class inheritance, and import dependencies. Inspired by Microsoft RPG/ZeroRepo (https://arxiv.org/abs/2509.16198). This is a code quality feature â€” it helps the Worker write structurally coherent code by understanding what exists, what depends on what, and where new code should go. Node types: module, file, function, class, interface, package, route, schema, test. Edge types: CONTAINS, IMPORTS, EXPORTS, DATA_FLOW, EXTENDS, IMPLEMENTS, DEPENDS_ON, TESTS, CALLS. Populated by: (1) Static analysis on init â€” AST parse via tree-sitter on repo load (~seconds for repos under 100K LoC). (2) Worker's Checkpointer â€” auto-updates after file edits (re-parse only changed files, diff old vs new, update edges incrementally). (3) Meta-Agent â€” can annotate with higher-level module boundaries and data flow intentions. Lightweight â€” no LLM needed for extraction. Query patterns: topo_order(module), data_flow(A,B), dependents(file), pattern(type,dir), where_to_add(capability).",
        "tags": [
          "new",
          "tree-sitter",
          "ast parsing",
          "disposable",
          "re-derivable",
          "query patterns"
        ],
        "sort_order": 42,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Static analysis on repo load using tree-sitter. Build initial graph from imports, exports, class hierarchy. Basic queries: list files in module, show imports for file. SQLite-backed.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "An RPG-style structural graph of the current codebase. Encodes file hierarchy, module boundaries, inter-module data flows, function signatures, class inheritance, and import dependencies. Inspired by Microsoft RPG/ZeroRepo (arxiv.org/abs/2509.16198). This is a code quality feature â€” it helps the Worker write structurally coherent code. Node types: module, file, function, class, interface, package, route, schema, test. Edge types: CONTAINS, IMPORTS, EXPORTS, DATA_FLOW, EXTENDS, IMPLEMENTS, DEPENDS_ON, TESTS, CALLS. What it solves: Dependency awareness (Worker knows what imports what before editing), placement decisions (\"where should rate limiting go?\" â†’ traverse moduleâ†’fileâ†’function), topological code generation (build in dependency order), data flow understanding (inter-module edges), pattern consistency (existing patterns visible), blast radius estimation (Meta-Agent traverses deps). Implementation: (1) Initial build on repo load â€” tree-sitter AST parse, extract files/imports/exports/classes/functions, infer modules from directory structure, ~seconds for repos under 100K LoC. (2) Incremental update on edit â€” Checkpointer detects file-edit tool calls, re-parses only changed files, diffs old vs new, updates edges incrementally. (3) Query patterns â€” topo_order(module), data_flow(A,B), dependents(file), pattern(type,dir), where_to_add(capability).",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Incremental updates via Checkpointer (re-parse only changed files). Dependency traversal (topo_order, dependents). Data flow edges between modules. Pattern queries.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Full where_to_add capability suggestions. Blast radius estimation. Meta-Agent annotations. Multi-language AST support. Dashboard Repo Map visualization.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-code-graph.feature",
              "title": "RPG Code Graph (MVP)",
              "content": "Feature: RPG Code Graph (MVP)\n  Static analysis on repo load builds a structural graph of the\n  codebase: files, imports, exports, classes, functions.\n\n  Background:\n    Given a repository with source files exists\n\n  Scenario: Build initial graph from repo\n    When the Code Graph builder runs on the repository\n    Then nodes are created for each source file\n    And edges are created for import relationships\n    And the graph is stored in SQLite\n\n  Scenario: Extract function exports\n    Given a file \"auth/handler.ts\" exports function \"verifyToken\"\n    When the AST parser processes the file\n    Then a function node \"verifyToken\" exists\n    And an EXPORTS edge connects the file to the function\n\n  Scenario: Extract import relationships\n    Given \"api/routes.ts\" imports from \"auth/handler.ts\"\n    When the AST parser processes both files\n    Then an IMPORTS edge connects \"api/routes.ts\" to \"auth/handler.ts\"\n\n  Scenario: Infer modules from directory structure\n    Given the repository has directories \"auth/\", \"api/\", \"db/\"\n    When the Code Graph builder runs\n    Then module nodes are created for \"auth\", \"api\", \"db\"\n    And CONTAINS edges connect modules to their files\n\n  Scenario: Query files in a module\n    Given the module \"auth\" contains \"handler.ts\" and \"middleware.ts\"\n    When querying files in module \"auth\"\n    Then both files are returned\n"
            }
          ]
        }
      },
      {
        "id": "meta-agent",
        "name": "Meta-Agent (Planner)",
        "type": "app",
        "layer": "dual-agents",
        "color": "orange",
        "icon": "ðŸ§ ",
        "description": "Stock OpenCode, planning system prompt, cheap/fast model (Haiku/Sonnet). This instance never touches the codebase or external APIs directly. It only plans, evaluates, and dispatches. Tier-0: recovered first. If this is down, the Worker has no direction. Only internal tools, no injection surface. Traverses User KG to align plans with user preferences, deadlines, team context. Reads Code Graph to understand repo structure before decomposing coding tasks. Also handles escalation responses: reads Worker's request_clarification entries from State Store, reasons about them, and writes guidance back â€” which the Worker receives on its next checkpoint resume or via check_escalation_response tool. System prompt: \"You are a task planner. Use your tools to: read the goal queue, check worker status, decompose goals into tasks, dispatch tasks, evaluate results, and generate follow-up goals. You may also evolve the worker's tools and config when needed.\" The loop emerges from the prompt + tool availability.",
        "tags": [
          "tier-0",
          "10 internal tools",
          "reads/writes user kg",
          "reads code graph",
          "haiku/sonnet",
          "new"
        ],
        "sort_order": 51,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Single OpenCode instance with planning system prompt. Read goal queue, decompose into sub-tasks, dispatch to Worker via State Store. Read Worker progress from checkpoints. Basic goal â†’ task decomposition.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Stock OpenCode, planning system prompt, cheap/fast model (Haiku/Sonnet). This instance never touches the codebase or external APIs directly. It only plans, evaluates, and dispatches. Tier-0: recovered first. If this is down, the Worker has no direction. Only internal tools, no injection surface. Traverses User KG to align plans with user preferences, deadlines, team context. Reads Code Graph to understand repo structure before decomposing coding tasks. Also handles escalation responses: reads Worker's request_clarification entries from State Store, reasons about them, and writes guidance back. System prompt: \"You are a task planner. Use your tools to: read the goal queue, check worker status, decompose goals into tasks, dispatch tasks, evaluate results, and generate follow-up goals. You may also evolve the worker's tools and config when needed.\" The loop emerges from the prompt + tool availability. How Meta-Agent uses both graphs: Before planning â€” \"What stack? Deadlines? Preferences?\" Before decomposing â€” \"Which modules? Dependency order? Blast radius?\" Task dispatch enrichment â€” includes relevant KG + Code Graph context in Worker's task prompt. Tool selection â€” User KG says \"prefers Brave over Google\" â†’ configures proxy. Knowledge curation â€” writes inferred preferences to User KG. Self-evolution: tool discovery via tool_registry.search, config mutation via typed builder DSL (not raw JSON), sub-goal generation, prompt evolution based on observed results. Guardrails: budget limits, scope limits, allowed tool categories, model whitelist.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Phase-locked BDD/TDD dispatch pipeline. Gate verification between phases. Escalation response handling. User KG reads for planning context. Code Graph reads for repo-aware decomposition.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Self-evolution: tool discovery + hot-swap via proxy_admin. Config mutation via DSL. Prompt evolution based on observed results. Knowledge curation â€” write inferred preferences to User KG. Budget and scope guardrails.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-planning-loop.feature",
              "title": "Meta-Agent Planning Loop (MVP)",
              "content": "Feature: Meta-Agent Planning Loop (MVP)\n  The Meta-Agent reads goals, decomposes them into tasks,\n  and dispatches work to the Worker via the State Store.\n\n  Background:\n    Given the Meta-Agent is running with a planning system prompt\n    And the State Store is accessible\n\n  Scenario: Read next goal from queue\n    Given the goal queue contains \"Build user authentication\"\n    When the Meta-Agent checks the goal queue\n    Then it receives the goal \"Build user authentication\"\n    And the goal status is set to \"in-progress\"\n\n  Scenario: Decompose goal into tasks\n    Given the Meta-Agent has received the goal \"Build user authentication\"\n    When it decomposes the goal\n    Then the State Store contains at least 2 sub-tasks\n    And each sub-task has a description and ordering\n\n  Scenario: Dispatch task to Worker\n    Given a sub-task \"Create login endpoint\" exists in the State Store\n    When the Meta-Agent dispatches the task\n    Then the task status is set to \"dispatched\"\n    And the task includes a description and success criteria\n\n  Scenario: Read Worker progress\n    Given a task has been dispatched to the Worker\n    When the Meta-Agent checks Worker progress\n    Then it receives the latest checkpoint for that task\n    And the checkpoint includes tool calls made and their results\n\n  Scenario: Complete goal when all tasks done\n    Given all sub-tasks for a goal are in \"complete\" status\n    When the Meta-Agent evaluates the goal\n    Then the goal status is set to \"complete\"\n    And the Meta-Agent reads the next goal from the queue\n"
            }
          ]
        }
      },
      {
        "id": "worker",
        "name": "Worker (Executor)",
        "type": "app",
        "layer": "dual-agents",
        "color": "cyan",
        "icon": "âš¡",
        "description": "Stock OpenCode, execution system prompt, strong model (Sonnet/Opus). Tier-1, ephemeral, no fork. Lower stability priority â€” if it crashes, the Meta-Agent re-dispatches. Treated as ephemeral and replaceable. On recovery, the agent continues without knowing it crashed â€” the Context Rebuilder injects a resume prompt that makes it look like a natural continuation. External tools, sanitiser required. Reads User KG to respect user preferences during execution (naming conventions, tech choices). Traverses Code Graph to write structurally coherent code (dependency-aware edits, correct placement). Has two escalation tools: request_clarification to pause and ask the planner for guidance, and check_escalation_response to poll for an answer. System prompt: \"If you're uncertain about scope, direction, or trade-offs, use request_clarification. Don't guess â€” ask.\" Receives tasks as structured system prompt injections: phase, task, constraints, forbidden_actions, available_tools, success_criteria. Single-phase isolation: the Worker sees \"Write failing step tests for this feature file. DO NOT implement any production code.\" It literally cannot skip ahead because it doesn't know what \"ahead\" is. The forbidden_actions field explicitly lists what it must not do (e.g. [\"create production files\", \"modify existing src/\", \"run tests in watch mode\"]).",
        "tags": [
          "tier-1",
          "dynamic tools",
          "reads user kg",
          "reads code graph",
          "sanitiser required",
          "ephemeral",
          "no fork",
          "sonnet/opus",
          "new"
        ],
        "sort_order": 52,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Single OpenCode instance with execution system prompt. Receive task from State Store, execute with available tools, report results. Basic tool access via MCP proxy.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Stock OpenCode, execution system prompt, strong model (Sonnet/Opus). Tier-1, ephemeral, no fork. Lower stability priority â€” if it crashes, the Meta-Agent re-dispatches. Treated as ephemeral and replaceable. On recovery, the agent continues without knowing it crashed â€” the Context Rebuilder injects a resume prompt that makes it look like a natural continuation. External tools, sanitiser required. Reads User KG to respect user preferences during execution. Traverses Code Graph for structurally coherent code. Has escalation tools: request_clarification and check_escalation_response. System prompt: \"If you're uncertain about scope, direction, or trade-offs, use request_clarification. Don't guess â€” ask.\" Receives tasks as structured injections: phase, task, constraints, forbidden_actions, available_tools, success_criteria. Single-phase isolation: Worker sees only current phase, never what comes next. Example forbidden_actions: [\"create production files\", \"modify existing src/\", \"run tests in watch mode\"]. The Worker literally cannot skip ahead because it doesn't know what \"ahead\" is.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Phase-locked execution (single phase per dispatch, forbidden_actions enforcement). Escalation tools (request_clarification, check_escalation_response). User KG reads for preference-aware execution. Code Graph reads for structural coherence.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Context-aware resume after crash (transparent to agent). Dynamic tool manifest â€” handles hot-swap mid-session. Confidence scoring on outputs. Full sanitiser integration on all external I/O.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-task-execution.feature",
              "title": "Worker Task Execution (MVP)",
              "content": "Feature: Worker Task Execution (MVP)\n  The Worker receives tasks from the State Store and executes\n  them using available tools via the MCP proxy.\n\n  Background:\n    Given the Worker is running with an execution system prompt\n    And the MCP proxy is accessible with at least one tool\n\n  Scenario: Receive dispatched task\n    Given a task \"Create login endpoint\" is in \"dispatched\" status\n    When the Worker checks for pending tasks\n    Then it receives the task with description and constraints\n\n  Scenario: Execute task with tools\n    Given the Worker has received a task\n    When it executes the task\n    Then it makes at least one tool call via the MCP proxy\n    And each tool call is logged to the State Store\n\n  Scenario: Report task completion\n    Given the Worker has finished executing a task\n    When it reports results\n    Then the task status is set to \"complete\"\n    And the result summary is written to the State Store\n\n  Scenario: Handle tool call failure\n    Given the Worker is executing a task\n    When a tool call returns an error\n    Then the Worker logs the error\n    And it attempts an alternative approach or reports failure\n\n  Scenario: Operate within provided constraints\n    Given the Worker receives a task with forbidden_actions [\"delete files\"]\n    When it executes the task\n    Then it does not call any tool that would delete files\n"
            }
          ]
        }
      },
      {
        "id": "state-store",
        "name": "State Store",
        "type": "app",
        "layer": "shared-state",
        "color": "blue",
        "icon": "ðŸ’¾",
        "description": "Append-only log that both instances read/write. The Meta-Agent writes goals and reads results. The Worker's Checkpointer writes tool call logs and progress. This is how the two OpenCode instances communicate without direct coupling â€” they share a database, not a connection. Also stores escalation records (question, context, response, status) and fast-path completion records so the Meta-Agent stays aware of tasks it didn't plan. Dashboard reads everything here.",
        "tags": [
          "checkpointer",
          "context rebuilder",
          "crash recovery"
        ],
        "sort_order": 71,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "SQLite WAL database with tables for goals, tasks, and tool_logs. Basic CRUD operations. Both agents read/write via simple SQL. No pruning, no optimization.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Append-only log that both instances read/write. The Meta-Agent writes goals and reads results. The Worker's Checkpointer writes tool call logs and progress. This is how the two OpenCode instances communicate without direct coupling â€” they share a database, not a connection. Also stores escalation records (question, context, response, status) and fast-path completion records so the Meta-Agent stays aware of tasks it didn't plan. Dashboard reads everything here.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Add checkpoints table, escalation records, fast-path completion records. Context rebuilder queries. Pruning policy (keep last N days). Indexes for common query patterns.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "SSE/WebSocket push for live dashboard updates. Postgres option for multi-machine deployments. Full audit trail with retention policies. Query optimization for dashboard views.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-shared-state.feature",
              "title": "Shared State Store (MVP)",
              "content": "Feature: Shared State Store (MVP)\n  The State Store is a SQLite WAL database that both agents\n  read and write. It stores goals, tasks, and tool logs.\n\n  Background:\n    Given the State Store SQLite database exists\n\n  Scenario: Create a goal\n    When a goal \"Build user authentication\" is inserted\n    Then the goal exists with status \"pending\"\n    And the goal has a created_at timestamp\n\n  Scenario: Create tasks for a goal\n    Given a goal exists with id \"goal-1\"\n    When tasks are inserted for goal \"goal-1\"\n    Then each task references the parent goal\n    And each task has status \"pending\" and an ordering index\n\n  Scenario: Log a tool call\n    Given a task exists with id \"task-1\"\n    When a tool call log is inserted with tool \"filesystem\", args hash, and result hash\n    Then the tool log exists with a timestamp\n    And the tool log references \"task-1\"\n\n  Scenario: Both agents can read/write concurrently\n    Given the Meta-Agent is writing a goal\n    And the Worker is writing a tool log\n    Then both writes succeed without conflict\n    And the WAL journal mode handles concurrent access\n\n  Scenario: Query task status by goal\n    Given a goal has 3 tasks with statuses \"complete\", \"in-progress\", \"pending\"\n    When querying tasks for that goal\n    Then all 3 tasks are returned with their statuses\n"
            }
          ]
        }
      },
      {
        "id": "checkpointer",
        "name": "Checkpointer",
        "type": "app",
        "layer": "shared-state",
        "color": "blue",
        "icon": "ðŸ“¸",
        "description": "Taps Worker's Proxy. Writes after every tool response: task ID, tool name, args, result hash, timestamp, plan summary. Also snapshots escalation state so crash recovery can restore a paused-and-waiting Worker correctly. If tool was a file edit, also triggers AST re-parse and Code Graph update. Strategy: tool results are facts; LLM reasoning can be re-derived. So we save the facts (tool call + result) and let the Context Rebuilder regenerate the reasoning frame on recovery. Runs async â€” doesn't block the agent. The proxy fires-and-forgets to the checkpointer; the Worker never waits for a checkpoint write to complete.",
        "tags": [
          "async",
          "fire-and-forget",
          "non-blocking",
          "code graph update"
        ],
        "sort_order": 72,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Intercept tool responses from Worker proxy. Write task_id, tool_name, args_hash, result_hash, timestamp to State Store. Fire-and-forget (async, non-blocking).",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Taps Worker's Proxy. Writes after every tool response: task ID, tool name, args, result hash, timestamp, plan summary. Also snapshots escalation state so crash recovery can restore a paused-and-waiting Worker correctly. If tool was a file edit, triggers AST re-parse and Code Graph update. Strategy: tool results are facts; LLM reasoning can be re-derived. So we save the facts and let the Context Rebuilder regenerate the reasoning frame on recovery. Runs async â€” the proxy fires-and-forgets; the Worker never waits for a checkpoint write to complete.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Escalation state snapshots. File-edit detection triggering Code Graph AST re-parse. Plan summary snapshots for context rebuilder. Idempotency markers for crash recovery.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Configurable checkpoint granularity. Compressed checkpoint storage. Checkpoint pruning with retention policy. Metrics on checkpoint write latency.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-checkpointing.feature",
              "title": "Checkpointer (MVP)",
              "content": "Feature: Checkpointer (MVP)\n  The Checkpointer taps the Worker's MCP proxy and records\n  every tool call for crash recovery and progress tracking.\n\n  Background:\n    Given the Checkpointer is attached to the Worker's MCP proxy\n    And the State Store is accessible\n\n  Scenario: Record tool call after response\n    Given the Worker calls the \"filesystem\" tool with args \"read file.ts\"\n    When the tool returns a successful response\n    Then the Checkpointer writes a record to the State Store\n    And the record includes task_id, tool_name, args_hash, result_hash, and timestamp\n\n  Scenario: Non-blocking operation\n    Given the Worker is executing a task\n    When a tool call completes\n    Then the Checkpointer writes asynchronously\n    And the Worker does not wait for the checkpoint write\n\n  Scenario: Maintain ordering of tool calls\n    Given the Worker makes 3 sequential tool calls\n    When all 3 are checkpointed\n    Then the records are ordered by timestamp\n    And each has a sequential index within the task\n\n  Scenario: Handle write failure gracefully\n    Given the State Store is temporarily unavailable\n    When the Checkpointer attempts to write\n    Then it retries with backoff\n    And the Worker execution is not affected\n"
            }
          ]
        }
      },
      {
        "id": "context-rebuilder",
        "name": "Context Rebuilder",
        "type": "app",
        "layer": "shared-state",
        "color": "blue",
        "icon": "ðŸ“",
        "description": "On crash recovery of either instance: generates resume prompt from compressed checkpoint + relevant graph context. For Worker: \"you were doing X, completed Y, next step Z\". For Meta-Agent: \"current goal is X, worker status is Y, pending goals are Z\". If Worker was in paused:awaiting_guidance state, resume prompt includes the escalation question and any response received while it was down. Lossy by design. You can't clone LLM hidden state â€” it's non-serialisable. This is like a save game, not a VM snapshot. The rebuilt context is \"good enough\" â€” the agent continues without knowing it crashed, picking up from the last checkpoint with a compressed summary of what came before.",
        "tags": [
          "lossy by design",
          "save game",
          "resume prompt"
        ],
        "sort_order": 73,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Read last checkpoint from State Store. Generate basic resume prompt: \"you were doing X, completed Y, next step Z\". Inject as system prompt on respawn.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "On crash recovery of either instance: generates resume prompt from compressed checkpoint + relevant graph context. For Worker: \"you were doing X, completed Y, next step Z\". For Meta-Agent: \"current goal is X, worker status is Y, pending goals are Z\". If Worker was in paused:awaiting_guidance state, resume prompt includes the escalation question and any response received while it was down. Lossy by design. You can't clone LLM hidden state â€” it's non-serialisable. This is like a save game, not a VM snapshot.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Include relevant User KG context in resume prompt. Include Code Graph context for coding tasks. Handle paused:awaiting_guidance state.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Compressed multi-checkpoint summaries. Relevance-ranked context selection. Token budget management for resume prompts.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-context-rebuild.feature",
              "title": "Context Rebuilder (MVP)",
              "content": "Feature: Context Rebuilder (MVP)\n  On crash recovery, the Context Rebuilder generates a resume\n  prompt from the last checkpoint so the agent can continue.\n\n  Background:\n    Given the State Store contains checkpoints for a crashed agent\n\n  Scenario: Generate resume prompt for Worker\n    Given the Worker crashed mid-task on task \"Create login endpoint\"\n    And the last checkpoint shows 3 completed tool calls\n    When the Context Rebuilder generates a resume prompt\n    Then the prompt includes \"you were doing: Create login endpoint\"\n    And it lists the 3 completed tool calls with their results\n    And it states the next expected action\n\n  Scenario: Generate resume prompt for Meta-Agent\n    Given the Meta-Agent crashed while processing goal \"Build auth\"\n    And the goal has 5 tasks, 2 completed and 1 in-progress\n    When the Context Rebuilder generates a resume prompt\n    Then the prompt includes the current goal state\n    And it lists completed and pending tasks\n    And it states the current Worker status\n\n  Scenario: Handle empty checkpoint\n    Given no checkpoints exist for the crashed agent\n    When the Context Rebuilder generates a resume prompt\n    Then it produces a minimal prompt with no prior context\n    And the agent starts fresh\n"
            }
          ]
        }
      },
      {
        "id": "mcp-proxy-meta",
        "name": "MCP Proxy â€” Meta-Agent",
        "type": "app",
        "layer": "mcp-proxies",
        "color": "orange",
        "icon": "â‡„",
        "description": "Hosts 10 planning tools (6 planning + 4 graph). These are your custom MCP servers â€” small, stable, purpose-built. No external API calls, no injection risk.",
        "tags": [
          "static",
          "no sanitiser",
          "low risk"
        ],
        "sort_order": 81,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Static MCP server hosting goal_queue, state_reader, worker_control tools. Simple stdio transport. No hot-swap needed.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Hosts 10 planning tools (6 planning + 4 graph). These are your custom MCP servers â€” small, stable, purpose-built. No external API calls, no injection risk.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {}
      },
      {
        "id": "mcp-proxy-worker",
        "name": "MCP Proxy â€” Worker",
        "type": "app",
        "layer": "mcp-proxies",
        "color": "cyan",
        "icon": "â‡„",
        "description": "Hosts all external-facing tools + escalation + graph reads. Dynamic manifest â€” the Meta-Agent's proxy_admin tool adds/removes servers here at runtime. All responses pass through the Sanitiser. Health & circuit breaker: heartbeats downstream servers; dead endpoints auto-removed from manifest, Meta-Agent notified via State Store so it can find replacements.",
        "tags": [
          "sanitiser required",
          "hot-swappable",
          "checkpoint tap",
          "circuit breaker"
        ],
        "sort_order": 82,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "MCP proxy with configurable tool list. Route tool calls to downstream servers. Pass responses through sanitiser. Basic health check on downstream servers.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Hosts all external-facing tools + escalation + graph reads. Dynamic manifest â€” Meta-Agent's proxy_admin adds/removes servers at runtime. All responses pass through Sanitiser. Health & circuit breaker: heartbeats downstream servers; dead endpoints auto-removed from manifest, Meta-Agent notified via State Store so it can find replacements.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {}
      },
      {
        "id": "sanitiser",
        "name": "3-Stage Sanitiser",
        "type": "app",
        "layer": "security-sandbox",
        "color": "red",
        "icon": "ðŸ›¡",
        "description": "Sits between Worker's Proxy and downstream servers. Stage 1: Heuristic regex for common injection patterns. Stage 2: Structural strip (remove role tags, cap response length). Stage 3: Optional LLM classifier for sophisticated detection. The Meta-Agent's proxy does NOT need a sanitiser â€” its tools are all internal, no external input. Isolated subprocess. Fail-closed. Scans inbound responses (injection defence) and outbound tool args (prevents data exfiltration via a tricked agent â€” e.g. an injected prompt that encodes secrets into a search query). Injection events visible in dashboard Security Events panel.",
        "tags": [
          "3-stage",
          "fail-closed",
          "isolated subprocess",
          "bidirectional"
        ],
        "sort_order": 91,
        "current_version": null,
        "display_state": "Concept",
        "versions": {
          "mvp": {
            "content": "Regex-based heuristic scanner for common injection patterns. Structural strip (remove role tags, cap response length). Pass/block verdict on each tool response. Logging to State Store.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "overview": {
            "content": "Sits between Worker's Proxy and downstream servers. Stage 1: Heuristic regex for common injection patterns. Stage 2: Structural strip (remove role tags, cap response length). Stage 3: Optional LLM classifier for sophisticated detection. The Meta-Agent's proxy does NOT need a sanitiser â€” its tools are all internal, no external input. Isolated subprocess. Fail-closed. Scans inbound responses (injection defence) and outbound tool args (prevents data exfiltration â€” e.g. an injected prompt encoding secrets into a search query). Injection events visible in dashboard Security Events panel.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v1": {
            "content": "Outbound scanning (prevent data exfiltration via tool args). Configurable rule sets per tool. Injection frequency tracking. Auto-disable tools exceeding threshold. Dashboard integration.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          },
          "v2": {
            "content": "Optional LLM classifier stage. Adaptive rules based on observed attack patterns. Per-tool confidence scoring. Full audit trail with payload samples.",
            "progress": 0,
            "status": "planned",
            "updated_at": "2026-02-11 14:27:05"
          }
        },
        "features": {
          "mvp": [
            {
              "filename": "mvp-sanitiser.feature",
              "title": "3-Stage Sanitiser (MVP)",
              "content": "Feature: 3-Stage Sanitiser (MVP)\n  The Sanitiser sits between the Worker's MCP proxy and downstream\n  tool servers. It scans tool responses for injection attempts.\n\n  Background:\n    Given the Sanitiser is running as an isolated subprocess\n\n  Scenario: Pass clean tool response\n    Given a tool response contains normal text content\n    When the Sanitiser processes the response\n    Then the verdict is \"pass\"\n    And the response is forwarded to the Worker unchanged\n\n  Scenario: Block response with injection pattern\n    Given a tool response contains \"ignore previous instructions\"\n    When the Sanitiser processes the response\n    Then the verdict is \"block\"\n    And the response is not forwarded to the Worker\n    And the injection event is logged to the State Store\n\n  Scenario: Strip role tags from response\n    Given a tool response contains \"<system>\" tags\n    When the Sanitiser applies structural stripping\n    Then the role tags are removed from the response\n    And the cleaned response is forwarded\n\n  Scenario: Cap response length\n    Given a tool response exceeds the maximum allowed length\n    When the Sanitiser processes the response\n    Then the response is truncated to the maximum length\n    And the truncation is noted in the log\n\n  Scenario: Fail closed on processing error\n    Given the Sanitiser encounters an internal error during processing\n    When processing a tool response\n    Then the response is blocked (not forwarded)\n    And the error is logged\n"
            }
          ]
        }
      }
    ],
    "edges": [
      {
        "source_id": "meta-agent",
        "target_id": "mcp-proxy-meta",
        "type": "DEPENDS_ON",
        "label": "tool access"
      },
      {
        "source_id": "worker",
        "target_id": "mcp-proxy-worker",
        "type": "DEPENDS_ON",
        "label": "tool access"
      },
      {
        "source_id": "supervisor",
        "target_id": "state-store",
        "type": "DEPENDS_ON",
        "label": "reads checkpoints"
      },
      {
        "source_id": "checkpointer",
        "target_id": "state-store",
        "type": "DEPENDS_ON",
        "label": "writes checkpoints"
      },
      {
        "source_id": "context-rebuilder",
        "target_id": "state-store",
        "type": "DEPENDS_ON",
        "label": "reads checkpoints"
      },
      {
        "source_id": "meta-agent",
        "target_id": "supervisor",
        "type": "DEPENDS_ON",
        "label": "spawned by supervisor"
      },
      {
        "source_id": "worker",
        "target_id": "supervisor",
        "type": "DEPENDS_ON",
        "label": "spawned by supervisor"
      },
      {
        "source_id": "mcp-proxy-meta",
        "target_id": "meta-agent",
        "type": "DEPENDS_ON",
        "label": "serves meta-agent tools"
      },
      {
        "source_id": "mcp-proxy-worker",
        "target_id": "worker",
        "type": "DEPENDS_ON",
        "label": "serves worker tools"
      },
      {
        "source_id": "sanitiser",
        "target_id": "mcp-proxy-worker",
        "type": "DEPENDS_ON",
        "label": "wraps worker proxy"
      },
      {
        "source_id": "live-dashboard",
        "target_id": "state-store",
        "type": "DEPENDS_ON",
        "label": "reads all state"
      },
      {
        "source_id": "live-dashboard",
        "target_id": "supervisor",
        "type": "DEPENDS_ON",
        "label": "reads health API"
      },
      {
        "source_id": "human-gate",
        "target_id": "state-store",
        "type": "DEPENDS_ON",
        "label": "reads/writes approvals"
      },
      {
        "source_id": "context-rebuilder",
        "target_id": "user-knowledge-graph",
        "type": "DEPENDS_ON",
        "label": "reads context"
      },
      {
        "source_id": "context-rebuilder",
        "target_id": "rpg-code-graph",
        "type": "DEPENDS_ON",
        "label": "reads context"
      }
    ]
  },
  "stats": {
    "total_nodes": 67,
    "total_edges": 119,
    "total_versions": 106,
    "total_features": 37
  }
}